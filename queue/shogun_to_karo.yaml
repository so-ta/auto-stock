queue:
  - id: cmd_001
    status: completed
  - id: cmd_002
    status: completed
  - id: cmd_003
    status: completed
  - id: cmd_004
    status: completed
  - id: cmd_005
    status: completed
  - id: cmd_006
    status: completed

  - id: cmd_007
    timestamp: "2026-01-28T16:59:23"
    command: "パフォーマンス最大化のための拡張を実装せよ"
    project: auto_allocation_system
    priority: critical
    status: completed
    context: |
      殿より、シャープレシオを維持しつつパフォーマンスを最大化する改善を命じられた。

      【現状の問題】
      - 対象期間が短い（train 252日、test 63日）
      - シグナルが単純（個別指標のみ）
      - パラメータ探索が限定的
      - 過去30日で -0.14% のリターン

      【改善方針】
      1. 対象期間の拡大: 2年以上のデータで学習
      2. 複合モデル: 複数シグナルのアンサンブル、レジーム検出
      3. パラメータ探索の拡充: より広い範囲、より多くの組み合わせ
      4. 新しいシグナル追加: ファクター系、センチメント系
      5. 動的重み調整: ボラティリティに応じた配分変更

      【目標】
      - Sharpe Ratio > 1.0 を維持
      - 年率リターン 10%+ を目指す
      - 最大ドローダウン < 15%

    tasks:
      - task_id: task_007_1
        description: |
          config/default.yaml の期間設定を拡大せよ

          【変更内容】
          ```yaml
          walk_forward:
            train_period_days: 504   # 2年（現状252日→504日）
            test_period_days: 126    # 6ヶ月（現状63日→126日）
            step_days: 21            # 月次更新
            embargo_days: 5          # データリーク防止
          ```

          【理由】
          - 長期トレンドを学習するため
          - 複数の市場サイクルを含めるため
          - 統計的に有意なサンプル数を確保
        priority: critical

      - task_id: task_007_2
        description: |
          src/signals/ensemble.py を新規作成せよ（複合シグナル）

          【実装内容】
          1. MomentumEnsemble: 複数期間モメンタムの加重平均
             - 5日, 10日, 20日, 60日 モメンタムを統合
             - 短期と長期のトレンド一致度を評価

          2. MeanReversionEnsemble: 複数指標の平均回帰シグナル
             - ボリンジャー + RSI + Zスコアの統合
             - 過売り/過買いの確信度を計算

          3. TrendStrength: トレンドの強さを測定
             - ADX + 移動平均乖離率 + 価格チャネル位置
             - トレンド相場 vs レンジ相場の判定

          4. RegimeDetector: 市場レジーム検出
             - ボラティリティレジーム（低/中/高）
             - トレンドレジーム（上昇/横ばい/下降）
             - レジームに応じた戦略切り替え

          【出力】
          各シグナルは [-1, +1] の標準化スコア
        priority: critical

      - task_id: task_007_3
        description: |
          src/signals/factor.py を新規作成せよ（ファクター系シグナル）

          【実装内容】
          1. ValueFactor: バリュー指標
             - 52週高値からの乖離率
             - 長期平均価格との比較

          2. QualityFactor: クオリティ指標
             - ボラティリティの安定性
             - リターンの一貫性（連続上昇/下降日数）

          3. LowVolFactor: 低ボラティリティ指標
             - 過去Nヶ月のボラティリティランキング
             - 低ボラ銘柄を優先

          4. MomentumFactor: クロスセクショナルモメンタム
             - 複数銘柄間での相対強度
             - 勝ち組/負け組の識別
        priority: high

      - task_id: task_007_4
        description: |
          src/meta/dynamic_weighter.py を新規作成せよ（動的重み調整）

          【実装内容】
          1. VolatilityScaling: ボラティリティに応じた配分調整
             - 高ボラ時 → 現金比率UP、各銘柄比率DOWN
             - 低ボラ時 → リスク資産比率UP
             - 目標ボラティリティを設定可能

          2. DrawdownProtection: ドローダウン保護
             - 直近の損失が閾値を超えたら現金比率UP
             - 回復したら徐々にリスク資産に戻す

          3. RegimeBasedWeighting: レジームに応じた戦略選択
             - トレンド相場 → モメンタム戦略重視
             - レンジ相場 → 平均回帰戦略重視
             - 高ボラ相場 → 現金重視

          【設定項目】
          ```yaml
          dynamic_weighting:
            target_volatility: 0.15      # 目標ボラ15%
            max_drawdown_trigger: 0.10   # DD10%で防御モード
            regime_lookback_days: 60     # レジーム判定期間
          ```
        priority: high

      - task_id: task_007_5
        description: |
          パラメータ探索を拡充せよ

          【変更内容】
          1. src/signals/momentum.py のパラメータ範囲拡大
             - lookback_period: [5, 10, 15, 20, 30, 40, 60, 90, 120]
             - 現状 [5, 10, 20] → 9パターンに拡大

          2. src/signals/mean_reversion.py のパラメータ範囲拡大
             - period: [10, 14, 20, 30, 40]
             - std_mult: [1.5, 2.0, 2.5, 3.0]
             - 現状より多くの組み合わせ

          3. config/default.yaml の探索設定
             ```yaml
             optimization:
               max_combinations: 500      # 現状100→500
               cv_folds: 5                # 交差検証
               scoring_metric: "sharpe"   # 最適化指標
             ```
        priority: high

      - task_id: task_007_6
        description: |
          src/meta/ensemble_combiner.py を新規作成せよ（アンサンブル統合）

          【実装内容】
          1. StackingEnsemble: スタッキングアンサンブル
             - 各戦略のスコアを特徴量として
             - メタモデル（線形回帰/Ridge）で最終スコア予測

          2. VotingEnsemble: 投票アンサンブル
             - 各戦略の「買い/売り/中立」投票
             - 多数決で最終判断

          3. WeightedAverageEnsemble: 加重平均アンサンブル
             - 各戦略の過去パフォーマンスで重み付け
             - 指数加重（最近を重視）

          【統合ロジック】
          ```
          final_score = Σ(strategy_weight × strategy_score)
          strategy_weight ∝ exp(β × past_sharpe)
          ```
        priority: high

      - task_id: task_007_7
        description: |
          pipeline.py を更新して新機能を統合せよ

          【変更内容】
          1. 新シグナル（ensemble, factor）の登録
          2. DynamicWeighter の統合
          3. EnsembleCombiner の統合
          4. レジーム検出の組み込み

          【新しいパイプラインフロー】
          ```
          データ取得 → 品質チェック → レジーム検出
              ↓
          シグナル生成（基本 + アンサンブル + ファクター）
              ↓
          戦略評価（拡大期間）
              ↓
          アンサンブル統合
              ↓
          動的重み調整（ボラスケール、DD保護）
              ↓
          アセット配分 → 出力
          ```
        priority: critical

      - task_id: task_007_8
        description: |
          長期バックテストを実行し、パフォーマンスを検証せよ

          【テスト内容】
          1. 対象: AAPL, MSFT, GOOGL, AMZN, META
          2. 期間: 過去2年（504日）
          3. 初期資産: $100

          【検証項目】
          - 年率リターン
          - Sharpe Ratio
          - 最大ドローダウン
          - 月次リターン分布
          - 戦略別貢献度

          【レポート出力】
          results/backtest_report_2year.json
        priority: medium

    execution_strategy: |
      task_007_1 → task_007_2, task_007_3 並行
      task_007_4, task_007_5 は task_007_2 完了後
      task_007_6 は task_007_4 完了後
      task_007_7 は task_007_6 完了後
      task_007_8 は task_007_7 完了後

      足軽の割り当て案:
      - task_007_1: 足軽1（設定変更）
      - task_007_2: 足軽2（アンサンブルシグナル）
      - task_007_3: 足軽3（ファクターシグナル）
      - task_007_4: 足軽4（動的重み調整）
      - task_007_5: 足軽5（パラメータ拡充）
      - task_007_6: 足軽6（アンサンブル統合）
      - task_007_7: 足軽7（パイプライン更新）
      - task_007_8: 足軽8（バックテスト検証）

    expected_improvement: |
      【現状】
      - Sharpe: -0.06（直近30日）
      - リターン: -0.14%
      - MDD: -4.70%

      【目標】
      - Sharpe: > 1.0
      - 年率リターン: > 10%
      - MDD: < 15%

      【改善ポイント】
      1. 長期データ → 過学習回避、安定性向上
      2. 複合モデル → 単一戦略リスク分散
      3. 動的調整 → 市場環境適応
      4. ファクター → 新しいアルファ源泉

  - id: cmd_008
    timestamp: "2026-01-28T18:05:00"
    command: "アセットユニバースを大幅に拡大せよ"
    project: auto_allocation_system
    priority: critical
    status: in_progress
    context: |
      殿より、5銘柄では不十分との指摘を受けた。
      取引可能な全アセットを対象とせよ。

      【現状】
      - 対象銘柄: 5銘柄（AAPL, MSFT, GOOGL, AMZN, META）
      - データソース: yfinance のみ

      【拡大要件】
      1. 米国株: S&P 500 全銘柄 + NASDAQ 100
      2. 日本株: 日経225 または TOPIX Core30
      3. ETF:
         - 株式ETF（SPY, QQQ, IWM, VTI, VOO）
         - 債券ETF（TLT, IEF, BND, AGG, LQD）
         - セクターETF（XLF, XLK, XLE, XLV, XLI）
         - コモディティETF（GLD, SLV, USO, UNG）
         - 国際ETF（EFA, EEM, VWO, IEFA）
      4. コモディティ: Gold, Silver, Oil, Natural Gas（先物またはETF経由）
      5. 為替: USD/JPY, EUR/USD, GBP/USD, AUD/USD 等

      【技術的課題】
      1. 日本株データ取得: yfinance で .T サフィックス（例: 7203.T = トヨタ）
      2. 為替データ取得: yfinance で ペア表記（例: USDJPY=X）
      3. スケーリング: 1000+ 銘柄への対応
      4. 異なる取引時間/カレンダーの考慮
      5. 通貨単位の正規化（円建て vs ドル建て）

    tasks:
      - task_id: task_008_1
        description: |
          config/universe.yaml を新規作成せよ（アセットユニバース定義）

          【構造】
          ```yaml
          universe:
            us_stocks:
              enabled: true
              source: "sp500"  # sp500, nasdaq100, or custom
              custom_tickers: []

            japan_stocks:
              enabled: true
              source: "nikkei225"  # nikkei225, topix_core30, or custom
              suffix: ".T"
              currency: "JPY"
              custom_tickers: []

            etfs:
              enabled: true
              categories:
                equity: [SPY, QQQ, IWM, VTI, VOO, DIA]
                bonds: [TLT, IEF, BND, AGG, LQD, HYG]
                sector: [XLF, XLK, XLE, XLV, XLI, XLY, XLP, XLU, XLB]
                commodity: [GLD, SLV, USO, UNG, DBA, DBC]
                international: [EFA, EEM, VWO, IEFA, VEA]

            commodities:
              enabled: true
              tickers:
                gold: GC=F
                silver: SI=F
                oil: CL=F
                natural_gas: NG=F

            forex:
              enabled: true
              pairs:
                - USDJPY=X
                - EURUSD=X
                - GBPUSD=X
                - AUDUSD=X
                - USDCHF=X
                - USDCAD=X

            filters:
              min_history_days: 504  # 2年分のデータ必須
              min_avg_volume: 100000  # 最低出来高
              max_spread_pct: 0.5  # 最大スプレッド
          ```
        priority: critical

      - task_id: task_008_2
        description: |
          src/data/universe_loader.py を新規作成せよ

          【機能】
          1. UniverseLoader クラス
             - config/universe.yaml を読み込み
             - 各カテゴリのティッカーリストを生成
             - S&P 500/日経225 等のインデックス構成銘柄を自動取得

          2. get_sp500_tickers(): Wikipedia等からS&P500銘柄一覧取得
          3. get_nikkei225_tickers(): 日経225銘柄一覧取得
          4. get_all_tickers(): 全ティッカーをマージして返す

          【出力形式】
          ```python
          {
            "us_stocks": ["AAPL", "MSFT", ...],
            "japan_stocks": ["7203.T", "6758.T", ...],
            "etfs": ["SPY", "QQQ", ...],
            "commodities": ["GC=F", "SI=F", ...],
            "forex": ["USDJPY=X", "EURUSD=X", ...]
          }
          ```
        priority: critical

      - task_id: task_008_3
        description: |
          src/data/adapters/multi_source_adapter.py を新規作成せよ

          【機能】
          1. MultiSourceAdapter クラス
             - 複数のデータソースを統合
             - アセットタイプに応じて適切なアダプターを選択

          2. データ正規化
             - 全てUSD建てに変換（日本株は為替レートで変換）
             - タイムゾーン統一（UTC）
             - 欠損値の補間ルール統一

          3. バッチ取得
             - 並列でデータ取得（ThreadPoolExecutor）
             - レート制限対応
             - リトライロジック

          【インターフェース】
          ```python
          adapter = MultiSourceAdapter(universe_config)
          data = adapter.fetch_all(start_date, end_date)
          # data: Dict[str, pd.DataFrame]
          ```
        priority: critical

      - task_id: task_008_4
        description: |
          src/data/currency_converter.py を新規作成せよ

          【機能】
          1. CurrencyConverter クラス
             - リアルタイム為替レート取得
             - 過去の為替レートキャッシュ

          2. convert_to_usd(prices: pd.Series, currency: str) -> pd.Series
             - JPY, EUR, GBP 等をUSDに変換

          3. get_fx_rate(pair: str, date: datetime) -> float
             - 特定日の為替レートを取得

          【キャッシュ戦略】
          - 為替レートは日次でキャッシュ
          - data/cache/fx_rates.parquet に保存
        priority: high

      - task_id: task_008_5
        description: |
          src/data/calendar_manager.py を新規作成せよ

          【機能】
          1. CalendarManager クラス
             - 各市場の取引カレンダー管理
             - 米国: NYSE
             - 日本: JPX（東証）

          2. is_trading_day(market: str, date: datetime) -> bool
          3. get_common_trading_days(markets: List[str], start, end) -> List[date]
          4. align_to_common_calendar(data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]

          【依存】
          - exchange_calendars ライブラリ推奨
          - または pandas_market_calendars
        priority: high

      - task_id: task_008_6
        description: |
          pipeline.py を更新してユニバース拡大に対応せよ

          【変更内容】
          1. UniverseLoader の統合
             - 起動時にユニバース定義を読み込み
             - 全ティッカーリストを取得

          2. MultiSourceAdapter への切り替え
             - 既存の YFinanceFetcher を置き換え
             - または YFinanceFetcher を内部で使用

          3. スケーリング対応
             - バッチ処理（100銘柄ずつ）
             - メモリ効率化（必要な期間のみ保持）
             - 進捗表示

          4. 品質チェックの拡張
             - 銘柄数が多いため、サマリーレポート形式に
             - 除外銘柄リストの出力
        priority: critical

      - task_id: task_008_7
        description: |
          config/default.yaml を更新して新設定を追加せよ

          【追加設定】
          ```yaml
          universe:
            config_file: "config/universe.yaml"
            enable_us_stocks: true
            enable_japan_stocks: true
            enable_etfs: true
            enable_commodities: true
            enable_forex: true
            max_assets: 500  # メモリ制限のため上限設定

          data:
            batch_size: 50  # 一度に取得する銘柄数
            parallel_workers: 4  # 並列ワーカー数
            base_currency: "USD"

          performance:
            use_disk_cache: true
            cache_format: "parquet"
            memory_limit_gb: 8
          ```
        priority: high

      - task_id: task_008_8
        description: |
          拡大したユニバースでテスト実行せよ

          【テスト内容】
          1. 小規模テスト: ETF 20銘柄のみで動作確認
          2. 中規模テスト: ETF + 米国株 100銘柄
          3. 大規模テスト: 全カテゴリ 500銘柄

          【検証項目】
          - データ取得成功率
          - 処理時間
          - メモリ使用量
          - 戦略評価の結果

          【レポート出力】
          results/universe_expansion_test.json
        priority: medium

    execution_strategy: |
      task_008_1, task_008_2 並行（設定 + ローダー）
      task_008_3, task_008_4, task_008_5 並行（データアダプター系）
      task_008_6 は task_008_1-5 完了後
      task_008_7 は task_008_1 完了後
      task_008_8 は task_008_6, task_008_7 完了後

      足軽の割り当て案:
      - task_008_1: 足軽1（ユニバース設定）
      - task_008_2: 足軽2（ユニバースローダー）
      - task_008_3: 足軽3（マルチソースアダプター）
      - task_008_4: 足軽4（通貨変換）
      - task_008_5: 足軽5（カレンダー管理）
      - task_008_6: 足軽6（パイプライン更新）
      - task_008_7: 足軽7（設定更新）
      - task_008_8: 足軽8（テスト実行）

    expected_outcome: |
      【拡大後】
      - 対象銘柄: 500+ （現状5銘柄）
      - カバー範囲: 米国株、日本株、ETF、コモディティ、為替
      - データソース: yfinance（統合アダプター経由）

      【期待効果】
      1. 分散効果: より多くのアセットで分散
      2. 相関低減: 異なる資産クラスを組み合わせ
      3. 機会拡大: より多くの投資機会を捕捉
      4. リスク低減: 単一市場への依存を減らす

  - id: cmd_009
    timestamp: "2026-01-28T18:15:00"
    command: "Settings.data 属性エラーを緊急修正せよ"
    project: auto_allocation_system
    priority: critical
    status: completed
    context: |
      task_008_8 のテストで以下のエラーが発生：
      "'Settings' object has no attribute 'data'"

      【原因分析】
      - pipeline.py の 565, 605-607 行目で self.settings.data を参照
      - しかし Settings クラス（settings.py）に data フィールドがない
      - config/default.yaml には data: セクションが存在するが、スキーマに反映されていない

      【該当コード】
      ```python
      # pipeline.py:565
      getattr(self.settings.data, 'use_multi_source', False)

      # pipeline.py:605-607
      batch_size = getattr(self.settings.data, 'batch_size', 50)
      base_currency = getattr(self.settings.data, 'base_currency', 'USD')
      parallel_workers = getattr(self.settings.data, 'parallel_workers', 4)
      ```

    tasks:
      - task_id: task_009_1
        description: |
          src/config/settings.py に DataConfig クラスを追加せよ

          【追加内容】
          ```python
          class DataConfig(BaseModel):
              """Data source configuration."""
              model_config = ConfigDict(frozen=True)

              batch_size: int = Field(default=50)
              parallel_workers: int = Field(default=4)
              base_currency: str = Field(default="USD")
              retry_count: int = Field(default=3)
              rate_limit_delay: float = Field(default=0.5)
              use_multi_source: bool = Field(default=False)
          ```

          Settings クラスに追加:
          ```python
          data: DataConfig = Field(default_factory=DataConfig)
          ```
        priority: critical

      - task_id: task_009_2
        description: |
          修正後、テストを再実行して動作確認せよ

          【テスト内容】
          python -m src.main --backtest --tickers SPY,QQQ,TLT,GLD --days 30

          【期待結果】
          - エラーなく完了
          - weights 出力
        priority: critical

    execution_strategy: |
      task_009_1 → task_009_2（順次実行）

      足軽の割り当て:
      - task_009_1: 足軽1（スキーマ修正）
      - task_009_2: 足軽2（テスト実行）

  - id: cmd_010
    timestamp: "2026-01-28T18:30:00"
    command: "日次・月次リバランス両対応のバックテストエンジンを実装せよ"
    project: auto_allocation_system
    priority: critical
    status: completed
    context: |
      殿より重要なご指摘があった。

      【現状の問題】
      現在のバックテストは「最後の1回だけ」スコアリングして配分を決めている。
      これは真のバックテストではない。

      【あるべき姿】
      例: 2023年のバックテスト
      ```
      2023年1月1日: 2021年〜2022年のデータでスコアリング → 配分決定 → 1日運用
      2023年1月2日: 2021年〜2022年+1/1までのデータでスコアリング → 配分 → 運用
      ...（毎日または毎月繰り返し）
      2023年12月31日: 最終結果集計
      ```

      【要件】
      1. 日次リバランス: 毎日スコアリング＆配分変更
      2. 月次リバランス: 月末にスコアリング＆配分変更（現状相当）
      3. 週次リバランス: オプション
      4. 未来データ漏洩の完全防止
      5. バックテスト期間の指定（例: --start 2023-01-01 --end 2023-12-31）

    tasks:
      - task_id: task_010_1
        description: |
          src/backtest/engine.py を新規作成せよ（バックテストエンジン）

          【クラス設計】
          ```python
          class BacktestConfig:
              start_date: datetime        # バックテスト開始日
              end_date: datetime          # バックテスト終了日
              rebalance_frequency: str    # "daily" | "weekly" | "monthly"
              train_period_days: int      # 学習期間（日数）
              initial_capital: float      # 初期資本
              transaction_cost_bps: float # 取引コスト（bps）

          class BacktestEngine:
              def __init__(self, config: BacktestConfig, settings: Settings):
                  ...

              def run(self, universe: list[str]) -> BacktestResult:
                  """
                  バックテストを実行

                  1. 全期間のデータを一括取得
                  2. リバランス日ごとにループ:
                     a. その日までのデータのみを使用（未来データ排除）
                     b. シグナル生成 & 戦略評価
                     c. 配分決定
                     d. ポートフォリオ更新
                     e. リターン計算
                  3. 結果集計
                  """
                  pass

              def _get_rebalance_dates(self) -> list[datetime]:
                  """リバランス日のリストを生成"""
                  pass

              def _calculate_returns(self, weights: dict, prices: pd.DataFrame) -> pd.Series:
                  """日次リターンを計算"""
                  pass
          ```

          【重要】
          - リバランス日のスコアリングでは、その日までのデータのみを使用
          - 未来のデータは絶対に参照しない
        priority: critical

      - task_id: task_010_2
        description: |
          src/backtest/result.py を新規作成せよ（バックテスト結果）

          【クラス設計】
          ```python
          @dataclass
          class DailySnapshot:
              date: datetime
              weights: dict[str, float]
              portfolio_value: float
              daily_return: float
              cumulative_return: float

          @dataclass
          class BacktestResult:
              config: BacktestConfig
              snapshots: list[DailySnapshot]
              final_value: float
              total_return: float
              annualized_return: float
              sharpe_ratio: float
              sortino_ratio: float
              max_drawdown: float
              calmar_ratio: float
              total_trades: int
              turnover: float
              transaction_costs: float

              def to_dataframe(self) -> pd.DataFrame:
                  """日次データをDataFrameに変換"""
                  pass

              def plot_equity_curve(self, save_path: str = None):
                  """エクイティカーブをプロット"""
                  pass

              def get_monthly_returns(self) -> pd.Series:
                  """月次リターンを取得"""
                  pass

              def get_drawdown_series(self) -> pd.Series:
                  """ドローダウン系列を取得"""
                  pass
          ```
        priority: critical

      - task_id: task_010_3
        description: |
          src/backtest/simulator.py を新規作成せよ（ポートフォリオシミュレーター）

          【クラス設計】
          ```python
          class PortfolioSimulator:
              """ポートフォリオの時系列シミュレーション"""

              def __init__(self, initial_capital: float, transaction_cost_bps: float):
                  self.capital = initial_capital
                  self.cost_bps = transaction_cost_bps
                  self.positions: dict[str, float] = {}  # symbol -> shares
                  self.history: list[DailySnapshot] = []

              def rebalance(self, target_weights: dict[str, float], prices: dict[str, float], date: datetime):
                  """
                  目標配分にリバランス

                  1. 現在のポートフォリオ価値を計算
                  2. 目標配分に必要な取引を計算
                  3. 取引コストを差し引く
                  4. ポジションを更新
                  """
                  pass

              def update_value(self, prices: dict[str, float], date: datetime):
                  """日次の価値更新（リバランスなし）"""
                  pass

              def get_current_weights(self, prices: dict[str, float]) -> dict[str, float]:
                  """現在の配分を取得"""
                  pass

              def get_portfolio_value(self, prices: dict[str, float]) -> float:
                  """現在のポートフォリオ価値を取得"""
                  pass
          ```
        priority: high

      - task_id: task_010_4
        description: |
          main.py を更新してバックテストオプションを拡張せよ

          【追加オプション】
          ```
          --start DATE          バックテスト開始日（例: 2023-01-01）
          --end DATE            バックテスト終了日（例: 2023-12-31）
          --rebalance FREQ      リバランス頻度: daily | weekly | monthly
          --train-days N        学習期間（日数）デフォルト: 504
          --capital N           初期資本（デフォルト: 10000）
          --cost-bps N          取引コスト（bps、デフォルト: 10）
          ```

          【使用例】
          ```bash
          # 2023年の日次リバランスバックテスト
          python -m src.main --backtest \
            --start 2023-01-01 --end 2023-12-31 \
            --rebalance daily \
            --universe SPY,QQQ,TLT,GLD

          # 2022-2023年の月次リバランスバックテスト
          python -m src.main --backtest \
            --start 2022-01-01 --end 2023-12-31 \
            --rebalance monthly \
            --train-days 756 \
            --universe SPY,QQQ,TLT,GLD,EFA,EEM
          ```
        priority: high

      - task_id: task_010_5
        description: |
          バックテストエンジンとパイプラインを統合せよ

          【変更内容】
          1. BacktestEngine から Pipeline を呼び出し
          2. Pipeline に「特定日時点でのスコアリング」モードを追加
             - data_cutoff_date パラメータ
             - この日付以降のデータを使用しない

          【Pipeline への追加】
          ```python
          def run(self, universe: list[str], data_cutoff_date: datetime = None):
              """
              data_cutoff_date が指定された場合、
              その日付までのデータのみを使用してスコアリング
              """
              pass
          ```
        priority: high

      - task_id: task_010_6
        description: |
          テストを実行して動作確認せよ

          【テスト内容】
          1. 日次リバランス（短期）
             ```bash
             python -m src.main --backtest \
               --start 2024-01-01 --end 2024-03-31 \
               --rebalance daily \
               --universe SPY,QQQ,TLT,GLD
             ```

          2. 月次リバランス（1年）
             ```bash
             python -m src.main --backtest \
               --start 2023-01-01 --end 2023-12-31 \
               --rebalance monthly \
               --universe SPY,QQQ,TLT,GLD
             ```

          3. 結果比較
             - 日次 vs 月次のリターン差
             - 取引コストの影響
             - Sharpe Ratio の違い

          【レポート出力】
          results/backtest_comparison_report.json
        priority: medium

    execution_strategy: |
      task_010_1, task_010_2, task_010_3 並行（基盤クラス）
      task_010_4 は task_010_1 完了後
      task_010_5 は task_010_1, task_010_4 完了後
      task_010_6 は task_010_5 完了後

      足軽の割り当て:
      - task_010_1: 足軽1（バックテストエンジン）
      - task_010_2: 足軽2（結果クラス）
      - task_010_3: 足軽3（シミュレーター）
      - task_010_4: 足軽4（CLI拡張）
      - task_010_5: 足軽5（パイプライン統合）
      - task_010_6: 足軽6（テスト実行）

    expected_outcome: |
      【新機能】
      - 日次リバランスバックテスト
      - 月次リバランスバックテスト
      - 週次リバランスバックテスト（オプション）
      - バックテスト期間の自由な指定
      - 未来データ漏洩の完全防止

      【出力例】
      ```
      === Backtest Result ===
      Period: 2023-01-01 ~ 2023-12-31
      Rebalance: daily (252 rebalances)
      Initial: $10,000 → Final: $11,234

      Performance:
        Total Return: +12.34%
        Annualized: +12.34%
        Sharpe Ratio: 1.45
        Max Drawdown: -8.5%
        Calmar Ratio: 1.45

      Trading:
        Total Trades: 1,008
        Turnover: 45.2%
        Transaction Costs: $123.45
      ```

  - id: cmd_011
    timestamp: "2026-01-28T20:10:00"
    command: "バックテスト高速化＆15年長期検証＆パフォーマンスチューニング"
    project: auto_allocation_system
    priority: critical
    status: in_progress
    context: |
      殿より大規模な改善要求があった。

      【要求事項】
      1. 速度改善: 並列実行などでバックテスト高速化
      2. 長期検証: 過去15年の日次/月次バックテスト
      3. ベンチマーク比較: 主要指数（SPY, QQQ等）との比較
      4. パフォーマンスチューニング: 自律的に改善策を考え実行
      5. 新シグナル追加: セクター情報、トレンドフォロー等
      6. 配当対応: 配当金をアセット価値に含める

      【現状の問題】
      - 日次リバランスが非常に遅い（毎回フルパイプライン実行）
      - 15年分のデータ × 日次 = 約3,750回のリバランス
      - 現状のままでは実用的な時間で完了しない

    tasks:
      - task_id: task_011_1
        description: |
          バックテストエンジンの高速化を実装せよ（Phase 1: 並列化）

          【高速化手法】
          1. シグナル計算の並列化
             - 各シグナル（momentum, mean_reversion等）を並列計算
             - concurrent.futures.ProcessPoolExecutor 使用
             - CPU コア数に応じた並列度

          2. 銘柄ごとの並列化
             - 各銘柄のシグナル計算を並列実行
             - ThreadPoolExecutor（I/O bound）または ProcessPoolExecutor（CPU bound）

          3. キャッシュ戦略の強化
             - シグナル計算結果をキャッシュ
             - 日付ごとにインクリメンタル計算（差分のみ計算）
             - メモリキャッシュ + ディスクキャッシュ（Parquet）

          4. ベクトル化計算
             - ループ処理をNumPy/Pandas のベクトル演算に置換
             - Numba JIT コンパイル（可能な箇所）

          【目標】
          - 月次1年: 現状 ~60秒 → 10秒以下
          - 日次1年: 実行可能な時間内に完了
        priority: critical

      - task_id: task_011_2
        description: |
          配当対応を実装せよ

          【実装内容】
          1. YFinance から配当データ取得
             - stock.dividends で配当履歴取得
             - 配当権利落ち日の処理

          2. 調整済み株価の使用
             - adjusted close price を使用（配当・分割調整済み）
             - 配当再投資を仮定したリターン計算

          3. 配当込みリターンの計算
             ```python
             total_return = price_return + dividend_return
             dividend_return = dividend / prev_price
             ```

          4. PortfolioSimulator への統合
             - 配当受取のシミュレーション
             - 配当は現金として計上 or 自動再投資

          【設定オプション】
          ```yaml
          backtest:
            dividend_handling: "reinvest"  # reinvest | cash | ignore
            use_adjusted_prices: true
          ```
        priority: high

      - task_id: task_011_3
        description: |
          セクター情報シグナルを追加せよ

          【新規ファイル】
          src/signals/sector.py

          【実装内容】
          1. SectorMomentum: セクター別モメンタム
             - 各銘柄のセクター分類（GICS）
             - セクターETF（XLK, XLF, XLE等）のモメンタム
             - セクターローテーション戦略

          2. SectorRelativeStrength: セクター内相対強度
             - 同セクター内での銘柄ランキング
             - セクター強者銘柄を優先

          3. SectorBreadth: セクター幅
             - セクター内の上昇/下落銘柄比率
             - セクター全体の健全性指標

          【セクターマッピング】
          ```python
          SECTOR_ETFS = {
              "Technology": "XLK",
              "Financials": "XLF",
              "Energy": "XLE",
              "Healthcare": "XLV",
              "Consumer Discretionary": "XLY",
              "Consumer Staples": "XLP",
              "Industrials": "XLI",
              "Materials": "XLB",
              "Utilities": "XLU",
              "Real Estate": "XLRE",
              "Communication Services": "XLC"
          }
          ```
        priority: high

      - task_id: task_011_4
        description: |
          トレンドフォローシグナルを強化せよ

          【新規/更新ファイル】
          src/signals/trend.py（新規または既存拡張）

          【実装内容】
          1. DualMomentum: デュアルモメンタム
             - 絶対モメンタム（自身の過去との比較）
             - 相対モメンタム（他銘柄との比較）
             - 両方が正の場合のみロング

          2. TrendFollowing: トレンドフォロー
             - 移動平均クロスオーバー（短期/長期）
             - トレンド方向の強さ（ADX）
             - ブレイクアウト検出（Donchian Channel）

          3. AdaptiveTrend: 適応型トレンド
             - ボラティリティに応じたパラメータ調整
             - 高ボラ時は短期パラメータ
             - 低ボラ時は長期パラメータ

          4. CrossAssetMomentum: クロスアセットモメンタム
             - 株式 vs 債券 vs コモディティの相対強度
             - リスクオン/リスクオフの判定
        priority: high

      - task_id: task_011_5
        description: |
          15年長期バックテストを実行せよ（月次）

          【テスト設定】
          - 期間: 2010-01-01 ~ 2024-12-31（15年）
          - リバランス: 月次
          - ユニバース: SPY, QQQ, IWM, EFA, EEM, TLT, GLD, SLV, USO, VNQ
          - 初期資本: $10,000
          - 取引コスト: 10bps

          【ベンチマーク】
          - SPY（S&P 500）
          - 60/40ポートフォリオ（SPY 60%, TLT 40%）

          【比較指標】
          - 累積リターン
          - 年率リターン
          - Sharpe Ratio
          - Sortino Ratio
          - Max Drawdown
          - Calmar Ratio
          - 年別リターン
          - 月別リターン分布

          【レポート出力】
          results/backtest_15year_monthly.json
          results/benchmark_comparison.json
        priority: critical

      - task_id: task_011_6
        description: |
          15年長期バックテストを実行せよ（日次）※高速化後

          【テスト設定】
          - 期間: 2010-01-01 ~ 2024-12-31（15年）
          - リバランス: 日次
          - ユニバース: SPY, QQQ, TLT, GLD（4銘柄で高速化検証）
          - 初期資本: $10,000

          【備考】
          - task_011_1 の高速化完了後に実行
          - 約3,750回のリバランス
          - 並列化効果の検証

          【レポート出力】
          results/backtest_15year_daily.json
        priority: high

      - task_id: task_011_7
        description: |
          パフォーマンス分析＆チューニングを実施せよ

          【分析内容】
          1. 戦略別貢献度分析
             - どのシグナルが最も貢献したか
             - どの期間で各戦略が有効だったか

          2. ドローダウン分析
             - 最大DD発生時期と原因
             - 回復期間

          3. 市場レジーム別分析
             - 上昇相場/下落相場での成績
             - 高ボラ/低ボラ時の成績

          【チューニング項目】
          1. シグナルパラメータ最適化
             - Sharpe最大化のパラメータ探索
             - 過学習回避（クロスバリデーション）

          2. 戦略配分の最適化
             - 各戦略への配分比率
             - レジームに応じた動的配分

          3. リバランス頻度の最適化
             - 日次 vs 週次 vs 月次のトレードオフ
             - 取引コストとのバランス

          【レポート出力】
          results/performance_analysis.json
          results/tuning_recommendations.json
        priority: high

      - task_id: task_011_8
        description: |
          チューニング結果を適用し、再検証せよ

          【手順】
          1. task_011_7 の推奨設定を適用
          2. 同条件で再度バックテスト
          3. 改善効果を測定

          【比較指標】
          - チューニング前 vs チューニング後
          - ベンチマーク比較の再実施

          【最終レポート】
          results/final_performance_report.json
        priority: medium

    execution_strategy: |
      Phase 1（並列）: task_011_1, task_011_2, task_011_3, task_011_4
        - 高速化、配当対応、新シグナル追加を並行実施

      Phase 2: task_011_5
        - 月次15年バックテスト（高速化前でも実行可能）

      Phase 3: task_011_6
        - 日次15年バックテスト（高速化後）

      Phase 4: task_011_7
        - パフォーマンス分析＆チューニング

      Phase 5: task_011_8
        - チューニング適用＆再検証

      足軽の割り当て:
      - task_011_1: 足軽1（高速化）
      - task_011_2: 足軽2（配当対応）
      - task_011_3: 足軽3（セクターシグナル）
      - task_011_4: 足軽4（トレンドフォロー）
      - task_011_5: 足軽5（月次15年BT）
      - task_011_6: 足軽6（日次15年BT）
      - task_011_7: 足軽7（分析＆チューニング）
      - task_011_8: 足軽8（再検証）

    expected_outcome: |
      【高速化目標】
      - 月次1年: 10秒以下
      - 日次15年: 1時間以内

      【パフォーマンス目標】
      - SPY を上回る年率リターン
      - Sharpe Ratio > 1.0
      - Max Drawdown < SPY

      【新機能】
      - 配当込みリターン計算
      - セクターローテーション
      - 適応型トレンドフォロー

      【成果物】
      - 15年バックテスト結果
      - ベンチマーク比較レポート
      - チューニング推奨設定
      - 最終パフォーマンスレポート

  - id: cmd_013
    timestamp: "2026-01-28T22:30:00"
    command: "ベイズ最適化による最高性能モデルを実装せよ"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      殿より、実装コストを度外視し最高パフォーマンスのモデルを実装せよとの命があった。

      【現状のパラメータ最適化】
      - グリッドサーチ（固定グリッド探索）
      - 時系列CV（5-fold, purge gap 5日）
      - OOS ±20%制約

      【ベイズ最適化の優位性】
      - 探索効率: グリッドサーチの1/10以下の試行で同等以上の解を発見
      - 連続パラメータ: 離散グリッドに限らず連続空間を探索
      - 獲得関数: Exploration/Exploitation の自動バランス
      - ガウス過程回帰: 過去の試行から次の探索点を賢く選択

      【目標アーキテクチャ】
      1. 階層的アンサンブル（Trend/Reversion/Macro レイヤー）
      2. レイヤー内スタッキング（XGBoostメタモデル）
      3. レジーム適応重み付け
      4. 適応的ルックバック（市場環境に応じた期間選択）
      5. ベイズ最適化によるハイパーパラメータ調整

      【期待効果】
      - 現状 Sharpe 0.878 → 1.1-1.4 への改善
      - 過学習防止と性能向上の両立

    tasks:
      - task_id: task_013_1
        description: |
          src/meta/bayesian_optimizer.py を新規作成せよ

          【実装内容】
          1. BayesianOptimizer クラス
             - scikit-optimize (skopt) の gp_minimize ラッパー
             - 獲得関数: Expected Improvement (EI)
             - ガウス過程: Matern カーネル

          2. 探索空間の定義
             ```python
             from skopt.space import Real, Integer, Categorical

             SEARCH_SPACE = [
                 Integer(10, 120, name="momentum_lookback"),
                 Integer(5, 30, name="rsi_period"),
                 Integer(10, 60, name="bollinger_period"),
                 Real(0.1, 0.6, name="trend_weight"),
                 Real(0.1, 0.5, name="reversion_weight"),
                 Real(0.1, 0.4, name="macro_weight"),
                 Real(1.0, 4.0, name="beta"),
                 Integer(3, 15, name="top_n"),
                 Real(0.1, 0.3, name="w_asset_max"),
             ]
             ```

          3. 目的関数
             ```python
             def objective(params) -> float:
                 # パラメータ適用
                 self._set_params(params)
                 # 時系列CVでシャープ計算
                 sharpe = self._cross_validate()
                 # 過学習ペナルティ
                 penalty = self._calc_overfitting_penalty()
                 return -(sharpe - 0.1 * penalty)  # 最小化
             ```

          4. 最適化実行
             ```python
             result = gp_minimize(
                 objective,
                 SEARCH_SPACE,
                 n_calls=100,        # 試行回数
                 n_random_starts=20, # 初期ランダム探索
                 acq_func="EI",      # 獲得関数
                 random_state=42,
             )
             ```

          【出力】
          - 最適パラメータ
          - 収束曲線
          - パラメータ重要度
        priority: critical

      - task_id: task_013_2
        description: |
          src/meta/adaptive_lookback.py を新規作成せよ

          【実装内容】
          1. AdaptiveLookback クラス
             - 市場レジームに応じたルックバック期間の動的選択
             - 高ボラ時は短期（速い適応）
             - 低ボラ時は長期（安定重視）

          2. レジーム別設定
             ```python
             CONFIGS = {
                 "bull_trend": {
                     "short": [5, 10, 20],
                     "medium": [20, 40, 60],
                     "long": [60, 120, 252],
                     "decay_weights": [0.5, 0.3, 0.2],
                 },
                 "bear_market": {
                     "short": [3, 5, 10],
                     "medium": [10, 20, 30],
                     "long": [30, 60, 90],
                     "decay_weights": [0.6, 0.3, 0.1],
                 },
                 "high_vol": {
                     "short": [3, 5, 10],
                     "medium": [10, 20, 40],
                     "long": [40, 60, 90],
                     "decay_weights": [0.55, 0.30, 0.15],
                 },
                 "low_vol": {
                     "short": [10, 20, 40],
                     "medium": [40, 60, 90],
                     "long": [90, 120, 252],
                     "decay_weights": [0.25, 0.35, 0.40],
                 },
             }
             ```

          3. マルチピリオドスコアリング
             - 複数期間のシャープを加重平均
             - レジームに応じた重み配分
        priority: critical

      - task_id: task_013_3
        description: |
          src/meta/hierarchical_ensemble.py を新規作成せよ

          【実装内容】
          1. HierarchicalEnsemble クラス
             - 3レイヤー構成:
               - Trend Layer: MultiTimeframe, DualMomentum, AdaptiveTrend, TrendFollowing
               - Reversion Layer: Bollinger, RSI, ZScore, Stochastic
               - Macro Layer: MacroRegimeComposite, FearGreedComposite, CreditSpread, YieldCurve

          2. レイヤー内統合
             - 各シグナルの出力を特徴量として
             - XGBoost または Ridge でメタモデル学習
             - レイヤーごとの統合スコア出力

          3. レイヤー間統合
             - レジーム適応重み付け
             ```python
             layer_weights = {
                 "bull_trend": {"trend": 0.50, "reversion": 0.20, "macro": 0.30},
                 "bear_market": {"trend": 0.20, "reversion": 0.30, "macro": 0.50},
                 "high_vol": {"trend": 0.25, "reversion": 0.25, "macro": 0.50},
                 "low_vol": {"trend": 0.45, "reversion": 0.35, "macro": 0.20},
                 "range": {"trend": 0.25, "reversion": 0.50, "macro": 0.25},
             }
             ```

          4. 最終スコア計算
             ```python
             final_score = (
                 layer_weights[regime]["trend"] * trend_score +
                 layer_weights[regime]["reversion"] * reversion_score +
                 layer_weights[regime]["macro"] * macro_score
             )
             ```
        priority: critical

      - task_id: task_013_4
        description: |
          src/meta/stacking_model.py を新規作成せよ

          【実装内容】
          1. StackingMetaModel クラス
             - レイヤー内のシグナル統合用メタモデル
             - XGBoost または Ridge 回帰

          2. 特徴量エンジニアリング
             - 各シグナルの生スコア
             - シグナル間の交差項
             - ローリング統計量

          3. 学習と推論
             ```python
             class StackingMetaModel:
                 def __init__(self, model_type: str = "xgboost"):
                     if model_type == "xgboost":
                         self.model = XGBRegressor(...)
                     else:
                         self.model = Ridge(...)

                 def fit(self, signal_scores: pd.DataFrame, target: pd.Series):
                     """シグナルスコアから将来リターンを予測するモデルを学習"""
                     pass

                 def predict(self, signal_scores: pd.DataFrame) -> pd.Series:
                     """統合スコアを予測"""
                     pass
             ```

          4. 過学習防止
             - 時系列CV
             - 正則化（L1/L2）
             - 早期停止
        priority: high

      - task_id: task_013_5
        description: |
          src/meta/learner.py を更新して階層的アンサンブルを統合せよ

          【変更内容】
          1. MetaLearner に HierarchicalEnsemble を統合
          2. BayesianOptimizer でパラメータ自動調整
          3. AdaptiveLookback でレジーム適応

          【新しいフロー】
          ```
          データ入力
              ↓
          レジーム検出（RegimeDetector）
              ↓
          適応的ルックバック選択（AdaptiveLookback）
              ↓
          階層的アンサンブル（HierarchicalEnsemble）
           ├─ Trend Layer → スタッキング
           ├─ Reversion Layer → スタッキング
           └─ Macro Layer → スタッキング
              ↓
          レジーム適応レイヤー統合
              ↓
          最終スコア → アセット配分
          ```
        priority: high

      - task_id: task_013_6
        description: |
          ベイズ最適化を実行し、最適パラメータを発見せよ

          【実行設定】
          - n_calls: 100（100回試行）
          - n_random_starts: 20（初期ランダム探索）
          - 時系列CV: 5-fold, purge gap 5日
          - 対象期間: 2015-01-01 ~ 2024-12-31（10年）

          【探索パラメータ】
          - momentum_lookback: [10, 120]
          - rsi_period: [5, 30]
          - bollinger_period: [10, 60]
          - trend_weight: [0.1, 0.6]
          - reversion_weight: [0.1, 0.5]
          - macro_weight: [0.1, 0.4]
          - beta: [1.0, 4.0]
          - top_n: [3, 15]
          - w_asset_max: [0.1, 0.3]

          【出力】
          results/bayesian_optimization_result.json
          - 最適パラメータ
          - 収束曲線
          - 探索履歴
        priority: critical

      - task_id: task_013_7
        description: |
          最適パラメータで15年バックテストを実行せよ

          【テスト設定】
          - 期間: 2010-01-01 ~ 2025-01-01
          - リバランス: 月次
          - 最適化されたパラメータを使用
          - ベンチマーク: SPY, 60/40

          【検証項目】
          - Sharpe Ratio >= 1.0 達成確認
          - MDD < 20% 達成確認
          - SPY超過 70%以上の期間

          【サブ期間分析】
          - 2010-2012: GFC回復期
          - 2013-2017: 低ボラ上昇相場
          - 2018-2019: ボラスパイク
          - 2020-2021: COVID
          - 2022-2025: 金利上昇

          【出力】
          results/hierarchical_ensemble_15year.json
        priority: critical

      - task_id: task_013_8
        description: |
          config/default.yaml に最適パラメータを反映せよ

          【更新内容】
          task_013_6 で発見された最適パラメータを設定ファイルに反映

          ```yaml
          hierarchical_ensemble:
            enabled: true
            layers:
              trend:
                signals: [multi_timeframe_momentum, dual_momentum, adaptive_trend, trend_following]
                stacking_model: "ridge"
              reversion:
                signals: [bollinger, rsi, zscore, stochastic]
                stacking_model: "ridge"
              macro:
                signals: [macro_regime_composite, fear_greed_composite, credit_spread, yield_curve]
                stacking_model: "ridge"

          bayesian_optimization:
            enabled: false  # 初回のみ true で実行
            n_calls: 100
            n_random_starts: 20

          # 最適化されたパラメータ（task_013_6 の結果で更新）
          optimized_params:
            momentum_lookback: XX  # 最適値
            rsi_period: XX
            bollinger_period: XX
            trend_weight: X.XX
            reversion_weight: X.XX
            macro_weight: X.XX
            beta: X.X
            top_n: XX
            w_asset_max: X.XX
          ```
        priority: high

    execution_strategy: |
      Phase 1（並列）: task_013_1, task_013_2, task_013_3, task_013_4
        - ベイズ最適化、適応的ルックバック、階層的アンサンブル、スタッキングモデルを並行実装

      Phase 2: task_013_5
        - MetaLearner への統合

      Phase 3: task_013_6
        - ベイズ最適化実行（100試行）

      Phase 4: task_013_7
        - 最適パラメータで15年バックテスト

      Phase 5: task_013_8
        - 設定ファイルへの反映

      足軽の割り当て:
      - task_013_1: 足軽1（ベイズ最適化）
      - task_013_2: 足軽2（適応的ルックバック）
      - task_013_3: 足軽3（階層的アンサンブル）
      - task_013_4: 足軽4（スタッキングモデル）
      - task_013_5: 足軽5（MetaLearner統合）
      - task_013_6: 足軽6（最適化実行）
      - task_013_7: 足軽7（15年バックテスト）
      - task_013_8: 足軽8（設定反映）

    expected_outcome: |
      【パフォーマンス目標】
      - Sharpe Ratio: 1.1 ~ 1.4（現状 0.878 から大幅改善）
      - MDD: < 20%
      - SPY超過: 70%以上の期間

      【新機能】
      - ベイズ最適化によるハイパーパラメータ自動調整
      - 階層的アンサンブル（3レイヤー）
      - レイヤー内スタッキング（メタモデル）
      - 適応的ルックバック（レジーム対応）

      【成果物】
      - 最適パラメータセット
      - 階層的アンサンブルモデル
      - 15年バックテスト結果
      - 設定ファイル更新

    dependencies:
      - scikit-optimize (skopt)
      - xgboost（オプション）

  - id: cmd_014
    timestamp: "2026-01-28T23:45:00"
    command: "動的パラメータ計算システムを実装せよ"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      殿より、固定値となっているパラメータを過去実績に基づき動的に計算せよとの命があった。
      変数はポートフォリオ毎にも異なるはずである。

      【現状の問題】
      多くのパラメータが固定値（マジックナンバー）となっており、
      市場環境やポートフォリオ特性に適応できていない。

      【目標】
      過去の実績データに基づいて各パラメータを動的に計算し、
      ポートフォリオ毎に最適な値を自動設定する。

    tasks:
      - task_id: task_014_1
        description: |
          src/analysis/dynamic_threshold.py を新規作成せよ

          【DynamicThresholdCalculator クラス】
          過去実績に基づく動的閾値計算

          1. calculate_rebalance_threshold(portfolio_returns, transaction_costs, lookback_days=252)
             - ロジック: ボラティリティ × √保有期間 + 取引コスト × 2（往復）
             - 閾値 = max(min_threshold, vol_threshold)
             - min_threshold = transaction_costs * 2
             - vol_threshold = daily_vol * sqrt(avg_holding_period)

          2. calculate_vix_thresholds(vix_history, lookback_days=252)
             - low: 過去の20パーセンタイル
             - high: 過去の80パーセンタイル
             - extreme: 過去の95パーセンタイル

          3. calculate_correlation_threshold(returns, lookback_days=252)
             - warning: 平均相関 + 1.5 × 標準偏差
             - critical: 平均相関 + 2.5 × 標準偏差
             - baseline: 平均相関

          4. calculate_kelly_params(strategy_returns, lookback_days=252)
             - win_rate: 勝率
             - payoff_ratio: 平均勝ち / 平均負け
             - full_kelly: (win_rate × payoff_ratio - (1 - win_rate)) / payoff_ratio
             - half_kelly: full_kelly × 0.5
             - quarter_kelly: full_kelly × 0.25
        priority: critical

      - task_id: task_014_2
        description: |
          src/analysis/portfolio_specific_params.py を新規作成せよ

          【PortfolioSpecificParams データクラス】
          - portfolio_id: str
          - rebalance_threshold: float
          - vix_thresholds: dict  # low, high, extreme
          - correlation_thresholds: dict  # warning, critical, baseline
          - kelly_params: dict
          - lookback_start: datetime
          - lookback_end: datetime
          - last_updated: datetime
          - update_frequency: str = "monthly"

          【AdaptiveParameterManager クラス】
          - params_cache: dict[str, PortfolioSpecificParams]
          - get_or_update_params(): パラメータ取得（必要に応じて再計算）
          - _needs_update(): 更新が必要か判定
          - save_to_file() / load_from_file(): 永続化
        priority: critical

      - task_id: task_014_3
        description: |
          src/signals/dynamic_params.py を新規作成せよ

          以下のシグナルパラメータを動的計算:

          1. MomentumDynamicParams
             - scale: 過去リターンの標準偏差から計算
             - tanh_scale = 1 / (3 * returns_std)

          2. BollingerDynamicParams
             - num_std: 過去のバンドヒット率から最適化
             - period: ボラティリティレジームに応じて選択

          3. RSIDynamicParams
             - oversold_level: 過去のRSI分布の10パーセンタイル
             - overbought_level: 過去のRSI分布の90パーセンタイル

          4. ZScoreDynamicParams
             - entry_threshold: 過去のZスコア分布から95%信頼区間
        priority: high

      - task_id: task_014_4
        description: |
          src/meta/dynamic_scorer_params.py を新規作成せよ

          ScorerConfig のパラメータを動的計算:

          1. calculate_penalty_coefficients(strategy_metrics, lookback_days=252)
             - penalty_turnover: 過去のターンオーバーとリターンの相関から
             - penalty_mdd: 過去のMDDとリターンの相関から
             - penalty_instability: 勝率のばらつきから

          2. calculate_mdd_normalization(historical_mdd)
             - 過去のMDD分布の75パーセンタイルを基準に

          3. calculate_sharpe_adjustment(market_conditions)
             - 低ボラ環境: sharpe_adjustment > 1.0（高評価）
             - 高ボラ環境: sharpe_adjustment < 1.0（厳しく評価）
        priority: high

      - task_id: task_014_5
        description: |
          src/meta/dynamic_weighter_params.py を新規作成せよ

          WeighterConfig のパラメータを動的計算:

          1. calculate_optimal_beta(strategy_scores, lookback_days=252)
             - 過去のスコア分布から最適な温度パラメータを計算
             - スコアの分散が大きい → beta低め（分散重視）
             - スコアの分散が小さい → beta高め（集中許容）

          2. calculate_w_strategy_max(num_strategies, diversification_target)
             - 戦略数と目標分散度から計算
             - w_max = 1 / (min_effective_n)

          3. calculate_score_threshold(strategy_scores)
             - 過去スコア分布の下位10パーセンタイル
        priority: high

      - task_id: task_014_6
        description: |
          src/allocation/dynamic_allocation_params.py を新規作成せよ

          AllocatorConfig のパラメータを動的計算:

          1. calculate_w_asset_max(num_assets, concentration_limit)
             - 銘柄数と集中度制限から
             - w_max = min(concentration_limit, 3 / num_assets)

          2. calculate_delta_max(historical_volatility, transaction_costs)
             - 過去のボラティリティと取引コストのバランス
             - delta_max = min(base_delta, cost_adjusted_delta)

          3. calculate_smooth_alpha(volatility_regime)
             - 高ボラ時: alpha低め（安定重視）
             - 低ボラ時: alpha高め（追従重視）
        priority: high

      - task_id: task_014_7
        description: |
          src/allocation/dynamic_covariance_params.py を新規作成せよ

          CovarianceConfig のパラメータを動的計算:

          1. calculate_ewma_halflife(market_regime)
             - クライシス時: 短い半減期（速い適応）
             - 通常時: 長い半減期（安定重視）

          2. calculate_correlation_adjustment(historical_correlations)
             - crisis_corr_adjustment: 過去クライシス時の相関上昇率から
             - low_vol_corr_adjustment: 過去低ボラ時の相関低下率から

          3. calculate_regime_thresholds(volatility_history)
             - crisis_vol_threshold: 過去分布から動的に
             - low_vol_threshold: 過去分布から動的に
        priority: high

      - task_id: task_014_8
        description: |
          各モジュールに動的パラメータ計算を統合せよ

          【変更ファイル】
          - src/meta/scorer.py: DynamicScorerParams統合
          - src/meta/weighter.py: DynamicWeighterParams統合
          - src/allocation/allocator.py: DynamicAllocationParams統合
          - src/allocation/covariance.py: DynamicCovarianceParams統合
          - src/signals/momentum.py: MomentumDynamicParams統合
          - src/signals/mean_reversion.py: 各シグナルにDynamicParams統合

          【統合方法】
          ```python
          class StrategyScorer:
              def __init__(self, config=None, use_dynamic=True):
                  if use_dynamic and config is None:
                      self.config = self._compute_dynamic_config()
                  else:
                      self.config = config or ScorerConfig()

              def _compute_dynamic_config(self):
                  # 過去データから動的に計算
                  ...
          ```

          【設定オプション】
          config/default.yaml に追加:
          ```yaml
          dynamic_params:
            enabled: true
            update_frequency: "monthly"
            lookback_days: 252
            min_history_days: 60
          ```
        priority: critical

    execution_strategy: |
      Phase 1（並列）: task_014_1, task_014_2
        - 基盤クラスの実装

      Phase 2（並列）: task_014_3, task_014_4, task_014_5, task_014_6, task_014_7
        - 各モジュール用の動的パラメータ計算

      Phase 3: task_014_8
        - 全モジュールへの統合

      足軽の割り当て:
      - task_014_1: 足軽1（動的閾値計算）
      - task_014_2: 足軽2（ポートフォリオ別パラメータ）
      - task_014_3: 足軽3（シグナル動的パラメータ）
      - task_014_4: 足軽4（スコアラー動的パラメータ）
      - task_014_5: 足軽5（ウェイター動的パラメータ）
      - task_014_6: 足軽6（アロケーター動的パラメータ）
      - task_014_7: 足軽7（共分散動的パラメータ）
      - task_014_8: 足軽8（統合）

    expected_outcome: |
      【動的化される固定値一覧】

      1. ScorerConfig:
         - penalty_turnover: 0.1 → 過去相関から計算
         - penalty_mdd: 0.2 → 過去相関から計算
         - penalty_instability: 0.15 → 勝率ばらつきから計算
         - mdd_normalization_pct: 25.0 → 過去MDD分布から計算

      2. WeighterConfig:
         - beta: 2.0 → スコア分布から計算
         - w_strategy_max: 0.5 → 戦略数と分散目標から計算
         - score_threshold: 0.0 → スコア分布から計算

      3. AllocatorConfig:
         - w_asset_max: 0.2 → 銘柄数と集中度から計算
         - delta_max: 0.05 → ボラと取引コストから計算
         - smooth_alpha: 0.3 → ボラレジームから計算

      4. CovarianceConfig:
         - ewma_halflife: 60 → レジームから計算
         - crisis_corr_adjustment: 0.30 → 過去クライシス相関から計算
         - low_vol_corr_adjustment: -0.15 → 過去低ボラ相関から計算
         - crisis_vol_threshold: 0.80 → ボラ分布から計算
         - low_vol_threshold: 0.25 → ボラ分布から計算

      5. EntropyConfig:
         - entropy_min: 0.8 → 過去多様性とリターンの相関から計算
         - adjustment_strength: 0.5 → 収束速度から計算

      6. SmootherConfig:
         - alpha: 0.3 → ボラレジームから計算
         - min_change_threshold: 0.005 → 取引コストから計算
         - max_single_change: 0.1 → 過去変動から計算

      7. シグナルパラメータ:
         - tanh scale: 固定値 → リターン標準偏差から計算
         - RSI oversold/overbought: 30/70 → 過去RSI分布から計算
         - Bollinger num_std: 2.0 → バンドヒット率から計算

      【期待効果】
      - 市場環境への適応性向上
      - ポートフォリオ特性への最適化
      - 過学習リスクの低減（データドリブン）

  - id: cmd_015
    timestamp: "2026-01-29T00:15:00"
    command: "リターン最大化のための追加最適化を実装せよ"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      殿より、cmd_014の設計は「リスク管理」に偏重しており、
      「リターン最大化」への直接的寄与が不十分との指摘があった。

      【cmd_014の問題点】
      - 期待リターン推定（μ）の動的化が不在
      - スコアリングがリスク偏重（リターンボーナスなし）
      - Kelly係数の活用不足
      - シグナルパラメータのレジーム調整なし
      - パラメータ間の相互依存が未考慮

      【cmd_015の目標】
      リターン最大化の観点から不足している要素を実装し、
      Sharpe Ratio だけでなく絶対リターンも向上させる。

    tasks:
      - task_id: task_015_1
        description: |
          src/allocation/return_estimator.py を新規作成せよ

          【期待リターン推定の動的化 - 最重要】

          1. DynamicReturnEstimator クラス
             複数の期待リターン推定手法を統合し、レジームに応じて選択

          2. 実装する推定手法:

             a. CrossSectionalMomentum（相対モメンタム）
                - 銘柄間の相対パフォーマンスをランキング
                - 上位銘柄に高い期待リターンを付与
                ```python
                def cross_sectional_momentum(prices, lookback=60):
                    returns = prices.pct_change(lookback)
                    ranks = returns.rank(pct=True)  # 0-1にランク
                    # ランクを期待リターンに変換
                    expected_return = (ranks - 0.5) * 2 * base_premium
                    return expected_return
                ```

             b. ImpliedReturns（Black-Litterman的）
                - 市場均衡からの逆算
                ```python
                def implied_returns(weights_market, cov, risk_aversion=2.5):
                    # π = δΣw_mkt
                    pi = risk_aversion * cov @ weights_market
                    return pi
                ```

             c. MeanReversionForecast（平均回帰予測）
                - 長期平均からの乖離を期待リターンに
                ```python
                def mean_reversion_forecast(prices, lookback=252):
                    long_term_mean = prices.rolling(lookback).mean()
                    deviation = (long_term_mean - prices) / prices
                    # 乖離の50%が戻ると仮定
                    expected_return = deviation * 0.5 / (lookback / 252)
                    return expected_return
                ```

             d. FactorPremiumTiming（ファクタープレミアム）
                - バリュー、モメンタム、クオリティのプレミアム推定
                ```python
                def factor_premium(factor_exposures, factor_returns_history):
                    # 過去のファクターリターンから期待値推定
                    expected_factor_returns = factor_returns_history.ewm(
                        halflife=60
                    ).mean().iloc[-1]
                    return factor_exposures @ expected_factor_returns
                ```

          3. レジーム別統合
             ```python
             def estimate(self, prices, regime):
                 if regime == "bull_trend":
                     # モメンタム重視
                     weights = {"cross_sectional": 0.5, "implied": 0.3, "factor": 0.2}
                 elif regime == "range_bound":
                     # 平均回帰重視
                     weights = {"mean_reversion": 0.5, "implied": 0.3, "factor": 0.2}
                 elif regime == "high_vol":
                     # 保守的（implied中心）
                     weights = {"implied": 0.6, "cross_sectional": 0.2, "mean_reversion": 0.2}
                 else:
                     weights = {"implied": 0.4, "cross_sectional": 0.3, "mean_reversion": 0.3}

                 return self._weighted_combine(weights)
             ```
        priority: critical

      - task_id: task_015_2
        description: |
          src/meta/scorer.py を修正して、リターンボーナスを追加せよ

          【現状】
          score = Sharpe_adj - penalty

          【改善後】
          score = Sharpe_adj * (1 + return_bonus) - penalty + alpha_bonus

          【追加要素】

          1. return_bonus（リターンボーナス）
             - 絶対リターンが高い戦略を優遇
             ```python
             def _calculate_return_bonus(self, annualized_return: float) -> float:
                 # 20%リターンで飽和するtanh
                 return np.tanh(annualized_return / 0.20) * self.config.return_bonus_scale
             ```

          2. alpha_bonus（アルファボーナス）
             - ベンチマーク超過リターンを評価
             ```python
             def _calculate_alpha_bonus(self, strategy_return, benchmark_return) -> float:
                 alpha = strategy_return - benchmark_return
                 if alpha > 0:
                     return alpha * self.config.alpha_bonus_scale
                 return 0
             ```

          3. win_streak_bonus（連勝ボーナス）
             - 直近の連続プラスリターンを評価
             ```python
             def _calculate_win_streak_bonus(self, period_returns: list) -> float:
                 streak = 0
                 for r in reversed(period_returns):
                     if r > 0:
                         streak += 1
                     else:
                         break
                 return min(streak * 0.02, 0.10)  # 最大10%ボーナス
             ```

          【ScorerConfig への追加】
          ```python
          return_bonus_scale: float = 0.3  # リターンボーナス係数
          alpha_bonus_scale: float = 0.5   # アルファボーナス係数
          benchmark_ticker: str = "SPY"    # ベンチマーク
          ```
        priority: critical

      - task_id: task_015_3
        description: |
          src/allocation/kelly_allocator.py を新規作成せよ

          【Kelly係数ベースのポジションサイジング】

          1. KellyAllocator クラス
             ```python
             class KellyAllocator:
                 def __init__(self, fraction: float = 0.25, max_weight: float = 0.25):
                     self.fraction = fraction  # Kelly比率（0.25 = Quarter Kelly）
                     self.max_weight = max_weight

                 def calculate_kelly_weight(
                     self,
                     win_rate: float,
                     avg_win: float,
                     avg_loss: float
                 ) -> float:
                     """
                     Kelly公式: f* = (p*b - q) / b
                     where:
                       p = 勝率
                       q = 1 - p = 敗率
                       b = avg_win / avg_loss = ペイオフ比
                     """
                     if avg_loss == 0:
                         return 0

                     b = avg_win / avg_loss  # ペイオフ比
                     q = 1 - win_rate
                     f_star = (win_rate * b - q) / b

                     # 負のKellyは0に
                     f_star = max(0, f_star)

                     # Fractional Kelly
                     f_adjusted = f_star * self.fraction

                     # 上限適用
                     return min(f_adjusted, self.max_weight)
             ```

          2. 戦略別Kelly計算
             ```python
             def calculate_strategy_kelly(
                 self,
                 strategy_returns: pd.Series,
                 lookback_days: int = 252
             ) -> dict:
                 returns = strategy_returns.tail(lookback_days)

                 wins = returns[returns > 0]
                 losses = returns[returns < 0]

                 win_rate = len(wins) / len(returns)
                 avg_win = wins.mean() if len(wins) > 0 else 0
                 avg_loss = abs(losses.mean()) if len(losses) > 0 else 0.01

                 kelly_weight = self.calculate_kelly_weight(win_rate, avg_win, avg_loss)

                 return {
                     "win_rate": win_rate,
                     "avg_win": avg_win,
                     "avg_loss": avg_loss,
                     "payoff_ratio": avg_win / avg_loss if avg_loss > 0 else 0,
                     "full_kelly": kelly_weight / self.fraction,
                     "adjusted_kelly": kelly_weight,
                 }
             ```

          3. ポートフォリオ統合
             ```python
             def allocate(
                 self,
                 strategy_returns: dict[str, pd.Series],
                 base_weights: dict[str, float]
             ) -> dict[str, float]:
                 """
                 基本配分をKelly係数で調整
                 """
                 kelly_weights = {}
                 for strategy_id, returns in strategy_returns.items():
                     kelly_info = self.calculate_strategy_kelly(returns)
                     # 基本配分とKelly配分の加重平均
                     base_w = base_weights.get(strategy_id, 0)
                     kelly_w = kelly_info["adjusted_kelly"]
                     # Kelly比重を50%として統合
                     kelly_weights[strategy_id] = 0.5 * base_w + 0.5 * kelly_w

                 # 正規化
                 total = sum(kelly_weights.values())
                 if total > 0:
                     kelly_weights = {k: v/total for k, v in kelly_weights.items()}

                 return kelly_weights
             ```
        priority: critical

      - task_id: task_015_4
        description: |
          src/signals/regime_adaptive_params.py を新規作成せよ

          【シグナルパラメータのレジーム調整】

          1. REGIME_SIGNAL_PARAMS 定義
             ```python
             REGIME_SIGNAL_PARAMS = {
                 "bull_trend": {
                     # 長期トレンドを追う
                     "momentum": {
                         "lookback": [40, 60, 90],
                         "scale": 3.0,  # 感度低め（過剰反応防止）
                     },
                     "bollinger": {
                         "period": [30, 40],
                         "num_std": [2.5, 3.0],  # 広めのバンド
                     },
                     "rsi": {
                         "period": [21],
                         "oversold": 25,
                         "overbought": 75,
                     },
                 },
                 "bear_market": {
                     # 素早い反応
                     "momentum": {
                         "lookback": [10, 20, 40],
                         "scale": 8.0,  # 感度高め
                     },
                     "bollinger": {
                         "period": [14, 20],
                         "num_std": [1.5, 2.0],  # 狭めのバンド
                     },
                     "rsi": {
                         "period": [10, 14],
                         "oversold": 20,
                         "overbought": 80,
                     },
                 },
                 "range_bound": {
                     # 短期振動を捉える
                     "momentum": {
                         "lookback": [5, 10, 20],
                         "scale": 10.0,
                     },
                     "bollinger": {
                         "period": [14, 20],
                         "num_std": [2.0, 2.5],
                     },
                     "rsi": {
                         "period": [14],
                         "oversold": 30,
                         "overbought": 70,
                     },
                 },
                 "high_vol": {
                     # 慎重な設定
                     "momentum": {
                         "lookback": [20, 40],
                         "scale": 2.0,  # 感度最低
                     },
                     "bollinger": {
                         "period": [20, 30],
                         "num_std": [2.5, 3.0],
                     },
                     "rsi": {
                         "period": [14, 21],
                         "oversold": 20,
                         "overbought": 80,
                     },
                 },
                 "low_vol": {
                     # 積極的な設定
                     "momentum": {
                         "lookback": [20, 40, 60],
                         "scale": 6.0,
                     },
                     "bollinger": {
                         "period": [20],
                         "num_std": [1.5, 2.0],  # タイトなバンド
                     },
                     "rsi": {
                         "period": [14],
                         "oversold": 35,
                         "overbought": 65,  # 狭いレンジで反応
                     },
                 },
             }
             ```

          2. RegimeAdaptiveSignalFactory クラス
             ```python
             class RegimeAdaptiveSignalFactory:
                 def create_signals(self, regime: str) -> list[Signal]:
                     params = REGIME_SIGNAL_PARAMS.get(regime, REGIME_SIGNAL_PARAMS["range_bound"])

                     signals = []
                     # モメンタム
                     for lb in params["momentum"]["lookback"]:
                         signals.append(MomentumReturnSignal(
                             lookback=lb,
                             scale=params["momentum"]["scale"]
                         ))
                     # ボリンジャー
                     for period in params["bollinger"]["period"]:
                         for std in params["bollinger"]["num_std"]:
                             signals.append(BollingerReversionSignal(
                                 period=period,
                                 num_std=std
                             ))
                     # RSI
                     for period in params["rsi"]["period"]:
                         signals.append(RSISignal(
                             period=period,
                             oversold_level=params["rsi"]["oversold"],
                             overbought_level=params["rsi"]["overbought"]
                         ))

                     return signals
             ```
        priority: high

      - task_id: task_015_5
        description: |
          src/meta/param_consistency.py を新規作成せよ

          【パラメータ間整合性制約】

          1. ParameterConsistencyChecker クラス
             ```python
             class ParameterConsistencyChecker:
                 def check_and_adjust(self, params: dict) -> dict:
                     adjusted = params.copy()

                     # 1. top_n と w_asset_max の整合性
                     # top_n * w_asset_max >= 1.0 が必要
                     if params["top_n"] * params["w_asset_max"] < 1.0:
                         # w_asset_max を調整
                         adjusted["w_asset_max"] = 1.0 / params["top_n"] + 0.05
                         logger.warning(
                             f"Adjusted w_asset_max: {params['w_asset_max']} -> {adjusted['w_asset_max']}"
                         )

                     # 2. smooth_alpha と delta_max の整合性
                     # 両方が効くと調整が過度に抑制される
                     effective_change = params["smooth_alpha"] * params["delta_max"]
                     if effective_change < 0.01:
                         # 最低1%の変更は許容
                         adjusted["smooth_alpha"] = max(params["smooth_alpha"], 0.01 / params["delta_max"])

                     # 3. beta と entropy_min の整合性
                     # 高beta（集中）と高entropy_min（分散）は矛盾
                     if params["beta"] > 3.0 and params["entropy_min"] > 0.8:
                         # betaを下げるか、entropy_minを下げる
                         adjusted["beta"] = min(params["beta"], 2.5)
                         logger.warning(
                             f"Adjusted beta for entropy consistency: {params['beta']} -> {adjusted['beta']}"
                         )

                     # 4. 戦略数と最小配分の整合性
                     if params.get("min_active_strategies", 2) > params["top_n"]:
                         adjusted["min_active_strategies"] = params["top_n"]

                     return adjusted

                 def validate(self, params: dict) -> list[str]:
                     """矛盾点をリストで返す"""
                     issues = []

                     if params["top_n"] * params["w_asset_max"] < 1.0:
                         issues.append(
                             f"top_n({params['top_n']}) * w_asset_max({params['w_asset_max']}) < 1.0"
                         )

                     if params["beta"] > 3.0 and params["entropy_min"] > 0.8:
                         issues.append(
                             f"High beta({params['beta']}) conflicts with high entropy_min({params['entropy_min']})"
                         )

                     return issues
             ```

          2. 自動調整ロジック
             ```python
             def auto_adjust_for_return_maximization(self, params: dict) -> dict:
                 """リターン最大化のための自動調整"""
                 adjusted = params.copy()

                 # リターン重視モードでは集中を許容
                 if params.get("optimization_target") == "return":
                     adjusted["w_asset_max"] = min(0.30, params["w_asset_max"] * 1.5)
                     adjusted["entropy_min"] = max(0.6, params["entropy_min"] - 0.15)
                     adjusted["beta"] = min(4.0, params["beta"] * 1.3)

                 return adjusted
             ```
        priority: high

      - task_id: task_015_6
        description: |
          src/strategy/entry_exit_optimizer.py を新規作成せよ

          【エントリー/エグジット最適化】

          1. HysteresisFilter クラス
             - ヒステリシス付きのシグナルフィルター
             ```python
             class HysteresisFilter:
                 def __init__(
                     self,
                     entry_threshold: float = 0.3,
                     exit_threshold: float = 0.1,
                     min_holding_periods: int = 5
                 ):
                     self.entry_threshold = entry_threshold
                     self.exit_threshold = exit_threshold
                     self.min_holding_periods = min_holding_periods
                     self.positions = {}  # asset -> (is_active, periods_held)

                 def filter_signal(
                     self,
                     asset: str,
                     raw_score: float
                 ) -> float:
                     """
                     ヒステリシスフィルタ適用

                     - ポジションなし: entry_thresholdを超えたらエントリー
                     - ポジションあり: exit_thresholdを下回ったらエグジット
                     - 最低保有期間: min_holding_periods
                     """
                     is_active, periods_held = self.positions.get(asset, (False, 0))

                     if not is_active:
                         # エントリー判定
                         if raw_score >= self.entry_threshold:
                             self.positions[asset] = (True, 1)
                             return raw_score
                         else:
                             return 0.0
                     else:
                         # 最低保有期間チェック
                         if periods_held < self.min_holding_periods:
                             self.positions[asset] = (True, periods_held + 1)
                             return raw_score

                         # エグジット判定
                         if raw_score < self.exit_threshold:
                             self.positions[asset] = (False, 0)
                             return 0.0
                         else:
                             self.positions[asset] = (True, periods_held + 1)
                             return raw_score
             ```

          2. DynamicThresholdCalculator（エントリー閾値の動的計算）
             ```python
             def calculate_dynamic_entry_threshold(
                 self,
                 scores_history: pd.Series,
                 vix_current: float,
                 lookback: int = 60
             ) -> float:
                 """
                 動的エントリー閾値

                 - VIX高い → 閾値上げ（厳選）
                 - VIX低い → 閾値下げ（積極的）
                 - スコア分布から基準を設定
                 """
                 base_threshold = scores_history.tail(lookback).quantile(0.6)

                 # VIX調整
                 vix_adjustment = (vix_current - 20) * 0.01
                 adjusted_threshold = base_threshold + vix_adjustment

                 return np.clip(adjusted_threshold, 0.1, 0.5)
             ```

          3. SignalDecayFunction（シグナル減衰）
             ```python
             def apply_signal_decay(
                 self,
                 signal_value: float,
                 signal_age_days: int,
                 halflife: int = 5
             ) -> float:
                 """
                 シグナル発生からの経過時間で減衰

                 - 新しいシグナルは強い
                 - 古いシグナルは弱くなる
                 """
                 decay_factor = 0.5 ** (signal_age_days / halflife)
                 return signal_value * decay_factor
             ```
        priority: high

      - task_id: task_015_7
        description: |
          src/signals/macro_timing.py を新規作成せよ

          【マクロ経済指標連動】

          1. EconomicCycleAllocator クラス
             ```python
             class EconomicCycleAllocator:
                 """
                 経済サイクルに基づくセクターアロケーション

                 フェーズ:
                 - Early Expansion: Technology, Consumer Discretionary
                 - Mid Expansion: Industrials, Materials
                 - Late Expansion: Energy, Commodities
                 - Recession: Utilities, Consumer Staples, Healthcare
                 """

                 CYCLE_ALLOCATIONS = {
                     "early_expansion": {
                         "XLK": 0.25, "XLY": 0.20, "XLF": 0.15,
                         "XLI": 0.10, "others": 0.30
                     },
                     "mid_expansion": {
                         "XLI": 0.20, "XLB": 0.15, "XLK": 0.15,
                         "XLF": 0.15, "others": 0.35
                     },
                     "late_expansion": {
                         "XLE": 0.20, "XLB": 0.15, "GLD": 0.15,
                         "XLI": 0.10, "others": 0.40
                     },
                     "recession": {
                         "XLU": 0.20, "XLP": 0.20, "XLV": 0.15,
                         "TLT": 0.20, "others": 0.25
                     },
                 }

                 def detect_cycle_phase(
                     self,
                     ism_pmi: float,
                     unemployment_change: float,
                     yield_curve_slope: float
                 ) -> str:
                     """
                     経済指標からサイクルフェーズを判定

                     - ISM PMI > 55, 失業率低下 → Early/Mid Expansion
                     - ISM PMI > 50, 横ばい → Late Expansion
                     - ISM PMI < 50, 失業率上昇 → Recession
                     - イールドカーブ逆転 → Recession警戒
                     """
                     if yield_curve_slope < 0:
                         return "recession"
                     if ism_pmi < 50 or unemployment_change > 0.5:
                         return "recession"
                     if ism_pmi > 55 and unemployment_change < -0.2:
                         return "early_expansion"
                     if ism_pmi > 52:
                         return "mid_expansion"
                     return "late_expansion"
             ```

          2. InflationAdjuster クラス
             ```python
             class InflationAdjuster:
                 """
                 インフレ環境に応じたアロケーション調整

                 - 高インフレ: コモディティ、TIPS、不動産増
                 - 低インフレ/デフレ: 成長株、長期債増
                 """

                 def adjust_for_inflation(
                     self,
                     base_weights: dict,
                     inflation_rate: float,
                     inflation_expectation: float
                 ) -> dict:
                     adjusted = base_weights.copy()

                     if inflation_rate > 4.0 or inflation_expectation > 3.5:
                         # 高インフレ対応
                         inflation_hedges = ["GLD", "GSG", "TIP", "VNQ"]
                         for asset in inflation_hedges:
                             if asset in adjusted:
                                 adjusted[asset] *= 1.3

                     elif inflation_rate < 1.5 and inflation_expectation < 2.0:
                         # 低インフレ/デフレ対応
                         growth_assets = ["QQQ", "XLK", "TLT"]
                         for asset in growth_assets:
                             if asset in adjusted:
                                 adjusted[asset] *= 1.2

                     # 正規化
                     total = sum(adjusted.values())
                     return {k: v/total for k, v in adjusted.items()}
             ```

          3. RealRateAllocator クラス
             ```python
             class RealRateAllocator:
                 """
                 実質金利に基づくリスク資産配分

                 - 実質金利マイナス → リスク資産増（現金の価値低下）
                 - 実質金利プラス高 → リスク資産減（債券魅力増）
                 """

                 def calculate_risk_asset_target(
                     self,
                     nominal_rate: float,
                     inflation_expectation: float,
                     base_risk_allocation: float = 0.7
                 ) -> float:
                     real_rate = nominal_rate - inflation_expectation

                     if real_rate < -1.0:
                         # 強いマイナス金利 → リスク資産増
                         adjustment = 0.15
                     elif real_rate < 0:
                         # 軽度マイナス → やや増
                         adjustment = 0.05
                     elif real_rate > 2.0:
                         # 高実質金利 → リスク資産減
                         adjustment = -0.15
                     else:
                         adjustment = 0

                     return np.clip(base_risk_allocation + adjustment, 0.4, 0.9)
             ```
        priority: high

      - task_id: task_015_8
        description: |
          各モジュールにリターン最大化機能を統合せよ

          【統合対象】

          1. src/orchestrator/pipeline.py
             - DynamicReturnEstimator統合
             - KellyAllocator統合
             - RegimeAdaptiveSignalFactory統合
             - HysteresisFilter統合

          2. src/meta/learner.py
             - スコアリングのリターンボーナス統合
             - ParameterConsistencyChecker統合

          3. src/allocation/allocator.py
             - 期待リターン推定の差し替え
             - Kelly配分の統合オプション

          4. config/default.yaml 追加設定
             ```yaml
             return_maximization:
               enabled: true

               # 期待リターン推定
               return_estimation:
                 method: "dynamic"  # dynamic, momentum, implied
                 regime_adaptive: true

               # スコアリング
               scoring:
                 return_bonus_scale: 0.3
                 alpha_bonus_scale: 0.5
                 benchmark: "SPY"

               # Kelly配分
               kelly:
                 enabled: true
                 fraction: 0.25  # Quarter Kelly
                 weight_in_final: 0.5  # 最終配分への寄与度

               # エントリー/エグジット
               entry_exit:
                 use_hysteresis: true
                 entry_threshold: 0.3
                 exit_threshold: 0.1
                 min_holding_periods: 5
                 signal_decay_halflife: 5

               # マクロ経済連動
               macro_timing:
                 enabled: true
                 cycle_allocation_weight: 0.3
                 inflation_adjustment: true
                 real_rate_adjustment: true

               # パラメータ整合性
               param_consistency:
                 auto_adjust: true
                 optimization_target: "sharpe"  # sharpe, return, risk
             ```

          【パイプラインフロー更新】
          ```
          データ取得
              ↓
          レジーム検出
              ↓
          レジーム適応シグナル生成（task_015_4）
              ↓
          期待リターン推定（task_015_1）
              ↓
          戦略評価 + リターンボーナス（task_015_2）
              ↓
          Kelly配分計算（task_015_3）
              ↓
          パラメータ整合性チェック（task_015_5）
              ↓
          エントリー/エグジットフィルター（task_015_6）
              ↓
          マクロ経済調整（task_015_7）
              ↓
          最終配分出力
          ```
        priority: critical

    execution_strategy: |
      Phase 1（並列・最重要）: task_015_1, task_015_2, task_015_3
        - 期待リターン推定、リターンボーナス、Kelly配分

      Phase 2（並列）: task_015_4, task_015_5, task_015_6
        - レジーム適応シグナル、整合性チェック、エントリー最適化

      Phase 3: task_015_7
        - マクロ経済連動

      Phase 4: task_015_8
        - 全機能統合

      足軽の割り当て:
      - task_015_1: 足軽1（期待リターン推定）
      - task_015_2: 足軽2（スコアリング改善）
      - task_015_3: 足軽3（Kelly配分）
      - task_015_4: 足軽4（レジーム適応シグナル）
      - task_015_5: 足軽5（パラメータ整合性）
      - task_015_6: 足軽6（エントリー/エグジット）
      - task_015_7: 足軽7（マクロ経済連動）
      - task_015_8: 足軽8（統合）

    expected_outcome: |
      【リターン向上の期待効果】

      | 施策 | 期待効果 | 根拠 |
      |------|----------|------|
      | 期待リターン推定の動的化 | +2-5%/年 | μの推定精度向上 |
      | リターンボーナス追加 | +1-2%/年 | 高リターン戦略の選択 |
      | Kelly配分 | +1-3%/年 | 最適ポジションサイジング |
      | レジーム適応シグナル | +1-2%/年 | 環境適応度向上 |
      | エントリー最適化 | +0.5-1%/年 | 取引タイミング改善 |
      | マクロ経済連動 | +0.5-1.5%/年 | 長期サイクル対応 |

      【合計期待効果】
      - 追加年率リターン: +6-14%/年
      - Sharpe改善: +0.2-0.5

      【リスク】
      - 過学習リスク: CVで検証必須
      - 複雑性増加: モジュール化で対応
      - 計算コスト増: 並列化で対応

      【検証方法】
      1. 15年バックテストで効果測定
      2. 各施策のon/off比較
      3. サブ期間分析で安定性確認

  - id: cmd_016
    timestamp: "2026-01-29T01:30:00"
    command: "無料データソースによる精度向上戦略を全て実装せよ"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      殿より、無料で実装可能な全ての精度向上戦略を実装せよとの命があった。
      データソースはyfinance、FRED API、CBOE VIXのみ（全て無料）。

      【目標】
      - Sharpe: 0.878 → 1.2-1.5
      - 年率リターン: 21.77% → 28-35%
      - MDD: 15-18%以下

      【実装カテゴリ】
      1. シグナル・戦略の強化（5施策）
      2. メタ層・重み付けの改善（4施策）※cmd_015と統合
      3. リスク管理・テールリスク（3施策）
      4. 動的パラメータ計算（4施策）※cmd_014と統合
      5. レジーム検出・適応（3施策）
      6. エントリー/エグジット最適化（3施策）

    tasks:
      # =========================================================================
      # カテゴリ1: シグナル・戦略の強化
      # =========================================================================
      - task_id: task_016_1
        description: |
          src/strategy/pairs_trading.py を新規作成せよ

          【ペアトレーディング（共和分ベース）】

          1. CointegrationPairsFinder クラス
             ```python
             class CointegrationPairsFinder:
                 """共和分ペアの発見"""

                 def find_cointegrated_pairs(
                     self,
                     prices: pd.DataFrame,
                     significance_level: float = 0.05
                 ) -> list[tuple[str, str, float]]:
                     """
                     Engle-Granger検定で共和分ペアを発見

                     Returns:
                         List of (asset1, asset2, p_value)
                     """
                     from statsmodels.tsa.stattools import coint

                     pairs = []
                     assets = prices.columns.tolist()

                     for i, asset1 in enumerate(assets):
                         for asset2 in assets[i+1:]:
                             score, pvalue, _ = coint(
                                 prices[asset1].dropna(),
                                 prices[asset2].dropna()
                             )
                             if pvalue < significance_level:
                                 pairs.append((asset1, asset2, pvalue))

                     return sorted(pairs, key=lambda x: x[2])
             ```

          2. PairsTrader クラス
             ```python
             class PairsTrader:
                 """ペアトレーディング戦略"""

                 def __init__(
                     self,
                     entry_zscore: float = 2.0,
                     exit_zscore: float = 0.5,
                     lookback: int = 60,
                     max_holding_days: int = 20
                 ):
                     self.entry_zscore = entry_zscore
                     self.exit_zscore = exit_zscore
                     self.lookback = lookback
                     self.max_holding_days = max_holding_days

                 def compute_spread(
                     self,
                     prices1: pd.Series,
                     prices2: pd.Series
                 ) -> pd.Series:
                     """ヘッジ比率を計算してスプレッドを算出"""
                     from sklearn.linear_model import LinearRegression

                     model = LinearRegression()
                     X = prices2.values.reshape(-1, 1)
                     y = prices1.values
                     model.fit(X, y)

                     hedge_ratio = model.coef_[0]
                     spread = prices1 - hedge_ratio * prices2
                     return spread, hedge_ratio

                 def generate_signals(
                     self,
                     spread: pd.Series
                 ) -> pd.Series:
                     """Zスコアベースのシグナル生成"""
                     zscore = (spread - spread.rolling(self.lookback).mean()) / \
                              spread.rolling(self.lookback).std()

                     signals = pd.Series(0.0, index=spread.index)
                     # Zスコア > entry: ショートスプレッド (-1)
                     signals[zscore > self.entry_zscore] = -1.0
                     # Zスコア < -entry: ロングスプレッド (+1)
                     signals[zscore < -self.entry_zscore] = 1.0
                     # |Zスコア| < exit: クローズ (0)
                     signals[abs(zscore) < self.exit_zscore] = 0.0

                     return signals
             ```

          3. 推奨ペア（ETF）
             - SPY/IVV, QQQ/TQQQ, TLT/IEF, GLD/GDX
             - XLF/KBE, XLE/OIH, VNQ/IYR
        priority: high

      - task_id: task_016_2
        description: |
          src/signals/cross_asset.py を新規作成せよ

          【クロスアセットモメンタム】

          1. CrossAssetMomentumRanker クラス
             ```python
             class CrossAssetMomentumRanker:
                 """アセットクラス間の相対モメンタム"""

                 ASSET_CLASSES = {
                     "us_equity": ["SPY", "QQQ", "IWM"],
                     "intl_equity": ["EFA", "EEM", "VWO"],
                     "bonds": ["TLT", "IEF", "LQD", "HYG"],
                     "commodities": ["GLD", "SLV", "USO", "DBA"],
                     "real_estate": ["VNQ", "IYR"],
                     "cash": ["SHY", "BIL"],
                 }

                 def rank_asset_classes(
                     self,
                     prices: pd.DataFrame,
                     lookback: int = 60
                 ) -> dict[str, float]:
                     """
                     アセットクラス毎のモメンタムスコアを計算

                     Returns:
                         {"us_equity": 0.8, "bonds": -0.2, ...}
                     """
                     scores = {}
                     for asset_class, tickers in self.ASSET_CLASSES.items():
                         available = [t for t in tickers if t in prices.columns]
                         if not available:
                             continue

                         # クラス平均リターン
                         returns = prices[available].pct_change(lookback).iloc[-1].mean()
                         # ランクスコアに変換
                         scores[asset_class] = returns

                     # クロスセクショナルランク（-1 to +1）
                     if scores:
                         values = list(scores.values())
                         min_v, max_v = min(values), max(values)
                         if max_v > min_v:
                             scores = {k: 2 * (v - min_v) / (max_v - min_v) - 1
                                       for k, v in scores.items()}

                     return scores

                 def generate_allocation_adjustment(
                     self,
                     base_weights: dict[str, float],
                     class_scores: dict[str, float],
                     adjustment_strength: float = 0.3
                 ) -> dict[str, float]:
                     """
                     モメンタムスコアに基づいて配分を調整

                     上位クラス: +adjustment_strength
                     下位クラス: -adjustment_strength
                     """
                     adjusted = base_weights.copy()

                     for asset, weight in base_weights.items():
                         # アセットがどのクラスに属するか判定
                         for asset_class, tickers in self.ASSET_CLASSES.items():
                             if asset in tickers:
                                 score = class_scores.get(asset_class, 0)
                                 adjustment = 1 + score * adjustment_strength
                                 adjusted[asset] = weight * adjustment
                                 break

                     # 正規化
                     total = sum(adjusted.values())
                     if total > 0:
                         adjusted = {k: v / total for k, v in adjusted.items()}

                     return adjusted
             ```
        priority: high

      - task_id: task_016_3
        description: |
          src/strategy/sector_rotation.py を新規作成せよ

          【セクターローテーション強化】

          1. EconomicCycleSectorRotator クラス
             ```python
             class EconomicCycleSectorRotator:
                 """経済サイクルに基づくセクターローテーション"""

                 # セクターETFマッピング
                 SECTOR_ETFS = {
                     "technology": "XLK",
                     "consumer_discretionary": "XLY",
                     "financials": "XLF",
                     "industrials": "XLI",
                     "materials": "XLB",
                     "energy": "XLE",
                     "utilities": "XLU",
                     "consumer_staples": "XLP",
                     "healthcare": "XLV",
                     "real_estate": "XLRE",
                     "communication": "XLC",
                 }

                 # 経済サイクル別の推奨セクター
                 CYCLE_SECTORS = {
                     "early_recovery": {
                         "overweight": ["XLY", "XLF", "XLI", "XLB"],
                         "underweight": ["XLU", "XLP"],
                     },
                     "mid_expansion": {
                         "overweight": ["XLK", "XLI", "XLB"],
                         "underweight": ["XLU", "XLP", "XLRE"],
                     },
                     "late_expansion": {
                         "overweight": ["XLE", "XLB", "XLV"],
                         "underweight": ["XLY", "XLF"],
                     },
                     "recession": {
                         "overweight": ["XLU", "XLP", "XLV"],
                         "underweight": ["XLY", "XLF", "XLI", "XLB"],
                     },
                 }

                 def detect_economic_phase(
                     self,
                     yield_curve_slope: float,  # 10Y - 2Y
                     ism_pmi: float,
                     unemployment_rate_change: float,
                     credit_spread: float  # HYG-LQD spread
                 ) -> str:
                     """経済指標からサイクルフェーズを判定"""

                     # イールドカーブ逆転 → リセッション警戒
                     if yield_curve_slope < -0.2:
                         return "recession"

                     # ISMベースの判定
                     if ism_pmi < 48:
                         return "recession"
                     elif ism_pmi < 52 and unemployment_rate_change > 0:
                         return "late_expansion"
                     elif ism_pmi > 55 and unemployment_rate_change < 0:
                         return "early_recovery"
                     else:
                         return "mid_expansion"

                 def get_sector_adjustments(
                     self,
                     phase: str,
                     adjustment_pct: float = 0.20
                 ) -> dict[str, float]:
                     """
                     セクター調整係数を取得

                     Returns:
                         {"XLK": 1.2, "XLU": 0.8, ...}
                     """
                     adjustments = {etf: 1.0 for etf in self.SECTOR_ETFS.values()}

                     phase_config = self.CYCLE_SECTORS.get(phase, {})

                     for etf in phase_config.get("overweight", []):
                         adjustments[etf] = 1 + adjustment_pct

                     for etf in phase_config.get("underweight", []):
                         adjustments[etf] = 1 - adjustment_pct

                     return adjustments
             ```

          2. FRED APIからの経済指標取得
             ```python
             def fetch_economic_indicators() -> dict:
                 """FREDから経済指標を取得"""
                 from fredapi import Fred
                 import os

                 fred = Fred(api_key=os.getenv("FRED_API_KEY"))

                 indicators = {
                     "yield_curve": fred.get_series("T10Y2Y"),  # 10Y-2Y spread
                     "ism_pmi": fred.get_series("MANEMP"),  # Manufacturing Employment
                     "unemployment": fred.get_series("UNRATE"),
                     "cpi": fred.get_series("CPIAUCSL"),
                 }

                 return indicators
             ```
        priority: high

      - task_id: task_016_4
        description: |
          src/signals/dual_momentum_enhanced.py を新規作成せよ

          【デュアルモメンタム強化】

          1. EnhancedDualMomentum クラス
             ```python
             class EnhancedDualMomentum:
                 """強化版デュアルモメンタム戦略"""

                 def __init__(
                     self,
                     abs_lookbacks: list[int] = [60, 120, 252],
                     rel_lookback: int = 60,
                     abs_weights: list[float] = [0.25, 0.35, 0.40],
                     safe_asset: str = "BIL"
                 ):
                     self.abs_lookbacks = abs_lookbacks
                     self.rel_lookback = rel_lookback
                     self.abs_weights = abs_weights
                     self.safe_asset = safe_asset

                 def compute_absolute_momentum(
                     self,
                     prices: pd.Series
                 ) -> float:
                     """
                     複数期間の絶対モメンタム

                     各期間のリターンを加重平均
                     """
                     scores = []
                     for lookback, weight in zip(self.abs_lookbacks, self.abs_weights):
                         if len(prices) >= lookback:
                             ret = prices.iloc[-1] / prices.iloc[-lookback] - 1
                             scores.append(ret * weight)

                     return sum(scores) if scores else 0

                 def compute_relative_momentum(
                     self,
                     prices: pd.DataFrame,
                     target_assets: list[str]
                 ) -> dict[str, float]:
                     """
                     相対モメンタム（クロスセクショナルランク）
                     """
                     returns = {}
                     for asset in target_assets:
                         if asset in prices.columns:
                             ret = prices[asset].iloc[-1] / prices[asset].iloc[-self.rel_lookback] - 1
                             returns[asset] = ret

                     # ランク変換
                     if returns:
                         sorted_assets = sorted(returns.keys(), key=lambda x: returns[x], reverse=True)
                         n = len(sorted_assets)
                         ranks = {asset: (n - i - 1) / (n - 1) * 2 - 1
                                  for i, asset in enumerate(sorted_assets)}
                         return ranks

                     return {}

                 def generate_signals(
                     self,
                     prices: pd.DataFrame,
                     target_assets: list[str]
                 ) -> dict[str, float]:
                     """
                     デュアルモメンタムシグナル生成

                     1. 絶対モメンタム > 0 のアセットのみ対象
                     2. 相対モメンタムでランキング
                     3. 上位に高配分、絶対モメンタム負は安全資産へ
                     """
                     signals = {}

                     # 絶対モメンタム計算
                     abs_mom = {}
                     for asset in target_assets:
                         if asset in prices.columns:
                             abs_mom[asset] = self.compute_absolute_momentum(prices[asset])

                     # 相対モメンタム計算
                     rel_mom = self.compute_relative_momentum(prices, target_assets)

                     # シグナル生成
                     for asset in target_assets:
                         if asset not in abs_mom:
                             continue

                         if abs_mom[asset] > 0:
                             # 絶対モメンタム正 → 相対モメンタムでスコア
                             signals[asset] = rel_mom.get(asset, 0) * (1 + abs_mom[asset])
                         else:
                             # 絶対モメンタム負 → シグナル0（安全資産へ）
                             signals[asset] = 0

                     # 安全資産シグナル
                     total_signal = sum(max(0, s) for s in signals.values())
                     if total_signal < 0.5:
                         signals[self.safe_asset] = 1 - total_signal

                     return signals
             ```
        priority: high

      - task_id: task_016_5
        description: |
          src/signals/low_vol_premium.py を新規作成せよ

          【ボラティリティ・プレミアム戦略】

          1. LowVolatilityPremium クラス
             ```python
             class LowVolatilityPremium:
                 """低ボラティリティアノマリーの活用"""

                 def __init__(
                     self,
                     vol_lookback: int = 60,
                     rebalance_frequency: int = 20,
                     target_percentile: float = 0.3  # 下位30%を選択
                 ):
                     self.vol_lookback = vol_lookback
                     self.rebalance_frequency = rebalance_frequency
                     self.target_percentile = target_percentile

                 def compute_volatility_scores(
                     self,
                     prices: pd.DataFrame
                 ) -> pd.Series:
                     """
                     各アセットのボラティリティスコアを計算

                     低ボラ = 高スコア（+1に近い）
                     """
                     returns = prices.pct_change()
                     volatility = returns.rolling(self.vol_lookback).std().iloc[-1] * np.sqrt(252)

                     # 逆ランク（低ボラが高スコア）
                     vol_rank = volatility.rank(ascending=True, pct=True)

                     # [-1, +1]にスケール（低ボラ=+1）
                     scores = (vol_rank - 0.5) * 2

                     return scores

                 def select_low_vol_assets(
                     self,
                     prices: pd.DataFrame,
                     universe: list[str]
                 ) -> list[str]:
                     """
                     低ボラ銘柄を選択
                     """
                     available = [a for a in universe if a in prices.columns]
                     returns = prices[available].pct_change()
                     volatility = returns.rolling(self.vol_lookback).std().iloc[-1] * np.sqrt(252)

                     threshold = volatility.quantile(self.target_percentile)
                     low_vol_assets = volatility[volatility <= threshold].index.tolist()

                     return low_vol_assets

                 def compute_vol_adjusted_weights(
                     self,
                     prices: pd.DataFrame,
                     base_weights: dict[str, float],
                     adjustment_strength: float = 0.3
                 ) -> dict[str, float]:
                     """
                     ボラティリティに応じた配分調整

                     低ボラ銘柄 → 配分増加
                     高ボラ銘柄 → 配分減少
                     """
                     vol_scores = self.compute_volatility_scores(prices)

                     adjusted = {}
                     for asset, weight in base_weights.items():
                         score = vol_scores.get(asset, 0)
                         # スコアに応じて調整
                         adjustment = 1 + score * adjustment_strength
                         adjusted[asset] = weight * max(0.5, adjustment)

                     # 正規化
                     total = sum(adjusted.values())
                     if total > 0:
                         adjusted = {k: v / total for k, v in adjusted.items()}

                     return adjusted
             ```

          2. MinimumVolatilityOptimizer
             ```python
             class MinimumVolatilityOptimizer:
                 """最小分散ポートフォリオ"""

                 def optimize(
                     self,
                     returns: pd.DataFrame,
                     constraints: dict = None
                 ) -> dict[str, float]:
                     """
                     分散最小化の最適化

                     min w'Σw
                     s.t. Σw = 1, w >= 0
                     """
                     from scipy.optimize import minimize

                     cov = returns.cov() * 252
                     n = len(returns.columns)

                     def portfolio_vol(w):
                         return np.sqrt(w @ cov @ w)

                     constraints_list = [
                         {"type": "eq", "fun": lambda w: np.sum(w) - 1}
                     ]

                     bounds = [(0, constraints.get("w_max", 0.2)) for _ in range(n)]

                     result = minimize(
                         portfolio_vol,
                         x0=np.ones(n) / n,
                         method="SLSQP",
                         bounds=bounds,
                         constraints=constraints_list
                     )

                     weights = dict(zip(returns.columns, result.x))
                     return weights
             ```
        priority: high

      # =========================================================================
      # カテゴリ3: リスク管理・テールリスク
      # =========================================================================
      - task_id: task_016_6
        description: |
          src/risk/vix_cash_allocation.py を新規作成せよ

          【VIXベースのキャッシュ配分】

          1. VIXCashAllocator クラス
             ```python
             class VIXCashAllocator:
                 """VIX水準に応じた動的キャッシュ配分"""

                 def __init__(
                     self,
                     vix_low: float = 15.0,
                     vix_mid: float = 20.0,
                     vix_high: float = 25.0,
                     vix_extreme: float = 35.0,
                     base_cash: float = 0.05
                 ):
                     self.vix_low = vix_low
                     self.vix_mid = vix_mid
                     self.vix_high = vix_high
                     self.vix_extreme = vix_extreme
                     self.base_cash = base_cash

                 def compute_cash_ratio(
                     self,
                     vix_current: float,
                     vix_history: pd.Series = None
                 ) -> float:
                     """
                     VIX水準からキャッシュ比率を決定

                     VIX < 15:     5% (base)
                     VIX 15-20:   10%
                     VIX 20-25:   20%
                     VIX 25-35:   35%
                     VIX > 35:    50%
                     """
                     if vix_current < self.vix_low:
                         return self.base_cash
                     elif vix_current < self.vix_mid:
                         return 0.10
                     elif vix_current < self.vix_high:
                         return 0.20
                     elif vix_current < self.vix_extreme:
                         return 0.35
                     else:
                         return 0.50

                 def compute_dynamic_thresholds(
                     self,
                     vix_history: pd.Series,
                     lookback_days: int = 252
                 ) -> dict:
                     """
                     過去のVIX分布から動的に閾値を設定
                     """
                     vix_data = vix_history.tail(lookback_days)

                     return {
                         "vix_low": vix_data.quantile(0.20),
                         "vix_mid": vix_data.quantile(0.50),
                         "vix_high": vix_data.quantile(0.80),
                         "vix_extreme": vix_data.quantile(0.95),
                     }

                 def adjust_weights_for_cash(
                     self,
                     weights: dict[str, float],
                     cash_ratio: float,
                     cash_asset: str = "BIL"
                 ) -> dict[str, float]:
                     """
                     キャッシュ比率を反映した配分調整
                     """
                     risk_ratio = 1 - cash_ratio

                     adjusted = {k: v * risk_ratio for k, v in weights.items()
                                 if k != cash_asset}
                     adjusted[cash_asset] = cash_ratio

                     return adjusted
             ```

          2. yfinanceからVIX取得
             ```python
             def fetch_vix_data(period: str = "5y") -> pd.Series:
                 import yfinance as yf
                 vix = yf.download("^VIX", period=period, progress=False)
                 return vix["Close"]
             ```
        priority: critical

      - task_id: task_016_7
        description: |
          src/risk/correlation_break_detector.py を新規作成せよ

          【相関ブレイク検出】

          1. CorrelationBreakDetector クラス
             ```python
             class CorrelationBreakDetector:
                 """分散効果崩壊の早期検知"""

                 def __init__(
                     self,
                     baseline_lookback: int = 252,
                     recent_lookback: int = 20,
                     warning_threshold: float = 0.3,
                     critical_threshold: float = 0.5
                 ):
                     self.baseline_lookback = baseline_lookback
                     self.recent_lookback = recent_lookback
                     self.warning_threshold = warning_threshold
                     self.critical_threshold = critical_threshold

                 def compute_average_correlation(
                     self,
                     returns: pd.DataFrame,
                     lookback: int
                 ) -> float:
                     """平均相関係数を計算"""
                     corr_matrix = returns.tail(lookback).corr()
                     # 対角成分を除外した平均
                     n = len(corr_matrix)
                     total = corr_matrix.sum().sum() - n  # 対角除外
                     avg_corr = total / (n * (n - 1))
                     return avg_corr

                 def detect_correlation_break(
                     self,
                     returns: pd.DataFrame
                 ) -> dict:
                     """
                     相関ブレイクを検出

                     Returns:
                         {
                             "status": "normal" | "warning" | "critical",
                             "baseline_corr": float,
                             "recent_corr": float,
                             "correlation_change": float
                         }
                     """
                     baseline_corr = self.compute_average_correlation(
                         returns, self.baseline_lookback
                     )
                     recent_corr = self.compute_average_correlation(
                         returns, self.recent_lookback
                     )

                     change = recent_corr - baseline_corr

                     if change > self.critical_threshold:
                         status = "critical"
                     elif change > self.warning_threshold:
                         status = "warning"
                     else:
                         status = "normal"

                     return {
                         "status": status,
                         "baseline_corr": baseline_corr,
                         "recent_corr": recent_corr,
                         "correlation_change": change
                     }

                 def get_risk_adjustment(
                     self,
                     status: str
                 ) -> float:
                     """
                     相関ブレイク状態に応じたリスク調整係数

                     normal: 1.0
                     warning: 0.8
                     critical: 0.5
                     """
                     adjustments = {
                         "normal": 1.0,
                         "warning": 0.8,
                         "critical": 0.5
                     }
                     return adjustments.get(status, 1.0)
             ```
        priority: high

      - task_id: task_016_8
        description: |
          src/risk/drawdown_protection.py を新規作成せよ

          【ドローダウン・プロテクション強化】

          1. DrawdownProtector クラス
             ```python
             class DrawdownProtector:
                 """段階的ドローダウン・プロテクション"""

                 def __init__(
                     self,
                     dd_levels: list[float] = [0.05, 0.10, 0.15, 0.20],
                     risk_reductions: list[float] = [0.9, 0.7, 0.5, 0.3],
                     recovery_threshold: float = 0.5  # DDの50%回復で解除
                 ):
                     self.dd_levels = dd_levels
                     self.risk_reductions = risk_reductions
                     self.recovery_threshold = recovery_threshold
                     self.hwm = None  # High Water Mark
                     self.current_dd = 0
                     self.protection_level = 0

                 def update(
                     self,
                     portfolio_value: float
                 ) -> dict:
                     """
                     ポートフォリオ価値を更新してプロテクション状態を判定
                     """
                     if self.hwm is None:
                         self.hwm = portfolio_value

                     # HWM更新
                     if portfolio_value > self.hwm:
                         self.hwm = portfolio_value
                         self.protection_level = 0

                     # ドローダウン計算
                     self.current_dd = 1 - portfolio_value / self.hwm

                     # プロテクションレベル判定
                     for i, level in enumerate(self.dd_levels):
                         if self.current_dd >= level:
                             self.protection_level = i + 1

                     # 回復チェック
                     recovery_needed = self.dd_levels[self.protection_level - 1] if self.protection_level > 0 else 0
                     if self.current_dd < recovery_needed * (1 - self.recovery_threshold):
                         self.protection_level = max(0, self.protection_level - 1)

                     return {
                         "hwm": self.hwm,
                         "current_dd": self.current_dd,
                         "protection_level": self.protection_level,
                         "risk_multiplier": self.get_risk_multiplier()
                     }

                 def get_risk_multiplier(self) -> float:
                     """現在のリスク乗数を取得"""
                     if self.protection_level == 0:
                         return 1.0
                     return self.risk_reductions[self.protection_level - 1]

                 def adjust_weights(
                     self,
                     weights: dict[str, float],
                     cash_asset: str = "BIL"
                 ) -> dict[str, float]:
                     """
                     プロテクションレベルに応じて配分調整
                     """
                     multiplier = self.get_risk_multiplier()

                     if multiplier >= 1.0:
                         return weights

                     adjusted = {}
                     cash_increase = 0

                     for asset, weight in weights.items():
                         if asset == cash_asset:
                             adjusted[asset] = weight
                         else:
                             new_weight = weight * multiplier
                             adjusted[asset] = new_weight
                             cash_increase += weight - new_weight

                     adjusted[cash_asset] = adjusted.get(cash_asset, 0) + cash_increase

                     return adjusted
             ```
        priority: high

      # =========================================================================
      # カテゴリ4: 動的パラメータ計算（cmd_014の具体実装）
      # =========================================================================
      - task_id: task_016_9
        description: |
          src/analysis/dynamic_thresholds.py を新規作成せよ

          【動的パラメータ計算の具体実装】

          1. DynamicRebalanceThreshold クラス
             ```python
             class DynamicRebalanceThreshold:
                 """リバランス閾値の動的計算"""

                 def calculate(
                     self,
                     portfolio_returns: pd.Series,
                     transaction_cost_bps: float = 10,
                     lookback_days: int = 60
                 ) -> float:
                     """
                     最適リバランス閾値を計算

                     閾値 = max(コストベース閾値, ボラベース閾値)

                     - コストベース: 取引コスト × 2（往復）を上回る必要
                     - ボラベース: 日次ボラ × √平均保有期間
                     """
                     returns = portfolio_returns.tail(lookback_days)
                     daily_vol = returns.std()

                     # コストベース閾値
                     cost_threshold = transaction_cost_bps / 10000 * 2

                     # ボラベース閾値（20日保有想定）
                     vol_threshold = daily_vol * np.sqrt(20)

                     return max(cost_threshold, vol_threshold)
             ```

          2. DynamicSmoothingAlpha クラス
             ```python
             class DynamicSmoothingAlpha:
                 """スムージングαの動的計算"""

                 def calculate(
                     self,
                     volatility_ratio: float,  # 現在ボラ / 長期ボラ
                     base_alpha: float = 0.3
                 ) -> float:
                     """
                     ボラティリティレジームに応じたα

                     高ボラ時: α低め（慎重に追従）
                     低ボラ時: α高め（積極的に追従）
                     """
                     if volatility_ratio > 1.5:
                         return base_alpha * 0.5  # 高ボラ: 慎重
                     elif volatility_ratio > 1.2:
                         return base_alpha * 0.7
                     elif volatility_ratio < 0.7:
                         return base_alpha * 1.3  # 低ボラ: 積極的
                     else:
                         return base_alpha
             ```

          3. DynamicCovarianceParams クラス
             ```python
             class DynamicCovarianceParams:
                 """共分散パラメータの動的計算"""

                 def calculate_ewma_halflife(
                     self,
                     volatility_regime: str
                 ) -> int:
                     """
                     レジームに応じたEWMA半減期

                     crisis: 20日（速い適応）
                     high_vol: 40日
                     normal: 60日
                     low_vol: 90日（安定重視）
                     """
                     halflife_map = {
                         "crisis": 20,
                         "high_vol": 40,
                         "normal": 60,
                         "low_vol": 90
                     }
                     return halflife_map.get(volatility_regime, 60)

                 def calculate_correlation_adjustment(
                     self,
                     historical_correlations: pd.DataFrame,
                     regime: str
                 ) -> float:
                     """
                     レジーム別相関調整係数

                     過去のレジーム別相関から計算
                     """
                     if regime == "crisis":
                         return 0.30  # +30%
                     elif regime == "high_vol":
                         return 0.15  # +15%
                     elif regime == "low_vol":
                         return -0.10  # -10%
                     return 0.0
             ```

          4. DynamicVIXThresholds クラス
             ```python
             class DynamicVIXThresholds:
                 """VIX閾値の動的計算"""

                 def calculate(
                     self,
                     vix_history: pd.Series,
                     lookback_days: int = 252
                 ) -> dict:
                     """
                     過去分布からVIX閾値を計算
                     """
                     vix = vix_history.tail(lookback_days)

                     return {
                         "low": vix.quantile(0.20),
                         "mid": vix.quantile(0.50),
                         "high": vix.quantile(0.80),
                         "extreme": vix.quantile(0.95)
                     }
             ```
        priority: critical

      # =========================================================================
      # カテゴリ5: レジーム検出・適応
      # =========================================================================
      - task_id: task_016_10
        description: |
          src/signals/regime_signal_params.py を新規作成せよ

          【レジーム別シグナルパラメータ】

          cmd_015_4 の具体実装。
          レジームに応じてシグナルパラメータを動的調整。

          ```python
          REGIME_SIGNAL_PARAMS = {
              "bull_trend": {
                  "momentum_lookbacks": [40, 60, 90],
                  "momentum_scale": 3.0,
                  "bollinger_period": 30,
                  "bollinger_std": 2.5,
                  "rsi_period": 21,
                  "rsi_oversold": 25,
                  "rsi_overbought": 75,
              },
              "bear_market": {
                  "momentum_lookbacks": [10, 20, 40],
                  "momentum_scale": 8.0,
                  "bollinger_period": 14,
                  "bollinger_std": 2.0,
                  "rsi_period": 10,
                  "rsi_oversold": 20,
                  "rsi_overbought": 80,
              },
              "high_vol": {
                  "momentum_lookbacks": [20, 40],
                  "momentum_scale": 2.0,
                  "bollinger_period": 20,
                  "bollinger_std": 3.0,
                  "rsi_period": 14,
                  "rsi_oversold": 20,
                  "rsi_overbought": 80,
              },
              "low_vol": {
                  "momentum_lookbacks": [20, 40, 60],
                  "momentum_scale": 6.0,
                  "bollinger_period": 20,
                  "bollinger_std": 1.5,
                  "rsi_period": 14,
                  "rsi_oversold": 35,
                  "rsi_overbought": 65,
              },
              "range_bound": {
                  "momentum_lookbacks": [5, 10, 20],
                  "momentum_scale": 10.0,
                  "bollinger_period": 14,
                  "bollinger_std": 2.0,
                  "rsi_period": 14,
                  "rsi_oversold": 30,
                  "rsi_overbought": 70,
              },
          }

          class RegimeAdaptiveSignalGenerator:
              def get_params_for_regime(self, regime: str) -> dict:
                  return REGIME_SIGNAL_PARAMS.get(regime, REGIME_SIGNAL_PARAMS["range_bound"])

              def generate_adaptive_signals(
                  self,
                  prices: pd.DataFrame,
                  regime: str
              ) -> dict[str, pd.Series]:
                  params = self.get_params_for_regime(regime)
                  signals = {}

                  # モメンタム
                  for lb in params["momentum_lookbacks"]:
                      signal = MomentumReturnSignal(lookback=lb, scale=params["momentum_scale"])
                      signals[f"momentum_{lb}"] = signal.compute(prices).scores

                  # ボリンジャー
                  signal = BollingerReversionSignal(
                      period=params["bollinger_period"],
                      num_std=params["bollinger_std"]
                  )
                  signals["bollinger"] = signal.compute(prices).scores

                  # RSI
                  signal = RSISignal(
                      period=params["rsi_period"],
                      oversold_level=params["rsi_oversold"],
                      overbought_level=params["rsi_overbought"]
                  )
                  signals["rsi"] = signal.compute(prices).scores

                  return signals
          ```
        priority: high

      - task_id: task_016_11
        description: |
          src/signals/yield_curve_signal.py を拡張せよ

          【イールドカーブ形状の活用強化】

          既存の YieldCurveSignal を拡張し、より詳細な形状分析を追加。

          ```python
          class EnhancedYieldCurveSignal:
              """拡張イールドカーブシグナル"""

              # FRED系列コード
              YIELDS = {
                  "3M": "DGS3MO",
                  "2Y": "DGS2",
                  "5Y": "DGS5",
                  "10Y": "DGS10",
                  "30Y": "DGS30"
              }

              def compute_curve_shape(
                  self,
                  yields: dict[str, float]
              ) -> dict:
                  """
                  イールドカーブ形状を分析

                  Returns:
                      {
                          "slope_2_10": float,  # 10Y - 2Y
                          "slope_3m_10": float,  # 10Y - 3M
                          "curvature": float,  # 2*(5Y) - (2Y + 10Y)
                          "shape": "normal" | "flat" | "inverted" | "humped"
                      }
                  """
                  slope_2_10 = yields["10Y"] - yields["2Y"]
                  slope_3m_10 = yields["10Y"] - yields["3M"]
                  curvature = 2 * yields["5Y"] - (yields["2Y"] + yields["10Y"])

                  # 形状判定
                  if slope_2_10 < -0.2:
                      shape = "inverted"
                  elif slope_2_10 < 0.5:
                      shape = "flat"
                  elif curvature > 0.3:
                      shape = "humped"
                  else:
                      shape = "normal"

                  return {
                      "slope_2_10": slope_2_10,
                      "slope_3m_10": slope_3m_10,
                      "curvature": curvature,
                      "shape": shape
                  }

              def get_allocation_bias(
                  self,
                  shape: str
              ) -> dict:
                  """
                  カーブ形状に応じた配分バイアス

                  inverted → ディフェンシブ
                  normal → リスクオン
                  """
                  biases = {
                      "inverted": {
                          "equity_bias": -0.2,
                          "bond_bias": 0.1,
                          "cash_bias": 0.1,
                          "defensive_sectors": True
                      },
                      "flat": {
                          "equity_bias": -0.1,
                          "bond_bias": 0.0,
                          "cash_bias": 0.1,
                          "defensive_sectors": True
                      },
                      "normal": {
                          "equity_bias": 0.1,
                          "bond_bias": 0.0,
                          "cash_bias": -0.1,
                          "defensive_sectors": False
                      },
                      "humped": {
                          "equity_bias": 0.0,
                          "bond_bias": 0.05,
                          "cash_bias": 0.0,
                          "defensive_sectors": False
                      }
                  }
                  return biases.get(shape, biases["normal"])
          ```
        priority: high

      # =========================================================================
      # カテゴリ6: エントリー/エグジット最適化
      # =========================================================================
      - task_id: task_016_12
        description: |
          src/strategy/hysteresis_filter.py を新規作成せよ

          【ヒステリシス・フィルター】

          cmd_015_6 の具体実装。

          ```python
          class HysteresisFilter:
              """
              エントリー/エグジット閾値を分離して過剰取引を防止

              - エントリー閾値: 高め（確信度が必要）
              - エグジット閾値: 低め（一度入ったら粘る）
              """

              def __init__(
                  self,
                  entry_threshold: float = 0.3,
                  exit_threshold: float = 0.1,
                  min_holding_periods: int = 5
              ):
                  self.entry_threshold = entry_threshold
                  self.exit_threshold = exit_threshold
                  self.min_holding_periods = min_holding_periods
                  self.positions = {}  # asset -> {"active": bool, "periods": int}

              def filter(
                  self,
                  asset: str,
                  signal_score: float
              ) -> float:
                  """
                  ヒステリシスフィルタを適用

                  Returns:
                      フィルタ後のシグナル（0 or signal_score）
                  """
                  position = self.positions.get(asset, {"active": False, "periods": 0})

                  if not position["active"]:
                      # ポジションなし → エントリー判定
                      if abs(signal_score) >= self.entry_threshold:
                          self.positions[asset] = {"active": True, "periods": 1}
                          return signal_score
                      return 0.0

                  else:
                      # ポジションあり
                      position["periods"] += 1

                      # 最低保有期間チェック
                      if position["periods"] < self.min_holding_periods:
                          self.positions[asset] = position
                          return signal_score

                      # エグジット判定
                      if abs(signal_score) < self.exit_threshold:
                          self.positions[asset] = {"active": False, "periods": 0}
                          return 0.0

                      self.positions[asset] = position
                      return signal_score

              def filter_batch(
                  self,
                  signals: dict[str, float]
              ) -> dict[str, float]:
                  """複数アセットのシグナルをフィルタ"""
                  return {asset: self.filter(asset, score)
                          for asset, score in signals.items()}
          ```
        priority: critical

      - task_id: task_016_13
        description: |
          src/strategy/signal_decay.py を新規作成せよ

          【シグナル減衰】

          ```python
          class SignalDecayFilter:
              """
              古いシグナルの重みを減衰

              シグナル発生からの経過時間で指数減衰
              """

              def __init__(
                  self,
                  halflife: int = 5,  # 5日で半減
                  min_weight: float = 0.1  # 最低10%は維持
              ):
                  self.halflife = halflife
                  self.min_weight = min_weight
                  self.signal_history = {}  # asset -> {"value": float, "age": int}

              def update_and_decay(
                  self,
                  asset: str,
                  new_signal: float
              ) -> float:
                  """
                  新しいシグナルを受け取り、減衰を適用

                  - 新シグナルがあれば更新
                  - なければ既存シグナルを減衰
                  """
                  if abs(new_signal) > 0.01:
                      # 新しいシグナル
                      self.signal_history[asset] = {"value": new_signal, "age": 0}
                      return new_signal

                  # 既存シグナルを減衰
                  if asset in self.signal_history:
                      history = self.signal_history[asset]
                      history["age"] += 1

                      decay_factor = 0.5 ** (history["age"] / self.halflife)
                      decay_factor = max(decay_factor, self.min_weight)

                      decayed_signal = history["value"] * decay_factor
                      self.signal_history[asset] = history

                      return decayed_signal

                  return 0.0

              def get_effective_signals(
                  self,
                  raw_signals: dict[str, float]
              ) -> dict[str, float]:
                  """バッチ処理"""
                  return {asset: self.update_and_decay(asset, score)
                          for asset, score in raw_signals.items()}
          ```
        priority: high

      - task_id: task_016_14
        description: |
          src/strategy/min_holding_period.py を新規作成せよ

          【最低保有期間】

          ```python
          class MinHoldingPeriodFilter:
              """
              過剰取引を防止する最低保有期間フィルタ

              一度ポジションを取ったら最低N日は保持
              """

              def __init__(
                  self,
                  min_periods: int = 5,
                  force_exit_on_reversal: bool = True,
                  reversal_threshold: float = -0.5
              ):
                  self.min_periods = min_periods
                  self.force_exit_on_reversal = force_exit_on_reversal
                  self.reversal_threshold = reversal_threshold
                  self.holdings = {}  # asset -> {"direction": int, "periods": int, "entry_signal": float}

              def should_trade(
                  self,
                  asset: str,
                  signal: float,
                  current_weight: float
              ) -> dict:
                  """
                  取引すべきか判定

                  Returns:
                      {
                          "action": "hold" | "enter" | "exit" | "reverse",
                          "signal": float
                      }
                  """
                  holding = self.holdings.get(asset)

                  # ポジションなし
                  if holding is None or holding["direction"] == 0:
                      if abs(signal) > 0.1:
                          direction = 1 if signal > 0 else -1
                          self.holdings[asset] = {
                              "direction": direction,
                              "periods": 1,
                              "entry_signal": signal
                          }
                          return {"action": "enter", "signal": signal}
                      return {"action": "hold", "signal": 0}

                  # ポジションあり
                  holding["periods"] += 1
                  self.holdings[asset] = holding

                  # 最低保有期間内
                  if holding["periods"] < self.min_periods:
                      # 強制エグジット条件チェック
                      if self.force_exit_on_reversal:
                          if holding["direction"] > 0 and signal < self.reversal_threshold:
                              self.holdings[asset] = {"direction": 0, "periods": 0, "entry_signal": 0}
                              return {"action": "exit", "signal": 0}
                          if holding["direction"] < 0 and signal > -self.reversal_threshold:
                              self.holdings[asset] = {"direction": 0, "periods": 0, "entry_signal": 0}
                              return {"action": "exit", "signal": 0}

                      return {"action": "hold", "signal": holding["entry_signal"]}

                  # 最低保有期間経過 → 通常判定
                  if abs(signal) < 0.1:
                      self.holdings[asset] = {"direction": 0, "periods": 0, "entry_signal": 0}
                      return {"action": "exit", "signal": 0}

                  return {"action": "hold", "signal": signal}
          ```
        priority: high

      # =========================================================================
      # 統合タスク
      # =========================================================================
      - task_id: task_016_15
        description: |
          全ての新機能を統合し、パイプラインを更新せよ

          【統合対象ファイル】
          - src/orchestrator/pipeline.py
          - src/meta/learner.py
          - config/default.yaml

          【パイプライン更新】
          ```
          1. データ取得（yfinance, FRED）
          2. VIX取得 → VIXキャッシュ配分計算（task_016_6）
          3. レジーム検出
          4. レジーム適応シグナルパラメータ設定（task_016_10）
          5. シグナル生成
             - 既存シグナル
             - ペアトレーディング（task_016_1）
             - クロスアセットモメンタム（task_016_2）
             - デュアルモメンタム強化（task_016_4）
             - 低ボラプレミアム（task_016_5）
          6. セクターローテーション調整（task_016_3）
          7. ヒステリシスフィルター適用（task_016_12）
          8. シグナル減衰適用（task_016_13）
          9. 最低保有期間フィルター（task_016_14）
          10. 相関ブレイク検出（task_016_7）
          11. ドローダウンプロテクション（task_016_8）
          12. 動的パラメータ適用（task_016_9）
          13. イールドカーブ調整（task_016_11）
          14. 最終配分出力
          ```

          【config/default.yaml 追加】
          ```yaml
          cmd_016_features:
            # シグナル・戦略
            pairs_trading:
              enabled: true
              entry_zscore: 2.0
              exit_zscore: 0.5
              max_pairs: 5

            cross_asset_momentum:
              enabled: true
              lookback: 60
              adjustment_strength: 0.3

            sector_rotation:
              enabled: true
              adjustment_pct: 0.20

            dual_momentum:
              enabled: true
              abs_lookbacks: [60, 120, 252]
              safe_asset: "BIL"

            low_vol_premium:
              enabled: true
              target_percentile: 0.3
              adjustment_strength: 0.3

            # リスク管理
            vix_cash_allocation:
              enabled: true
              use_dynamic_thresholds: true

            correlation_break:
              enabled: true
              warning_threshold: 0.3
              critical_threshold: 0.5

            drawdown_protection:
              enabled: true
              dd_levels: [0.05, 0.10, 0.15, 0.20]

            # 動的パラメータ
            dynamic_thresholds:
              enabled: true
              update_frequency: "monthly"

            # レジーム適応
            regime_adaptive_signals:
              enabled: true

            yield_curve_enhanced:
              enabled: true

            # エントリー/エグジット
            hysteresis_filter:
              enabled: true
              entry_threshold: 0.3
              exit_threshold: 0.1
              min_holding_periods: 5

            signal_decay:
              enabled: true
              halflife: 5

            min_holding_period:
              enabled: true
              min_periods: 5
          ```
        priority: critical

    execution_strategy: |
      【Phase 1: 即効性の高い施策（並列）】
      task_016_6: VIXキャッシュ配分
      task_016_9: 動的パラメータ計算
      task_016_12: ヒステリシスフィルター

      【Phase 2: シグナル・戦略強化（並列）】
      task_016_1: ペアトレーディング
      task_016_2: クロスアセットモメンタム
      task_016_3: セクターローテーション
      task_016_4: デュアルモメンタム強化
      task_016_5: 低ボラプレミアム

      【Phase 3: リスク管理（並列）】
      task_016_7: 相関ブレイク検出
      task_016_8: ドローダウンプロテクション

      【Phase 4: レジーム・タイミング（並列）】
      task_016_10: レジーム別シグナルパラメータ
      task_016_11: イールドカーブ強化
      task_016_13: シグナル減衰
      task_016_14: 最低保有期間

      【Phase 5: 統合】
      task_016_15: 全機能統合

      足軽の割り当て:
      - 足軽1: task_016_1（ペアトレーディング）
      - 足軽2: task_016_2, task_016_3（クロスアセット、セクターローテーション）
      - 足軽3: task_016_4, task_016_5（デュアルモメンタム、低ボラ）
      - 足軽4: task_016_6, task_016_9（VIX、動的パラメータ）
      - 足軽5: task_016_7, task_016_8（相関、ドローダウン）
      - 足軽6: task_016_10, task_016_11（レジーム、イールドカーブ）
      - 足軽7: task_016_12, task_016_13, task_016_14（フィルター系）
      - 足軽8: task_016_15（統合）

    expected_outcome: |
      【期待効果サマリ】

      | カテゴリ | 施策数 | 期待リターン改善 | 期待MDD改善 |
      |----------|--------|-----------------|-------------|
      | シグナル・戦略 | 5 | +3-6%/年 | - |
      | リスク管理 | 3 | - | -5-8% |
      | 動的パラメータ | 4 | +1-2%/年 | -2-3% |
      | レジーム適応 | 2 | +1-2%/年 | -1-2% |
      | エントリー最適化 | 3 | +0.5-1.5%/年 | - |

      【累積目標】
      - Sharpe: 0.878 → 1.2-1.5
      - 年率リターン: 21.77% → 28-35%
      - MDD: → 15-18%以下

      【データソース】
      全て無料:
      - yfinance: 価格データ（20年+）
      - FRED API: マクロ経済指標
      - CBOE: VIXデータ

      【検証方法】
      1. 各施策のon/off比較テスト
      2. 15年バックテスト（2010-2025）
      3. サブ期間分析（GFC回復、低ボラ、COVID、金利上昇）
      4. ベンチマーク比較（SPY, 60/40）

  - id: cmd_017
    timestamp: "2026-01-29T02:00:00"
    command: "配分最適化・機械学習・検証の高度化を実装せよ"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      殿より、cmd_014-016に続く次期改善策として、
      配分最適化、機械学習、Walk-Forward最適化、執行最適化、
      リスク管理、バックテスト検証の高度化を全て実装せよとの命があった。

      【目標】
      - Sharpe: 1.2-1.5 → 1.5-2.0
      - 年率リターン: 28-35% → 35-45%
      - 取引コスト: -0.3-0.5%削減
      - 検証の統計的信頼性向上

      【実装カテゴリ】
      1. 配分最適化の高度化（4施策）
      2. 機械学習の高度化（4施策）
      3. Walk-Forward最適化の強化（4施策）
      4. 執行・取引最適化（3施策）
      5. リスク管理の高度化（4施策）
      6. バックテスト・検証の強化（4施策）

    tasks:
      # =========================================================================
      # カテゴリ1: 配分最適化の高度化
      # =========================================================================
      - task_id: task_017_1
        description: |
          src/allocation/nco.py を新規作成せよ

          【Nested Clustered Optimization (NCO)】

          HRPの改良版。クラスタ内で最適化を行い、クラスタ間でリスクパリティ配分。

          ```python
          import numpy as np
          import pandas as pd
          from scipy.cluster.hierarchy import linkage, fcluster
          from scipy.optimize import minimize

          class NestedClusteredOptimization:
              """
              Nested Clustered Optimization (NCO)

              López de Prado (2019) の改良版HRP:
              1. 階層的クラスタリングでアセットをグループ化
              2. 各クラスタ内で平均分散最適化
              3. クラスタ間でリスクパリティ配分
              """

              def __init__(
                  self,
                  n_clusters: int = 5,
                  intra_cluster_method: str = "min_variance",
                  inter_cluster_method: str = "risk_parity",
                  max_weight: float = 0.20
              ):
                  self.n_clusters = n_clusters
                  self.intra_cluster_method = intra_cluster_method
                  self.inter_cluster_method = inter_cluster_method
                  self.max_weight = max_weight

              def fit(
                  self,
                  returns: pd.DataFrame
              ) -> dict[str, float]:
                  """
                  NCO配分を計算

                  Steps:
                  1. 相関行列から距離行列を計算
                  2. 階層的クラスタリング
                  3. クラスタ内最適化
                  4. クラスタ間リスクパリティ
                  5. 最終ウェイト計算
                  """
                  cov = returns.cov() * 252
                  corr = returns.corr()

                  # 距離行列
                  dist = np.sqrt(0.5 * (1 - corr))

                  # 階層的クラスタリング
                  link = linkage(dist, method='ward')
                  clusters = fcluster(link, self.n_clusters, criterion='maxclust')

                  # クラスタ毎の処理
                  cluster_weights = {}
                  cluster_vars = {}

                  for c in range(1, self.n_clusters + 1):
                      mask = clusters == c
                      assets_in_cluster = returns.columns[mask].tolist()

                      if len(assets_in_cluster) == 0:
                          continue

                      # クラスタ内最適化
                      cluster_cov = cov.loc[assets_in_cluster, assets_in_cluster]
                      w_intra = self._optimize_intra_cluster(cluster_cov)

                      cluster_weights[c] = dict(zip(assets_in_cluster, w_intra))

                      # クラスタ分散
                      cluster_vars[c] = w_intra @ cluster_cov.values @ w_intra

                  # クラスタ間リスクパリティ
                  w_inter = self._risk_parity_inter_cluster(cluster_vars)

                  # 最終ウェイト
                  final_weights = {}
                  for c, w_c in w_inter.items():
                      for asset, w_a in cluster_weights[c].items():
                          final_weights[asset] = w_c * w_a

                  # 制約適用
                  final_weights = self._apply_constraints(final_weights)

                  return final_weights

              def _optimize_intra_cluster(
                  self,
                  cov: pd.DataFrame
              ) -> np.ndarray:
                  """クラスタ内最適化（最小分散）"""
                  n = len(cov)

                  if self.intra_cluster_method == "min_variance":
                      def objective(w):
                          return w @ cov.values @ w

                      constraints = [{"type": "eq", "fun": lambda w: np.sum(w) - 1}]
                      bounds = [(0, 1) for _ in range(n)]

                      result = minimize(
                          objective,
                          x0=np.ones(n) / n,
                          method="SLSQP",
                          bounds=bounds,
                          constraints=constraints
                      )
                      return result.x

                  else:  # equal_weight
                      return np.ones(n) / n

              def _risk_parity_inter_cluster(
                  self,
                  cluster_vars: dict[int, float]
              ) -> dict[int, float]:
                  """クラスタ間リスクパリティ"""
                  inv_vol = {c: 1 / np.sqrt(v) for c, v in cluster_vars.items()}
                  total = sum(inv_vol.values())
                  return {c: v / total for c, v in inv_vol.items()}

              def _apply_constraints(
                  self,
                  weights: dict[str, float]
              ) -> dict[str, float]:
                  """制約適用（最大ウェイト）"""
                  # 最大ウェイト制約
                  adjusted = {k: min(v, self.max_weight) for k, v in weights.items()}

                  # 正規化
                  total = sum(adjusted.values())
                  if total > 0:
                      adjusted = {k: v / total for k, v in adjusted.items()}

                  return adjusted
          ```
        priority: high

      - task_id: task_017_2
        description: |
          src/allocation/black_litterman.py を新規作成せよ

          【Black-Litterman モデル】

          市場均衡リターン + 主観的ビューの統合。

          ```python
          import numpy as np
          import pandas as pd

          class BlackLittermanModel:
              """
              Black-Litterman Model

              市場均衡からの期待リターン（π）に、
              投資家の主観的ビュー（Q）を統合。
              """

              def __init__(
                  self,
                  risk_aversion: float = 2.5,
                  tau: float = 0.05
              ):
                  self.risk_aversion = risk_aversion
                  self.tau = tau  # 不確実性スケーリング

              def compute_equilibrium_returns(
                  self,
                  cov: pd.DataFrame,
                  market_weights: pd.Series
              ) -> pd.Series:
                  """
                  市場均衡期待リターン（π）を計算

                  π = δ * Σ * w_mkt

                  where:
                    δ = risk aversion
                    Σ = covariance matrix
                    w_mkt = market cap weights
                  """
                  pi = self.risk_aversion * cov @ market_weights
                  return pi

              def combine_views(
                  self,
                  pi: pd.Series,
                  cov: pd.DataFrame,
                  P: np.ndarray,
                  Q: np.ndarray,
                  omega: np.ndarray = None
              ) -> pd.Series:
                  """
                  ビューと均衡リターンを統合

                  E[R] = [(τΣ)^-1 + P'Ω^-1 P]^-1 [(τΣ)^-1 π + P'Ω^-1 Q]

                  Args:
                      pi: 均衡期待リターン
                      cov: 共分散行列
                      P: ビュー行列（K x N）
                      Q: ビューリターン（K x 1）
                      omega: ビュー不確実性（K x K）
                  """
                  Sigma = cov.values
                  tau_Sigma = self.tau * Sigma

                  if omega is None:
                      # Idzorek method: Ω = diag(P * τΣ * P')
                      omega = np.diag(np.diag(P @ tau_Sigma @ P.T))

                  # 精度行列
                  tau_Sigma_inv = np.linalg.inv(tau_Sigma)
                  omega_inv = np.linalg.inv(omega)

                  # 事後期待リターン
                  M = np.linalg.inv(tau_Sigma_inv + P.T @ omega_inv @ P)
                  posterior_mean = M @ (tau_Sigma_inv @ pi.values + P.T @ omega_inv @ Q)

                  return pd.Series(posterior_mean, index=pi.index)

              def optimize(
                  self,
                  posterior_returns: pd.Series,
                  cov: pd.DataFrame,
                  max_weight: float = 0.20
              ) -> dict[str, float]:
                  """
                  事後リターンを使用した最適化
                  """
                  from scipy.optimize import minimize

                  n = len(posterior_returns)
                  mu = posterior_returns.values
                  Sigma = cov.values

                  def neg_sharpe(w):
                      port_ret = w @ mu
                      port_vol = np.sqrt(w @ Sigma @ w)
                      return -port_ret / port_vol if port_vol > 0 else 0

                  constraints = [{"type": "eq", "fun": lambda w: np.sum(w) - 1}]
                  bounds = [(0, max_weight) for _ in range(n)]

                  result = minimize(
                      neg_sharpe,
                      x0=np.ones(n) / n,
                      method="SLSQP",
                      bounds=bounds,
                      constraints=constraints
                  )

                  return dict(zip(posterior_returns.index, result.x))


          class ViewGenerator:
              """シグナルからBlack-Littermanビューを生成"""

              def generate_views_from_signals(
                  self,
                  signals: dict[str, float],
                  confidence: float = 0.5
              ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
                  """
                  シグナルスコアからP, Q, Ωを生成

                  Args:
                      signals: {asset: score} where score in [-1, +1]
                      confidence: ビューの確信度

                  Returns:
                      P: ビュー行列
                      Q: ビューリターン
                      omega: ビュー不確実性
                  """
                  assets = list(signals.keys())
                  n = len(assets)

                  # 各アセットに対する絶対ビュー
                  views = []
                  for i, (asset, score) in enumerate(signals.items()):
                      if abs(score) > 0.1:  # 閾値以上のみ
                          p = np.zeros(n)
                          p[i] = 1
                          q = score * 0.10  # スコア→期待リターン変換（10%スケール）
                          views.append((p, q, abs(score) * confidence))

                  if not views:
                      return None, None, None

                  P = np.array([v[0] for v in views])
                  Q = np.array([v[1] for v in views])
                  # 確信度に基づく不確実性（低確信 = 高不確実性）
                  omega = np.diag([1 / v[2] for v in views])

                  return P, Q, omega
          ```
        priority: high

      - task_id: task_017_3
        description: |
          src/allocation/cvar_optimizer.py を新規作成せよ

          【CVaR (Conditional Value at Risk) 最適化】

          テールリスクを考慮した配分最適化。

          ```python
          import numpy as np
          import pandas as pd
          from scipy.optimize import minimize

          class CVaROptimizer:
              """
              CVaR (Expected Shortfall) 最適化

              VaRを超える損失の期待値を最小化。
              テールリスクに対してロバストな配分を実現。
              """

              def __init__(
                  self,
                  alpha: float = 0.05,  # 5% CVaR
                  target_return: float = None,
                  max_weight: float = 0.20
              ):
                  self.alpha = alpha
                  self.target_return = target_return
                  self.max_weight = max_weight

              def compute_cvar(
                  self,
                  returns: np.ndarray,
                  weights: np.ndarray
              ) -> float:
                  """
                  ポートフォリオCVaRを計算

                  CVaR = E[Loss | Loss > VaR]
                  """
                  portfolio_returns = returns @ weights
                  var = np.percentile(portfolio_returns, self.alpha * 100)
                  cvar = portfolio_returns[portfolio_returns <= var].mean()
                  return -cvar  # 損失なので符号反転

              def optimize(
                  self,
                  returns: pd.DataFrame,
                  expected_returns: pd.Series = None
              ) -> dict[str, float]:
                  """
                  CVaR最小化の最適化

                  min CVaR
                  s.t. Σw = 1
                       w >= 0
                       E[R] >= target (optional)
                  """
                  n = len(returns.columns)
                  ret_matrix = returns.values

                  if expected_returns is None:
                      expected_returns = returns.mean() * 252

                  def objective(w):
                      return self.compute_cvar(ret_matrix, w)

                  constraints = [
                      {"type": "eq", "fun": lambda w: np.sum(w) - 1}
                  ]

                  if self.target_return is not None:
                      constraints.append({
                          "type": "ineq",
                          "fun": lambda w: w @ expected_returns.values - self.target_return
                      })

                  bounds = [(0, self.max_weight) for _ in range(n)]

                  result = minimize(
                      objective,
                      x0=np.ones(n) / n,
                      method="SLSQP",
                      bounds=bounds,
                      constraints=constraints
                  )

                  return dict(zip(returns.columns, result.x))

              def compute_portfolio_metrics(
                  self,
                  returns: pd.DataFrame,
                  weights: dict[str, float]
              ) -> dict:
                  """ポートフォリオのリスク指標を計算"""
                  w = np.array([weights.get(c, 0) for c in returns.columns])
                  port_returns = returns.values @ w

                  var_95 = np.percentile(port_returns, 5)
                  cvar_95 = port_returns[port_returns <= var_95].mean()
                  var_99 = np.percentile(port_returns, 1)
                  cvar_99 = port_returns[port_returns <= var_99].mean()

                  return {
                      "VaR_95": -var_95 * np.sqrt(252),
                      "CVaR_95": -cvar_95 * np.sqrt(252),
                      "VaR_99": -var_99 * np.sqrt(252),
                      "CVaR_99": -cvar_99 * np.sqrt(252),
                      "Sharpe": port_returns.mean() / port_returns.std() * np.sqrt(252)
                  }
          ```
        priority: high

      - task_id: task_017_4
        description: |
          src/allocation/transaction_cost_optimizer.py を新規作成せよ

          【トランザクションコスト最適化】

          取引コストを考慮した最適配分。

          ```python
          import numpy as np
          import pandas as pd
          from scipy.optimize import minimize

          class TransactionCostOptimizer:
              """
              取引コストを考慮した配分最適化

              目的関数: max E[R] - λ * Var - γ * TxCost

              取引コストモデル:
              - 固定コスト（スプレッド + 手数料）
              - 市場インパクト（オプション）
              """

              def __init__(
                  self,
                  fixed_cost_bps: float = 10,
                  risk_aversion: float = 2.0,
                  cost_aversion: float = 1.0,
                  max_weight: float = 0.20
              ):
                  self.fixed_cost_bps = fixed_cost_bps / 10000
                  self.risk_aversion = risk_aversion
                  self.cost_aversion = cost_aversion
                  self.max_weight = max_weight

              def compute_transaction_cost(
                  self,
                  current_weights: np.ndarray,
                  target_weights: np.ndarray
              ) -> float:
                  """取引コストを計算"""
                  turnover = np.sum(np.abs(target_weights - current_weights))
                  return turnover * self.fixed_cost_bps

              def optimize(
                  self,
                  returns: pd.DataFrame,
                  current_weights: dict[str, float],
                  expected_returns: pd.Series = None
              ) -> dict[str, float]:
                  """
                  取引コスト込みの最適化

                  目的: max (期待リターン - リスクペナルティ - コストペナルティ)
                  """
                  n = len(returns.columns)
                  cov = returns.cov().values * 252

                  if expected_returns is None:
                      mu = returns.mean().values * 252
                  else:
                      mu = expected_returns.values

                  w_current = np.array([
                      current_weights.get(c, 0) for c in returns.columns
                  ])

                  def objective(w):
                      # 期待リターン
                      ret = w @ mu
                      # リスク
                      risk = self.risk_aversion * w @ cov @ w
                      # 取引コスト
                      cost = self.cost_aversion * self.compute_transaction_cost(w_current, w)
                      return -(ret - risk - cost)

                  constraints = [
                      {"type": "eq", "fun": lambda w: np.sum(w) - 1}
                  ]

                  bounds = [(0, self.max_weight) for _ in range(n)]

                  result = minimize(
                      objective,
                      x0=w_current if w_current.sum() > 0 else np.ones(n) / n,
                      method="SLSQP",
                      bounds=bounds,
                      constraints=constraints
                  )

                  return dict(zip(returns.columns, result.x))

              def compute_optimal_rebalance_threshold(
                  self,
                  portfolio_volatility: float,
                  holding_period_days: int = 20
              ) -> float:
                  """
                  最適リバランス閾値を計算

                  閾値 = sqrt(2 * コスト * 期待分散改善)
                  """
                  # Constant-mix approximation
                  expected_drift = portfolio_volatility * np.sqrt(holding_period_days / 252)
                  threshold = np.sqrt(2 * self.fixed_cost_bps * expected_drift)
                  return max(threshold, self.fixed_cost_bps * 2)


          class TurnoverConstrainedOptimizer:
              """ターンオーバー制約付き最適化"""

              def __init__(
                  self,
                  max_turnover: float = 0.20,  # 最大20%のターンオーバー
                  max_weight: float = 0.20
              ):
                  self.max_turnover = max_turnover
                  self.max_weight = max_weight

              def optimize(
                  self,
                  returns: pd.DataFrame,
                  current_weights: dict[str, float],
                  target_weights: dict[str, float]
              ) -> dict[str, float]:
                  """
                  ターンオーバー制約を満たす範囲でターゲットに近づける
                  """
                  n = len(returns.columns)

                  w_current = np.array([
                      current_weights.get(c, 0) for c in returns.columns
                  ])
                  w_target = np.array([
                      target_weights.get(c, 0) for c in returns.columns
                  ])

                  # ターゲットとの乖離を最小化
                  def objective(w):
                      return np.sum((w - w_target) ** 2)

                  constraints = [
                      {"type": "eq", "fun": lambda w: np.sum(w) - 1},
                      {"type": "ineq", "fun": lambda w:
                          self.max_turnover - np.sum(np.abs(w - w_current))}
                  ]

                  bounds = [(0, self.max_weight) for _ in range(n)]

                  result = minimize(
                      objective,
                      x0=w_current if w_current.sum() > 0 else np.ones(n) / n,
                      method="SLSQP",
                      bounds=bounds,
                      constraints=constraints
                  )

                  return dict(zip(returns.columns, result.x))
          ```
        priority: critical

      # =========================================================================
      # カテゴリ2: 機械学習の高度化
      # =========================================================================
      - task_id: task_017_5
        description: |
          src/ml/xgboost_stacking.py を新規作成せよ

          【XGBoostスタッキング】

          現在のRidgeスタッキングをXGBoostに置換。

          ```python
          import numpy as np
          import pandas as pd
          from typing import Optional

          try:
              import xgboost as xgb
              HAS_XGBOOST = True
          except ImportError:
              HAS_XGBOOST = False

          class XGBoostStacker:
              """
              XGBoostを使用したシグナルスタッキング

              特徴:
              - 非線形関係の捕捉
              - 特徴量重要度の可視化
              - 過学習防止（early stopping）
              """

              def __init__(
                  self,
                  n_estimators: int = 100,
                  max_depth: int = 3,
                  learning_rate: float = 0.1,
                  subsample: float = 0.8,
                  colsample_bytree: float = 0.8,
                  early_stopping_rounds: int = 10
              ):
                  if not HAS_XGBOOST:
                      raise ImportError("xgboost is required. Install with: pip install xgboost")

                  self.params = {
                      "n_estimators": n_estimators,
                      "max_depth": max_depth,
                      "learning_rate": learning_rate,
                      "subsample": subsample,
                      "colsample_bytree": colsample_bytree,
                      "objective": "reg:squarederror",
                      "random_state": 42,
                      "n_jobs": -1
                  }
                  self.early_stopping_rounds = early_stopping_rounds
                  self.model = None
                  self.feature_names = None

              def fit(
                  self,
                  X: pd.DataFrame,
                  y: pd.Series,
                  eval_set: Optional[tuple] = None
              ):
                  """
                  モデルを学習

                  Args:
                      X: シグナル特徴量
                      y: ターゲット（将来リターン）
                      eval_set: 検証セット (X_val, y_val)
                  """
                  self.feature_names = X.columns.tolist()

                  self.model = xgb.XGBRegressor(**self.params)

                  fit_params = {}
                  if eval_set is not None:
                      fit_params["eval_set"] = [eval_set]
                      fit_params["verbose"] = False

                  self.model.fit(X, y, **fit_params)

              def predict(
                  self,
                  X: pd.DataFrame
              ) -> np.ndarray:
                  """予測"""
                  return self.model.predict(X)

              def get_feature_importance(self) -> pd.Series:
                  """特徴量重要度を取得"""
                  if self.model is None:
                      raise ValueError("Model not fitted")

                  importance = self.model.feature_importances_
                  return pd.Series(importance, index=self.feature_names).sort_values(ascending=False)


          class LightGBMStacker:
              """LightGBMを使用したスタッキング（より高速）"""

              def __init__(
                  self,
                  n_estimators: int = 100,
                  max_depth: int = 3,
                  learning_rate: float = 0.1,
                  num_leaves: int = 31
              ):
                  try:
                      import lightgbm as lgb
                      self.lgb = lgb
                  except ImportError:
                      raise ImportError("lightgbm is required. Install with: pip install lightgbm")

                  self.params = {
                      "n_estimators": n_estimators,
                      "max_depth": max_depth,
                      "learning_rate": learning_rate,
                      "num_leaves": num_leaves,
                      "objective": "regression",
                      "random_state": 42,
                      "n_jobs": -1,
                      "verbose": -1
                  }
                  self.model = None

              def fit(self, X: pd.DataFrame, y: pd.Series):
                  self.model = self.lgb.LGBMRegressor(**self.params)
                  self.model.fit(X, y)

              def predict(self, X: pd.DataFrame) -> np.ndarray:
                  return self.model.predict(X)


          class EnsembleStacker:
              """複数モデルのアンサンブル"""

              def __init__(
                  self,
                  models: list = None,
                  weights: list = None
              ):
                  if models is None:
                      # デフォルト: Ridge + XGBoost
                      from sklearn.linear_model import Ridge
                      self.models = [
                          ("ridge", Ridge(alpha=1.0)),
                          ("xgb", XGBoostStacker() if HAS_XGBOOST else None)
                      ]
                      self.models = [(n, m) for n, m in self.models if m is not None]
                  else:
                      self.models = models

                  self.weights = weights or [1 / len(self.models)] * len(self.models)

              def fit(self, X: pd.DataFrame, y: pd.Series):
                  for name, model in self.models:
                      model.fit(X, y)

              def predict(self, X: pd.DataFrame) -> np.ndarray:
                  predictions = []
                  for (name, model), weight in zip(self.models, self.weights):
                      pred = model.predict(X)
                      predictions.append(pred * weight)
                  return np.sum(predictions, axis=0)
          ```
        priority: high

      - task_id: task_017_6
        description: |
          src/ml/return_predictor.py を新規作成せよ

          【LightGBMリターン予測】

          直接リターンを予測するモデル。

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, List

          class ReturnPredictor:
              """
              機械学習によるリターン直接予測

              特徴量:
              - テクニカル指標
              - ファンダメンタル
              - マクロ経済指標
              - センチメント
              """

              def __init__(
                  self,
                  model_type: str = "lightgbm",
                  prediction_horizon: int = 20,  # 20日先を予測
                  lookback_features: List[int] = [5, 10, 20, 60]
              ):
                  self.model_type = model_type
                  self.prediction_horizon = prediction_horizon
                  self.lookback_features = lookback_features
                  self.model = None
                  self.feature_names = None

              def create_features(
                  self,
                  prices: pd.DataFrame,
                  volumes: pd.DataFrame = None
              ) -> pd.DataFrame:
                  """
                  予測用特徴量を生成
                  """
                  features = pd.DataFrame(index=prices.index)

                  for asset in prices.columns:
                      price = prices[asset]

                      # リターン系
                      for lb in self.lookback_features:
                          features[f"{asset}_ret_{lb}"] = price.pct_change(lb)
                          features[f"{asset}_vol_{lb}"] = price.pct_change().rolling(lb).std()

                      # テクニカル
                      features[f"{asset}_rsi"] = self._compute_rsi(price)
                      features[f"{asset}_macd"] = self._compute_macd(price)
                      features[f"{asset}_bb_position"] = self._compute_bb_position(price)

                      # 相対強弱
                      if "SPY" in prices.columns and asset != "SPY":
                          features[f"{asset}_rel_str"] = (
                              price.pct_change(20) - prices["SPY"].pct_change(20)
                          )

                  return features.dropna()

              def create_target(
                  self,
                  prices: pd.DataFrame,
                  asset: str
              ) -> pd.Series:
                  """予測ターゲット（将来リターン）を生成"""
                  future_return = prices[asset].pct_change(self.prediction_horizon).shift(-self.prediction_horizon)
                  return future_return

              def fit(
                  self,
                  prices: pd.DataFrame,
                  target_asset: str
              ):
                  """モデル学習"""
                  X = self.create_features(prices)
                  y = self.create_target(prices, target_asset)

                  # アラインメント
                  common_idx = X.index.intersection(y.dropna().index)
                  X = X.loc[common_idx]
                  y = y.loc[common_idx]

                  self.feature_names = X.columns.tolist()

                  if self.model_type == "lightgbm":
                      import lightgbm as lgb
                      self.model = lgb.LGBMRegressor(
                          n_estimators=100,
                          max_depth=3,
                          learning_rate=0.05,
                          random_state=42,
                          verbose=-1
                      )
                  else:
                      from sklearn.ensemble import RandomForestRegressor
                      self.model = RandomForestRegressor(
                          n_estimators=100,
                          max_depth=5,
                          random_state=42
                      )

                  self.model.fit(X, y)

              def predict(
                  self,
                  prices: pd.DataFrame
              ) -> float:
                  """最新データで予測"""
                  X = self.create_features(prices)
                  if len(X) == 0:
                      return 0.0
                  return self.model.predict(X.iloc[[-1]])[0]

              def _compute_rsi(self, price: pd.Series, period: int = 14) -> pd.Series:
                  delta = price.diff()
                  gain = delta.where(delta > 0, 0).rolling(period).mean()
                  loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                  rs = gain / loss
                  return 100 - (100 / (1 + rs))

              def _compute_macd(self, price: pd.Series) -> pd.Series:
                  ema12 = price.ewm(span=12).mean()
                  ema26 = price.ewm(span=26).mean()
                  return ema12 - ema26

              def _compute_bb_position(self, price: pd.Series, period: int = 20) -> pd.Series:
                  ma = price.rolling(period).mean()
                  std = price.rolling(period).std()
                  return (price - ma) / (2 * std)
          ```
        priority: high

      - task_id: task_017_7
        description: |
          src/ml/ppo_allocator.py を新規作成せよ

          【深層強化学習（PPO）による配分最適化】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Tuple, List
          from dataclasses import dataclass

          @dataclass
          class RLConfig:
              """強化学習設定"""
              state_dim: int = 50
              action_dim: int = 10
              hidden_dim: int = 64
              learning_rate: float = 3e-4
              gamma: float = 0.99
              gae_lambda: float = 0.95
              clip_ratio: float = 0.2
              epochs: int = 10
              batch_size: int = 64


          class TradingEnvironment:
              """
              ポートフォリオ取引環境

              State: [価格特徴量, 現在ポジション, 市場状態]
              Action: 各アセットへの配分 (連続値)
              Reward: リスク調整リターン
              """

              def __init__(
                  self,
                  prices: pd.DataFrame,
                  lookback: int = 20,
                  transaction_cost: float = 0.001
              ):
                  self.prices = prices
                  self.returns = prices.pct_change().dropna()
                  self.lookback = lookback
                  self.transaction_cost = transaction_cost
                  self.n_assets = len(prices.columns)

                  self.current_step = lookback
                  self.current_weights = np.zeros(self.n_assets)
                  self.portfolio_value = 1.0

              def reset(self) -> np.ndarray:
                  """環境リセット"""
                  self.current_step = self.lookback
                  self.current_weights = np.ones(self.n_assets) / self.n_assets
                  self.portfolio_value = 1.0
                  return self._get_state()

              def _get_state(self) -> np.ndarray:
                  """状態を取得"""
                  # 過去リターン
                  hist_returns = self.returns.iloc[
                      self.current_step - self.lookback:self.current_step
                  ].values.flatten()

                  # ボラティリティ
                  vol = self.returns.iloc[
                      self.current_step - self.lookback:self.current_step
                  ].std().values

                  # 現在ポジション
                  state = np.concatenate([
                      hist_returns[-self.n_assets * 5:],  # 直近5日のリターン
                      vol,
                      self.current_weights
                  ])

                  return state

              def step(
                  self,
                  action: np.ndarray
              ) -> Tuple[np.ndarray, float, bool, dict]:
                  """
                  1ステップ実行

                  Args:
                      action: ターゲット配分

                  Returns:
                      (次状態, 報酬, 終了フラグ, 情報)
                  """
                  # アクションを正規化
                  action = np.clip(action, 0, 1)
                  action = action / (action.sum() + 1e-8)

                  # 取引コスト
                  turnover = np.sum(np.abs(action - self.current_weights))
                  cost = turnover * self.transaction_cost

                  # リターン計算
                  step_returns = self.returns.iloc[self.current_step].values
                  portfolio_return = np.sum(action * step_returns) - cost

                  # ポートフォリオ価値更新
                  self.portfolio_value *= (1 + portfolio_return)
                  self.current_weights = action
                  self.current_step += 1

                  # 報酬（Sharpe-like）
                  reward = portfolio_return / (np.std(step_returns) + 1e-8)

                  # 終了判定
                  done = self.current_step >= len(self.returns) - 1

                  return self._get_state(), reward, done, {"portfolio_value": self.portfolio_value}


          class SimplePPOAgent:
              """
              シンプルなPPOエージェント（PyTorchなしで動作）

              本格的な実装にはStable-Baselines3を推奨
              """

              def __init__(
                  self,
                  state_dim: int,
                  action_dim: int,
                  learning_rate: float = 0.001
              ):
                  self.state_dim = state_dim
                  self.action_dim = action_dim
                  self.lr = learning_rate

                  # シンプルな線形ポリシー（デモ用）
                  self.policy_weights = np.random.randn(state_dim, action_dim) * 0.01
                  self.policy_bias = np.zeros(action_dim)

              def get_action(
                  self,
                  state: np.ndarray,
                  deterministic: bool = False
              ) -> np.ndarray:
                  """アクション取得"""
                  logits = state @ self.policy_weights + self.policy_bias

                  # Softmax
                  exp_logits = np.exp(logits - logits.max())
                  probs = exp_logits / exp_logits.sum()

                  if deterministic:
                      return probs
                  else:
                      # 確率的サンプリング
                      noise = np.random.randn(self.action_dim) * 0.1
                      action = probs + noise
                      action = np.clip(action, 0, 1)
                      return action / action.sum()

              def update(
                  self,
                  states: np.ndarray,
                  actions: np.ndarray,
                  rewards: np.ndarray
              ):
                  """ポリシー更新（シンプルな勾配上昇）"""
                  # 報酬の標準化
                  rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)

                  # 勾配計算（簡略化）
                  for s, a, r in zip(states, actions, rewards):
                      grad = np.outer(s, a - self.get_action(s, deterministic=True))
                      self.policy_weights += self.lr * r * grad


          def train_ppo_allocator(
              prices: pd.DataFrame,
              n_episodes: int = 100
          ) -> SimplePPOAgent:
              """PPOエージェントを学習"""
              env = TradingEnvironment(prices)
              state_dim = env._get_state().shape[0]
              action_dim = env.n_assets

              agent = SimplePPOAgent(state_dim, action_dim)

              for episode in range(n_episodes):
                  state = env.reset()
                  states, actions, rewards = [], [], []

                  done = False
                  while not done:
                      action = agent.get_action(state)
                      next_state, reward, done, info = env.step(action)

                      states.append(state)
                      actions.append(action)
                      rewards.append(reward)

                      state = next_state

                  # エピソード終了後に更新
                  agent.update(
                      np.array(states),
                      np.array(actions),
                      np.array(rewards)
                  )

              return agent
          ```

          注: 本格的なPPO実装には stable-baselines3 を推奨:
          ```bash
          pip install stable-baselines3
          ```
        priority: high

      - task_id: task_017_8
        description: |
          src/ml/dynamic_ensemble_weights.py を新規作成せよ

          【動的アンサンブル重み学習】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, List

          class DynamicEnsembleWeightLearner:
              """
              アンサンブルモデルの重みを動的に学習

              過去のパフォーマンスに基づいて各モデルの重みを調整
              """

              def __init__(
                  self,
                  models: List[str],
                  lookback: int = 60,
                  decay_factor: float = 0.95,
                  min_weight: float = 0.05
              ):
                  self.models = models
                  self.lookback = lookback
                  self.decay_factor = decay_factor
                  self.min_weight = min_weight
                  self.performance_history = {m: [] for m in models}

              def update_performance(
                  self,
                  model_predictions: Dict[str, float],
                  actual_return: float
              ):
                  """各モデルの予測精度を記録"""
                  for model, pred in model_predictions.items():
                      error = (pred - actual_return) ** 2
                      self.performance_history[model].append(error)

                      # 履歴制限
                      if len(self.performance_history[model]) > self.lookback:
                          self.performance_history[model].pop(0)

              def compute_weights(self) -> Dict[str, float]:
                  """
                  パフォーマンスに基づく動的重み計算

                  重み ∝ 1 / (平均二乗誤差)
                  """
                  weights = {}

                  for model in self.models:
                      errors = self.performance_history[model]
                      if len(errors) < 5:
                          weights[model] = 1.0 / len(self.models)
                          continue

                      # 指数加重平均誤差
                      weighted_errors = []
                      for i, e in enumerate(errors):
                          weight = self.decay_factor ** (len(errors) - i - 1)
                          weighted_errors.append(e * weight)

                      avg_error = np.mean(weighted_errors)
                      weights[model] = 1.0 / (avg_error + 1e-8)

                  # 正規化 + 最小重み保証
                  total = sum(weights.values())
                  weights = {m: max(w / total, self.min_weight) for m, w in weights.items()}

                  # 再正規化
                  total = sum(weights.values())
                  weights = {m: w / total for m, w in weights.items()}

                  return weights


          class RegimeAwareEnsemble:
              """
              レジームに応じたアンサンブル重み

              異なる市場環境で異なるモデルが有効
              """

              def __init__(
                  self,
                  models: List[str],
                  regimes: List[str] = ["bull", "bear", "high_vol", "low_vol", "range"]
              ):
                  self.models = models
                  self.regimes = regimes

                  # レジーム別パフォーマンス履歴
                  self.regime_performance = {
                      regime: {model: [] for model in models}
                      for regime in regimes
                  }

              def update(
                  self,
                  regime: str,
                  model_predictions: Dict[str, float],
                  actual_return: float
              ):
                  """レジーム別にパフォーマンスを記録"""
                  for model, pred in model_predictions.items():
                      error = (pred - actual_return) ** 2
                      self.regime_performance[regime][model].append(error)

              def get_weights_for_regime(
                  self,
                  regime: str
              ) -> Dict[str, float]:
                  """現在のレジームに適した重みを取得"""
                  perf = self.regime_performance.get(regime, {})

                  weights = {}
                  for model in self.models:
                      errors = perf.get(model, [])
                      if len(errors) < 10:
                          weights[model] = 1.0 / len(self.models)
                      else:
                          avg_error = np.mean(errors[-60:])
                          weights[model] = 1.0 / (avg_error + 1e-8)

                  # 正規化
                  total = sum(weights.values())
                  return {m: w / total for m, w in weights.items()}
          ```
        priority: high

      # =========================================================================
      # カテゴリ3: Walk-Forward最適化の強化
      # =========================================================================
      - task_id: task_017_9
        description: |
          src/backtest/adaptive_window.py を新規作成せよ

          【適応的ウィンドウサイズ】

          ```python
          import numpy as np
          import pandas as pd

          class AdaptiveWindowSelector:
              """
              レジームに応じた学習ウィンドウサイズの自動調整

              高ボラ/レジーム変化時: 短いウィンドウ（速い適応）
              安定期: 長いウィンドウ（より多くのデータ）
              """

              def __init__(
                  self,
                  min_window: int = 126,   # 最小6ヶ月
                  max_window: int = 756,   # 最大3年
                  default_window: int = 504  # デフォルト2年
              ):
                  self.min_window = min_window
                  self.max_window = max_window
                  self.default_window = default_window

              def compute_optimal_window(
                  self,
                  returns: pd.Series,
                  volatility_regime: str,
                  regime_change_detected: bool
              ) -> int:
                  """
                  最適ウィンドウサイズを計算

                  Args:
                      returns: 過去リターン
                      volatility_regime: 現在のボラレジーム
                      regime_change_detected: レジーム変化が検出されたか
                  """
                  # ベースウィンドウ
                  window = self.default_window

                  # レジームに応じた調整
                  regime_multipliers = {
                      "crisis": 0.5,
                      "high_vol": 0.7,
                      "normal": 1.0,
                      "low_vol": 1.3,
                  }
                  multiplier = regime_multipliers.get(volatility_regime, 1.0)
                  window = int(window * multiplier)

                  # レジーム変化検出時は短縮
                  if regime_change_detected:
                      window = int(window * 0.6)

                  # 制約適用
                  window = max(self.min_window, min(self.max_window, window))

                  return window

              def detect_regime_change(
                  self,
                  returns: pd.Series,
                  lookback: int = 60,
                  threshold: float = 2.0
              ) -> bool:
                  """
                  レジーム変化を検出

                  直近のボラティリティが長期平均から大きく乖離した場合
                  """
                  if len(returns) < lookback * 2:
                      return False

                  recent_vol = returns.tail(lookback).std() * np.sqrt(252)
                  long_vol = returns.tail(lookback * 4).std() * np.sqrt(252)

                  vol_ratio = recent_vol / long_vol if long_vol > 0 else 1.0

                  return vol_ratio > threshold or vol_ratio < 1 / threshold


          class ExpandingWindowOptimizer:
              """
              Expanding Window方式の最適化

              時間とともに学習データを累積
              """

              def __init__(
                  self,
                  min_train_size: int = 252,
                  test_size: int = 63,
                  step_size: int = 21
              ):
                  self.min_train_size = min_train_size
                  self.test_size = test_size
                  self.step_size = step_size

              def generate_splits(
                  self,
                  n_samples: int
              ) -> list:
                  """
                  Expanding windowの分割を生成

                  Returns:
                      List of (train_start, train_end, test_start, test_end)
                  """
                  splits = []

                  train_end = self.min_train_size
                  while train_end + self.test_size <= n_samples:
                      train_start = 0  # 常に最初から
                      test_start = train_end
                      test_end = test_start + self.test_size

                      splits.append((train_start, train_end, test_start, test_end))

                      train_end += self.step_size

                  return splits
          ```
        priority: high

      - task_id: task_017_10
        description: |
          src/backtest/regime_triggered_reoptimization.py を新規作成せよ

          【レジーム変化での強制再最適化】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Callable, Optional
          from dataclasses import dataclass

          @dataclass
          class ReoptimizationTrigger:
              """再最適化トリガー条件"""
              triggered: bool
              reason: str
              severity: str  # "low", "medium", "high"
              recommended_action: str

          class RegimeTriggeredReoptimizer:
              """
              レジーム変化を検出して自動的に再最適化をトリガー

              トリガー条件:
              1. ボラティリティレジーム変化
              2. 相関構造の変化
              3. パフォーマンス劣化
              4. VIXスパイク
              """

              def __init__(
                  self,
                  vol_change_threshold: float = 0.5,
                  corr_change_threshold: float = 0.3,
                  performance_threshold: float = -0.10,
                  vix_spike_threshold: float = 30,
                  cooldown_days: int = 10
              ):
                  self.vol_change_threshold = vol_change_threshold
                  self.corr_change_threshold = corr_change_threshold
                  self.performance_threshold = performance_threshold
                  self.vix_spike_threshold = vix_spike_threshold
                  self.cooldown_days = cooldown_days

                  self.last_reoptimization = None
                  self.baseline_vol = None
                  self.baseline_corr = None

              def check_triggers(
                  self,
                  returns: pd.DataFrame,
                  vix_current: float,
                  portfolio_drawdown: float,
                  current_date: pd.Timestamp
              ) -> ReoptimizationTrigger:
                  """
                  再最適化が必要か判定

                  Returns:
                      ReoptimizationTrigger with decision
                  """
                  # クールダウン期間チェック
                  if self.last_reoptimization is not None:
                      days_since = (current_date - self.last_reoptimization).days
                      if days_since < self.cooldown_days:
                          return ReoptimizationTrigger(
                              triggered=False,
                              reason="Cooldown period active",
                              severity="low",
                              recommended_action="wait"
                          )

                  # 1. ボラティリティ変化チェック
                  current_vol = returns.tail(20).std().mean() * np.sqrt(252)
                  if self.baseline_vol is None:
                      self.baseline_vol = current_vol
                  vol_change = abs(current_vol / self.baseline_vol - 1)

                  if vol_change > self.vol_change_threshold:
                      self.last_reoptimization = current_date
                      self.baseline_vol = current_vol
                      return ReoptimizationTrigger(
                          triggered=True,
                          reason=f"Volatility regime change: {vol_change:.1%}",
                          severity="high",
                          recommended_action="full_reoptimization"
                      )

                  # 2. 相関構造変化チェック
                  current_corr = returns.tail(20).corr().values
                  current_corr = current_corr[~np.eye(current_corr.shape[0], dtype=bool)].mean()

                  if self.baseline_corr is None:
                      self.baseline_corr = current_corr
                  corr_change = abs(current_corr - self.baseline_corr)

                  if corr_change > self.corr_change_threshold:
                      self.last_reoptimization = current_date
                      self.baseline_corr = current_corr
                      return ReoptimizationTrigger(
                          triggered=True,
                          reason=f"Correlation structure change: {corr_change:.2f}",
                          severity="high",
                          recommended_action="covariance_reestimation"
                      )

                  # 3. VIXスパイクチェック
                  if vix_current > self.vix_spike_threshold:
                      self.last_reoptimization = current_date
                      return ReoptimizationTrigger(
                          triggered=True,
                          reason=f"VIX spike: {vix_current:.1f}",
                          severity="high",
                          recommended_action="risk_reduction"
                      )

                  # 4. パフォーマンス劣化チェック
                  if portfolio_drawdown < self.performance_threshold:
                      self.last_reoptimization = current_date
                      return ReoptimizationTrigger(
                          triggered=True,
                          reason=f"Performance deterioration: {portfolio_drawdown:.1%}",
                          severity="medium",
                          recommended_action="parameter_adjustment"
                      )

                  return ReoptimizationTrigger(
                      triggered=False,
                      reason="No trigger conditions met",
                      severity="low",
                      recommended_action="continue"
                  )

              def execute_reoptimization(
                  self,
                  trigger: ReoptimizationTrigger,
                  optimize_func: Callable,
                  **kwargs
              ) -> dict:
                  """
                  トリガーに応じた再最適化を実行
                  """
                  if not trigger.triggered:
                      return {"action": "none", "result": None}

                  if trigger.recommended_action == "full_reoptimization":
                      # 全パラメータ再最適化
                      result = optimize_func(full=True, **kwargs)

                  elif trigger.recommended_action == "covariance_reestimation":
                      # 共分散のみ再推定
                      result = optimize_func(covariance_only=True, **kwargs)

                  elif trigger.recommended_action == "risk_reduction":
                      # リスク削減モード
                      result = {"cash_increase": 0.20}

                  elif trigger.recommended_action == "parameter_adjustment":
                      # パラメータ微調整
                      result = optimize_func(incremental=True, **kwargs)

                  else:
                      result = None

                  return {
                      "action": trigger.recommended_action,
                      "reason": trigger.reason,
                      "result": result
                  }
          ```
        priority: critical

      - task_id: task_017_11
        description: |
          src/backtest/parameter_stability_filter.py を新規作成せよ

          【パラメータ安定性フィルタ】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, List
          from collections import deque

          class ParameterStabilityFilter:
              """
              パラメータの急激な変更を抑制

              過学習や一時的なノイズによる不安定な最適化結果を
              フィルタリングして安定したパラメータを維持
              """

              def __init__(
                  self,
                  max_change_rate: float = 0.3,
                  history_length: int = 5,
                  smoothing_factor: float = 0.3
              ):
                  self.max_change_rate = max_change_rate
                  self.history_length = history_length
                  self.smoothing_factor = smoothing_factor

                  self.param_history: Dict[str, deque] = {}

              def filter_parameters(
                  self,
                  new_params: Dict[str, float],
                  param_names: List[str] = None
              ) -> Dict[str, float]:
                  """
                  新しいパラメータをフィルタリング

                  1. 過去との変化率をチェック
                  2. 大きすぎる変化は抑制
                  3. 履歴の移動平均でスムージング
                  """
                  if param_names is None:
                      param_names = list(new_params.keys())

                  filtered = {}

                  for name in param_names:
                      new_value = new_params.get(name)
                      if new_value is None:
                          continue

                      # 履歴初期化
                      if name not in self.param_history:
                          self.param_history[name] = deque(maxlen=self.history_length)
                          self.param_history[name].append(new_value)
                          filtered[name] = new_value
                          continue

                      history = self.param_history[name]
                      last_value = history[-1]

                      # 変化率計算
                      if last_value != 0:
                          change_rate = abs(new_value - last_value) / abs(last_value)
                      else:
                          change_rate = abs(new_value)

                      # 変化率が大きすぎる場合は抑制
                      if change_rate > self.max_change_rate:
                          # 最大許容変化に制限
                          direction = 1 if new_value > last_value else -1
                          new_value = last_value * (1 + direction * self.max_change_rate)

                      # スムージング
                      smoothed_value = (
                          self.smoothing_factor * new_value +
                          (1 - self.smoothing_factor) * last_value
                      )

                      history.append(smoothed_value)
                      filtered[name] = smoothed_value

                  return filtered

              def get_stability_score(
                  self,
                  param_name: str
              ) -> float:
                  """
                  パラメータの安定性スコアを計算

                  Returns:
                      0-1 のスコア（1=非常に安定）
                  """
                  if param_name not in self.param_history:
                      return 0.5

                  history = list(self.param_history[param_name])
                  if len(history) < 2:
                      return 0.5

                  # 変動係数の逆数
                  std = np.std(history)
                  mean = np.mean(history)
                  cv = std / abs(mean) if mean != 0 else 1

                  # 0-1にスケール
                  stability = 1 / (1 + cv)
                  return stability

              def should_accept_new_params(
                  self,
                  new_params: Dict[str, float],
                  performance_improvement: float,
                  min_improvement: float = 0.05
              ) -> bool:
                  """
                  新しいパラメータを採用すべきか判定

                  パフォーマンス改善が十分でない場合は
                  現在のパラメータを維持
                  """
                  # パフォーマンス改善が閾値未満なら拒否
                  if performance_improvement < min_improvement:
                      return False

                  # 安定性スコアチェック
                  avg_stability = np.mean([
                      self.get_stability_score(name)
                      for name in new_params.keys()
                      if name in self.param_history
                  ])

                  # 安定性が低い場合は高い改善が必要
                  required_improvement = min_improvement * (2 - avg_stability)
                  return performance_improvement >= required_improvement
          ```
        priority: high

      # =========================================================================
      # カテゴリ4: 執行・取引最適化
      # =========================================================================
      - task_id: task_017_12
        description: |
          src/execution/timing_optimizer.py を新規作成せよ

          【取引タイミング最適化】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, Tuple

          class TradingTimingOptimizer:
              """
              取引タイミングの最適化

              - 曜日効果
              - 月初/月末効果
              - ボラティリティに基づくタイミング
              """

              def __init__(
                  self,
                  analyze_lookback: int = 252
              ):
                  self.analyze_lookback = analyze_lookback
                  self.day_of_week_effects = None
                  self.month_effects = None

              def analyze_timing_effects(
                  self,
                  returns: pd.DataFrame
              ):
                  """過去データからタイミング効果を分析"""
                  # 曜日効果
                  returns_with_dow = returns.copy()
                  returns_with_dow['dow'] = returns.index.dayofweek

                  self.day_of_week_effects = returns_with_dow.groupby('dow').mean().mean(axis=1)

                  # 月効果
                  returns_with_dom = returns.copy()
                  returns_with_dom['dom'] = returns.index.day

                  self.month_effects = returns_with_dom.groupby('dom').mean().mean(axis=1)

              def get_optimal_execution_day(
                  self,
                  trade_direction: str = "buy"
              ) -> int:
                  """
                  最適な執行曜日を取得

                  Args:
                      trade_direction: "buy" or "sell"

                  Returns:
                      0=Monday, 4=Friday
                  """
                  if self.day_of_week_effects is None:
                      return 0  # デフォルト: 月曜

                  if trade_direction == "buy":
                      # 買い: リターンが低い日に執行
                      return self.day_of_week_effects.idxmin()
                  else:
                      # 売り: リターンが高い日に執行
                      return self.day_of_week_effects.idxmax()

              def should_delay_execution(
                  self,
                  current_volatility: float,
                  baseline_volatility: float,
                  vol_threshold: float = 1.5
              ) -> Tuple[bool, str]:
                  """
                  高ボラティリティ時は執行を遅延すべきか判定
                  """
                  vol_ratio = current_volatility / baseline_volatility

                  if vol_ratio > vol_threshold:
                      return True, f"High volatility ({vol_ratio:.1f}x baseline)"

                  return False, "Normal volatility"


          class LiquidityScorer:
              """
              流動性スコアリング
              """

              def __init__(
                  self,
                  volume_lookback: int = 20
              ):
                  self.volume_lookback = volume_lookback

              def compute_liquidity_score(
                  self,
                  volumes: pd.Series
              ) -> float:
                  """
                  流動性スコアを計算

                  Returns:
                      0-1 のスコア（1=高流動性）
                  """
                  if len(volumes) < self.volume_lookback:
                      return 0.5

                  avg_volume = volumes.tail(self.volume_lookback).mean()
                  recent_volume = volumes.iloc[-1]

                  # 平均との比較
                  ratio = recent_volume / avg_volume if avg_volume > 0 else 1

                  # 0-1にスケール
                  score = min(ratio, 2) / 2
                  return score

              def adjust_weights_for_liquidity(
                  self,
                  weights: Dict[str, float],
                  liquidity_scores: Dict[str, float],
                  min_liquidity: float = 0.3
              ) -> Dict[str, float]:
                  """
                  流動性に基づいて配分を調整

                  低流動性銘柄の配分を減らす
                  """
                  adjusted = {}

                  for asset, weight in weights.items():
                      score = liquidity_scores.get(asset, 0.5)

                      if score < min_liquidity:
                          # 低流動性: 配分削減
                          adjusted[asset] = weight * (score / min_liquidity)
                      else:
                          adjusted[asset] = weight

                  # 正規化
                  total = sum(adjusted.values())
                  if total > 0:
                      adjusted = {k: v / total for k, v in adjusted.items()}

                  return adjusted
          ```
        priority: high

      # =========================================================================
      # カテゴリ5: リスク管理の高度化
      # =========================================================================
      - task_id: task_017_13
        description: |
          src/risk/risk_budgeting.py を新規作成せよ

          【リスクバジェッティング】

          ```python
          import numpy as np
          import pandas as pd
          from scipy.optimize import minimize

          class RiskBudgetingAllocator:
              """
              リスクバジェッティング配分

              各アセットのリスク貢献度を目標に合わせて配分
              """

              def __init__(
                  self,
                  risk_budgets: Dict[str, float] = None,
                  max_weight: float = 0.25
              ):
                  self.risk_budgets = risk_budgets  # {"SPY": 0.3, "TLT": 0.2, ...}
                  self.max_weight = max_weight

              def compute_risk_contribution(
                  self,
                  weights: np.ndarray,
                  cov: np.ndarray
              ) -> np.ndarray:
                  """
                  各アセットのリスク貢献度を計算

                  RC_i = w_i * (Σw)_i / σ_p
                  """
                  port_vol = np.sqrt(weights @ cov @ weights)
                  marginal_risk = cov @ weights
                  risk_contribution = weights * marginal_risk / port_vol
                  return risk_contribution

              def optimize(
                  self,
                  returns: pd.DataFrame,
                  target_budgets: Dict[str, float] = None
              ) -> Dict[str, float]:
                  """
                  リスクバジェット配分を最適化

                  目的: Σ(RC_i/RC_target - 1)^2 を最小化
                  """
                  assets = returns.columns.tolist()
                  n = len(assets)
                  cov = returns.cov().values * 252

                  # デフォルト: 均等リスクバジェット
                  if target_budgets is None:
                      target_budgets = {a: 1/n for a in assets}

                  target_rc = np.array([target_budgets.get(a, 1/n) for a in assets])
                  target_rc = target_rc / target_rc.sum()  # 正規化

                  def objective(w):
                      rc = self.compute_risk_contribution(w, cov)
                      rc_normalized = rc / rc.sum()
                      return np.sum((rc_normalized / target_rc - 1) ** 2)

                  constraints = [
                      {"type": "eq", "fun": lambda w: np.sum(w) - 1}
                  ]

                  bounds = [(0.01, self.max_weight) for _ in range(n)]

                  result = minimize(
                      objective,
                      x0=np.ones(n) / n,
                      method="SLSQP",
                      bounds=bounds,
                      constraints=constraints
                  )

                  return dict(zip(assets, result.x))

              def analyze_risk_contribution(
                  self,
                  weights: Dict[str, float],
                  returns: pd.DataFrame
              ) -> pd.DataFrame:
                  """リスク貢献度分析レポート"""
                  assets = list(weights.keys())
                  w = np.array([weights[a] for a in assets])
                  cov = returns[assets].cov().values * 252

                  rc = self.compute_risk_contribution(w, cov)
                  rc_pct = rc / rc.sum()

                  return pd.DataFrame({
                      "weight": w,
                      "risk_contribution": rc,
                      "risk_contribution_pct": rc_pct,
                      "weight_to_risk_ratio": w / rc_pct
                  }, index=assets)
          ```
        priority: high

      - task_id: task_017_14
        description: |
          src/risk/stress_testing.py を新規作成せよ

          【ストレステスト自動化】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, List
          from dataclasses import dataclass

          @dataclass
          class StressScenario:
              """ストレスシナリオ定義"""
              name: str
              description: str
              shocks: Dict[str, float]  # {"SPY": -0.20, "TLT": 0.05, ...}
              correlation_multiplier: float = 1.5

          # 過去のクライシスシナリオ
          HISTORICAL_SCENARIOS = [
              StressScenario(
                  name="GFC_2008",
                  description="Global Financial Crisis 2008",
                  shocks={"SPY": -0.40, "QQQ": -0.45, "TLT": 0.20, "GLD": 0.10, "VNQ": -0.50},
                  correlation_multiplier=2.0
              ),
              StressScenario(
                  name="COVID_2020",
                  description="COVID-19 Crash March 2020",
                  shocks={"SPY": -0.34, "QQQ": -0.28, "TLT": 0.15, "GLD": -0.05, "USO": -0.70},
                  correlation_multiplier=1.8
              ),
              StressScenario(
                  name="Rate_Hike_2022",
                  description="Fed Rate Hike 2022",
                  shocks={"SPY": -0.25, "QQQ": -0.33, "TLT": -0.30, "GLD": -0.05},
                  correlation_multiplier=1.3
              ),
              StressScenario(
                  name="Flash_Crash",
                  description="Flash Crash Scenario",
                  shocks={"SPY": -0.10, "QQQ": -0.12, "TLT": 0.02},
                  correlation_multiplier=1.5
              ),
              StressScenario(
                  name="Stagflation",
                  description="Stagflation Scenario",
                  shocks={"SPY": -0.20, "QQQ": -0.25, "TLT": -0.15, "GLD": 0.15, "USO": 0.30},
                  correlation_multiplier=1.4
              ),
          ]


          class StressTester:
              """
              ポートフォリオのストレステスト
              """

              def __init__(
                  self,
                  scenarios: List[StressScenario] = None
              ):
                  self.scenarios = scenarios or HISTORICAL_SCENARIOS

              def run_stress_test(
                  self,
                  weights: Dict[str, float],
                  scenario: StressScenario
              ) -> Dict:
                  """
                  単一シナリオのストレステスト実行
                  """
                  portfolio_loss = 0
                  asset_impacts = {}

                  for asset, weight in weights.items():
                      shock = scenario.shocks.get(asset, 0)
                      impact = weight * shock
                      portfolio_loss += impact
                      asset_impacts[asset] = {
                          "weight": weight,
                          "shock": shock,
                          "impact": impact
                      }

                  return {
                      "scenario": scenario.name,
                      "description": scenario.description,
                      "portfolio_loss": portfolio_loss,
                      "asset_impacts": asset_impacts,
                      "survives": portfolio_loss > -0.25  # 25%以上の損失で失敗
                  }

              def run_all_stress_tests(
                  self,
                  weights: Dict[str, float]
              ) -> pd.DataFrame:
                  """全シナリオでストレステスト実行"""
                  results = []

                  for scenario in self.scenarios:
                      result = self.run_stress_test(weights, scenario)
                      results.append({
                          "scenario": result["scenario"],
                          "description": result["description"],
                          "portfolio_loss": result["portfolio_loss"],
                          "survives": result["survives"]
                      })

                  return pd.DataFrame(results)

              def compute_worst_case_loss(
                  self,
                  weights: Dict[str, float]
              ) -> float:
                  """最悪ケースの損失を計算"""
                  losses = [
                      self.run_stress_test(weights, s)["portfolio_loss"]
                      for s in self.scenarios
                  ]
                  return min(losses)

              def suggest_hedges(
                  self,
                  weights: Dict[str, float],
                  max_hedge_cost: float = 0.05
              ) -> Dict:
                  """
                  ストレス耐性を高めるヘッジ提案
                  """
                  results = self.run_all_stress_tests(weights)
                  worst_scenario = results.loc[results["portfolio_loss"].idxmin()]

                  suggestions = []

                  if worst_scenario["portfolio_loss"] < -0.20:
                      # 大きな損失が予想される場合
                      suggestions.append({
                          "action": "Increase TLT allocation",
                          "reason": "Bond hedge for equity drawdown"
                      })
                      suggestions.append({
                          "action": "Add GLD position",
                          "reason": "Safe haven during crisis"
                      })
                      suggestions.append({
                          "action": "Increase cash",
                          "reason": "Reduce overall exposure"
                      })

                  return {
                      "worst_scenario": worst_scenario["scenario"],
                      "expected_loss": worst_scenario["portfolio_loss"],
                      "suggestions": suggestions
                  }
          ```
        priority: high

      # =========================================================================
      # カテゴリ6: バックテスト・検証の強化
      # =========================================================================
      - task_id: task_017_15
        description: |
          src/backtest/synthetic_data.py を新規作成せよ

          【合成データ生成】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Dict, Tuple

          class SyntheticDataGenerator:
              """
              統計的検証のための合成データ生成

              - ブートストラップ
              - モンテカルロシミュレーション
              - ブロックブートストラップ（時系列依存性保持）
              """

              def __init__(
                  self,
                  random_seed: int = 42
              ):
                  self.rng = np.random.default_rng(random_seed)

              def bootstrap_returns(
                  self,
                  returns: pd.DataFrame,
                  n_samples: int = 1000,
                  sample_size: int = None
              ) -> list:
                  """
                  単純ブートストラップ

                  時系列依存性は考慮しない
                  """
                  if sample_size is None:
                      sample_size = len(returns)

                  samples = []
                  for _ in range(n_samples):
                      idx = self.rng.choice(len(returns), size=sample_size, replace=True)
                      sample = returns.iloc[idx].reset_index(drop=True)
                      samples.append(sample)

                  return samples

              def block_bootstrap(
                  self,
                  returns: pd.DataFrame,
                  n_samples: int = 1000,
                  block_size: int = 20
              ) -> list:
                  """
                  ブロックブートストラップ

                  時系列の自己相関構造を保持
                  """
                  n = len(returns)
                  n_blocks = n // block_size

                  samples = []
                  for _ in range(n_samples):
                      # ランダムにブロックを選択
                      selected_blocks = self.rng.choice(n_blocks, size=n_blocks, replace=True)

                      sample_data = []
                      for block_idx in selected_blocks:
                          start = block_idx * block_size
                          end = start + block_size
                          sample_data.append(returns.iloc[start:end])

                      sample = pd.concat(sample_data, ignore_index=True)
                      samples.append(sample)

                  return samples

              def monte_carlo_simulation(
                  self,
                  returns: pd.DataFrame,
                  n_simulations: int = 1000,
                  n_days: int = 252
              ) -> list:
                  """
                  モンテカルロシミュレーション

                  過去の統計量を使用して将来パスを生成
                  """
                  mean_returns = returns.mean().values
                  cov_matrix = returns.cov().values

                  simulations = []
                  for _ in range(n_simulations):
                      sim_returns = self.rng.multivariate_normal(
                          mean_returns, cov_matrix, size=n_days
                      )
                      sim_df = pd.DataFrame(sim_returns, columns=returns.columns)
                      simulations.append(sim_df)

                  return simulations

              def regime_aware_simulation(
                  self,
                  returns: pd.DataFrame,
                  regime_labels: pd.Series,
                  n_simulations: int = 1000,
                  n_days: int = 252
              ) -> list:
                  """
                  レジーム考慮シミュレーション

                  異なるレジームの統計量を使い分け
                  """
                  # レジーム別統計量
                  regime_stats = {}
                  for regime in regime_labels.unique():
                      mask = regime_labels == regime
                      regime_returns = returns[mask]
                      regime_stats[regime] = {
                          "mean": regime_returns.mean().values,
                          "cov": regime_returns.cov().values,
                          "prob": mask.mean()
                      }

                  simulations = []
                  for _ in range(n_simulations):
                      sim_returns = []

                      for _ in range(n_days):
                          # レジームをサンプリング
                          regimes = list(regime_stats.keys())
                          probs = [regime_stats[r]["prob"] for r in regimes]
                          regime = self.rng.choice(regimes, p=probs)

                          # そのレジームの分布からサンプリング
                          stats = regime_stats[regime]
                          daily_return = self.rng.multivariate_normal(
                              stats["mean"], stats["cov"]
                          )
                          sim_returns.append(daily_return)

                      sim_df = pd.DataFrame(sim_returns, columns=returns.columns)
                      simulations.append(sim_df)

                  return simulations


          class StatisticalSignificanceTester:
              """
              戦略の統計的有意性検定
              """

              def __init__(
                  self,
                  n_bootstrap: int = 1000,
                  confidence_level: float = 0.95
              ):
                  self.n_bootstrap = n_bootstrap
                  self.confidence_level = confidence_level
                  self.generator = SyntheticDataGenerator()

              def test_sharpe_significance(
                  self,
                  strategy_returns: pd.Series,
                  benchmark_returns: pd.Series = None
              ) -> Dict:
                  """
                  Sharpe Ratioの統計的有意性を検定
                  """
                  observed_sharpe = (
                      strategy_returns.mean() / strategy_returns.std() * np.sqrt(252)
                  )

                  # ブートストラップでSharpeの分布を推定
                  bootstrap_sharpes = []
                  for _ in range(self.n_bootstrap):
                      sample = strategy_returns.sample(
                          n=len(strategy_returns), replace=True
                      )
                      boot_sharpe = sample.mean() / sample.std() * np.sqrt(252)
                      bootstrap_sharpes.append(boot_sharpe)

                  bootstrap_sharpes = np.array(bootstrap_sharpes)

                  # 信頼区間
                  alpha = 1 - self.confidence_level
                  ci_lower = np.percentile(bootstrap_sharpes, alpha / 2 * 100)
                  ci_upper = np.percentile(bootstrap_sharpes, (1 - alpha / 2) * 100)

                  # p値（Sharpe > 0の検定）
                  p_value = (bootstrap_sharpes <= 0).mean()

                  return {
                      "observed_sharpe": observed_sharpe,
                      "ci_lower": ci_lower,
                      "ci_upper": ci_upper,
                      "p_value": p_value,
                      "significant": p_value < alpha,
                      "bootstrap_mean": bootstrap_sharpes.mean(),
                      "bootstrap_std": bootstrap_sharpes.std()
                  }
          ```
        priority: high

      - task_id: task_017_16
        description: |
          src/backtest/purged_kfold.py を新規作成せよ

          【Purged K-Fold クロスバリデーション】

          ```python
          import numpy as np
          import pandas as pd
          from typing import Iterator, Tuple

          class PurgedKFold:
              """
              Purged K-Fold Cross-Validation

              時系列データ用のCV。データリークを防ぐため:
              1. テストセットの前後にpurge期間を設ける
              2. embargo期間でテスト後のデータを除外
              """

              def __init__(
                  self,
                  n_splits: int = 5,
                  purge_gap: int = 5,
                  embargo_pct: float = 0.01
              ):
                  self.n_splits = n_splits
                  self.purge_gap = purge_gap
                  self.embargo_pct = embargo_pct

              def split(
                  self,
                  X: pd.DataFrame
              ) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
                  """
                  Purged K-Fold分割を生成

                  Yields:
                      (train_indices, test_indices)
                  """
                  n = len(X)
                  fold_size = n // self.n_splits
                  embargo = int(n * self.embargo_pct)

                  for i in range(self.n_splits):
                      test_start = i * fold_size
                      test_end = test_start + fold_size

                      if i == self.n_splits - 1:
                          test_end = n

                      # テストインデックス
                      test_indices = np.arange(test_start, test_end)

                      # 訓練インデックス（purge + embargo適用）
                      train_indices = []

                      # テスト前のデータ（purge適用）
                      train_before_end = test_start - self.purge_gap
                      if train_before_end > 0:
                          train_indices.extend(range(0, train_before_end))

                      # テスト後のデータ（embargo適用）
                      train_after_start = test_end + embargo
                      if train_after_start < n:
                          train_indices.extend(range(train_after_start, n))

                      yield np.array(train_indices), test_indices


          class CombinatorialPurgedCV:
              """
              Combinatorial Purged Cross-Validation

              より厳密な検証のため、複数の分割パターンを組み合わせ
              """

              def __init__(
                  self,
                  n_splits: int = 5,
                  n_test_splits: int = 2,
                  purge_gap: int = 5
              ):
                  self.n_splits = n_splits
                  self.n_test_splits = n_test_splits
                  self.purge_gap = purge_gap

              def split(
                  self,
                  X: pd.DataFrame
              ) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
                  """組み合わせ分割を生成"""
                  from itertools import combinations

                  n = len(X)
                  fold_size = n // self.n_splits

                  # 全foldのインデックス
                  folds = []
                  for i in range(self.n_splits):
                      start = i * fold_size
                      end = start + fold_size if i < self.n_splits - 1 else n
                      folds.append(np.arange(start, end))

                  # n_test_splits個のfoldをテストとする全組み合わせ
                  for test_fold_indices in combinations(range(self.n_splits), self.n_test_splits):
                      test_indices = np.concatenate([folds[i] for i in test_fold_indices])

                      # 訓練用fold（purge適用）
                      train_indices = []
                      for i in range(self.n_splits):
                          if i not in test_fold_indices:
                              fold = folds[i]

                              # purge: テストfoldに隣接する場合は除外
                              is_adjacent = any(
                                  abs(i - t) <= 1 for t in test_fold_indices
                              )
                              if is_adjacent:
                                  # 端をpurge
                                  fold = fold[self.purge_gap:-self.purge_gap] if len(fold) > 2 * self.purge_gap else []

                              if len(fold) > 0:
                                  train_indices.extend(fold)

                      if len(train_indices) > 0:
                          yield np.array(train_indices), test_indices
          ```
        priority: high

      # =========================================================================
      # 統合タスク
      # =========================================================================
      - task_id: task_017_17
        description: |
          全ての新機能を統合し、パイプラインを更新せよ

          【統合対象】
          - 配分最適化: NCO, Black-Litterman, CVaR, コスト最適化
          - 機械学習: XGBoost, LightGBM, PPO, 動的アンサンブル
          - Walk-Forward: 適応ウィンドウ, レジームトリガー, 安定性フィルタ
          - 執行: タイミング, 流動性
          - リスク: バジェッティング, ストレステスト
          - 検証: 合成データ, Purged K-Fold

          【config/default.yaml 追加】
          ```yaml
          cmd_017_features:
            # 配分最適化
            allocation:
              method: "nco"  # hrp | nco | black_litterman | cvar
              nco:
                n_clusters: 5
                intra_method: "min_variance"
              black_litterman:
                tau: 0.05
                risk_aversion: 2.5
              cvar:
                alpha: 0.05
              transaction_cost:
                enabled: true
                cost_aversion: 1.0

            # 機械学習
            ml:
              stacking_model: "xgboost"  # ridge | xgboost | lightgbm | ensemble
              return_predictor:
                enabled: true
                model: "lightgbm"
              ppo:
                enabled: false  # 計算コスト高
              dynamic_ensemble:
                enabled: true
                decay_factor: 0.95

            # Walk-Forward
            walk_forward:
              adaptive_window: true
              regime_triggered_reopt: true
              stability_filter: true
              max_change_rate: 0.3

            # 執行
            execution:
              timing_optimization: true
              liquidity_scoring: true

            # リスク
            risk:
              budgeting:
                enabled: true
                method: "equal_risk"
              stress_testing:
                enabled: true
                scenarios: "all"

            # 検証
            validation:
              synthetic_data:
                enabled: true
                method: "block_bootstrap"
                n_samples: 1000
              cv_method: "purged_kfold"
              n_splits: 5
              purge_gap: 5
          ```
        priority: critical

    execution_strategy: |
      【Phase 1: 配分最適化（並列）】
      task_017_1: NCO
      task_017_2: Black-Litterman
      task_017_3: CVaR
      task_017_4: トランザクションコスト最適化

      【Phase 2: 機械学習（並列）】
      task_017_5: XGBoostスタッキング
      task_017_6: LightGBMリターン予測
      task_017_7: PPO配分
      task_017_8: 動的アンサンブル重み

      【Phase 3: Walk-Forward（並列）】
      task_017_9: 適応ウィンドウ
      task_017_10: レジームトリガー再最適化
      task_017_11: パラメータ安定性フィルタ

      【Phase 4: 執行・リスク（並列）】
      task_017_12: タイミング最適化
      task_017_13: リスクバジェッティング
      task_017_14: ストレステスト

      【Phase 5: 検証（並列）】
      task_017_15: 合成データ生成
      task_017_16: Purged K-Fold

      【Phase 6: 統合】
      task_017_17: 全機能統合

      足軽の割り当て:
      - 足軽1: task_017_1, task_017_2（NCO, BL）
      - 足軽2: task_017_3, task_017_4（CVaR, コスト）
      - 足軽3: task_017_5, task_017_6（XGB, LGBM）
      - 足軽4: task_017_7, task_017_8（PPO, 動的アンサンブル）
      - 足軽5: task_017_9, task_017_10, task_017_11（WF強化）
      - 足軽6: task_017_12, task_017_13（執行, バジェット）
      - 足軽7: task_017_14, task_017_15, task_017_16（ストレス, 検証）
      - 足軽8: task_017_17（統合）

    expected_outcome: |
      【期待効果】

      | カテゴリ | 施策数 | 期待効果 |
      |----------|--------|----------|
      | 配分最適化 | 4 | Sharpe +0.1-0.2 |
      | 機械学習 | 4 | リターン +2-5% |
      | Walk-Forward | 3 | 安定性向上 |
      | 執行 | 2 | コスト -0.3% |
      | リスク | 2 | MDD改善 |
      | 検証 | 2 | 信頼性向上 |

      【累積目標】
      - Sharpe: 1.2-1.5 → 1.5-2.0
      - 年率リターン: 28-35% → 35-45%
      - MDD: 15-18% → 12-15%

      【依存ライブラリ】
      - xgboost（オプション）
      - lightgbm（オプション）
      - stable-baselines3（PPO用、オプション）

      【データ要件】
      全て無料（yfinance, FRED）

  - id: cmd_018
    timestamp: "2026-01-29T04:30:00"
    command: "データ品質チェック緩和 - OHLC不整合閾値の導入"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      【背景】
      cmd_011の15年バックテストで重大な問題が発覚。
      SPY, IWM, EEM, TLT, VNQ等の主要銘柄がOHLC品質チェックで除外され、
      ポートフォリオが実質的に機能していなかった。

      【検証結果】
      10銘柄中9銘柄が除外されていた（QQQのみ通過）

      | 銘柄 | 総行数 | 不整合数 | 不整合率 |
      |------|--------|----------|----------|
      | SPY  | 3,769  | 2        | 0.0531%  |
      | IWM  | 3,769  | 1        | 0.0265%  |
      | EEM  | 3,769  | 6        | 0.1592%  |
      | TLT  | 3,769  | 2        | 0.0531%  |
      | VNQ  | 3,769  | 4        | 0.1061%  |
      | QQQ  | 3,769  | 0        | 0.0000%  |
      | EFA  | 3,769  | 2        | 0.0531%  |
      | GLD  | 3,769  | 1        | 0.0265%  |
      | SLV  | 3,769  | 1        | 0.0265%  |
      | USO  | 3,769  | 1        | 0.0265%  |

      最大不整合率: 0.1592%（EEM）
      → 0.5%閾値で全銘柄が有効化される

      【原因】
      src/data/quality_checker.py の _check_ohlc_consistency() が
      不整合が1件でもあると severity="error" として銘柄を除外していた。

      【修正方針】
      1. 閾値を導入し、閾値以下なら warning（除外しない）
      2. 閾値超過時のみ error（除外）
      3. config で閾値を設定可能にする

    tasks:
      - task_id: task_018_1
        description: |
          config/default.yaml にOHLC不整合閾値を追加せよ

          【変更箇所】
          data.quality セクションに以下を追加:

          ```yaml
          quality:
            max_missing_rate: 0.05
            max_consecutive_missing: 5
            spike_threshold_pct: 50.0
            min_volume_threshold: 0
            check_monotonic_time: true
            # 新規追加
            ohlc_inconsistency_threshold: 0.005  # 0.5% = 閾値
          ```

          【設計根拠】
          - 検証結果: 最大不整合率0.1592%（EEM）
          - 0.5%閾値で全銘柄が有効化
          - 余裕を持たせて0.5%に設定
        priority: critical

      - task_id: task_018_2
        description: |
          src/config/settings.py の DataQualityConfig を更新せよ

          【変更内容】
          DataQualityConfig クラスに新フィールドを追加:

          ```python
          @dataclass
          class DataQualityConfig:
              max_missing_rate: float = 0.05
              max_consecutive_missing: int = 5
              spike_threshold_pct: float = 50.0
              min_volume_threshold: int = 0
              check_monotonic_time: bool = True
              # 新規追加
              ohlc_inconsistency_threshold: float = 0.005  # 0.5%
          ```
        priority: critical

      - task_id: task_018_3
        description: |
          src/data/quality_checker.py の _check_ohlc_consistency() を修正せよ

          【現状コード（Line 491-531）】
          ```python
          def _check_ohlc_consistency(self, df: pl.DataFrame) -> QualityCheckResult:
              # ... 不整合チェック ...
              total_invalid = len(invalid_hl) + len(invalid_open) + len(invalid_close)
              passed = total_invalid == 0

              return QualityCheckResult(
                  check_name="ohlc_consistency",
                  passed=passed,
                  value=float(total_invalid),
                  message=(...),
                  severity="error" if not passed else "info",  # ← 問題箇所
              )
          ```

          【修正後コード】
          ```python
          def _check_ohlc_consistency(self, df: pl.DataFrame) -> QualityCheckResult:
              # ... 不整合チェック（既存ロジック維持） ...

              total_invalid = len(invalid_hl) + len(invalid_open) + len(invalid_close)
              total_rows = len(df)

              # 不整合率を計算
              inconsistency_rate = total_invalid / total_rows if total_rows > 0 else 0.0

              # 閾値との比較
              threshold = self.config.ohlc_inconsistency_threshold
              passed = total_invalid == 0

              # 閾値以下なら warning（除外しない）、超過なら error（除外）
              if not passed:
                  if inconsistency_rate <= threshold:
                      severity = "warning"
                      message = (
                          f"Found {total_invalid} OHLC inconsistencies "
                          f"({inconsistency_rate:.4%}), within threshold {threshold:.2%}"
                      )
                  else:
                      severity = "error"
                      message = (
                          f"Found {total_invalid} OHLC inconsistencies "
                          f"({inconsistency_rate:.4%}), exceeds threshold {threshold:.2%}"
                      )
              else:
                  severity = "info"
                  message = ""

              return QualityCheckResult(
                  check_name="ohlc_consistency",
                  passed=passed,
                  value=float(total_invalid),
                  threshold=threshold,
                  message=message,
                  severity=severity,
              )
          ```

          【重要ポイント】
          - passed は従来通り（不整合0件で True）
          - severity が新ロジック（閾値ベース）
          - 除外判定は severity="error" のみで行われる（Line 156-159）
        priority: critical

      - task_id: task_018_4
        description: |
          修正後の動作確認テストを実行せよ

          【テスト内容】
          1. 10銘柄（SPY, IWM, EEM, TLT, VNQ, QQQ, EFA, GLD, SLV, USO）で
             品質チェックを実行
          2. 除外される銘柄がないことを確認
          3. 警告ログが適切に出力されることを確認

          【テストスクリプト】
          ```python
          import sys
          sys.path.insert(0, ".")

          from src.data.quality_checker import DataQualityChecker
          from src.config.settings import Settings
          import yfinance as yf
          import polars as pl

          settings = Settings.load()
          checker = DataQualityChecker(settings)

          tickers = ["SPY", "IWM", "EEM", "TLT", "VNQ", "QQQ", "EFA", "GLD", "SLV", "USO"]

          for ticker in tickers:
              df = yf.download(ticker, period="15y", progress=False)
              df = df.reset_index()
              df.columns = [c.lower() if isinstance(c, str) else c[0].lower() for c in df.columns]
              pl_df = pl.from_pandas(df)

              report = checker.check(pl_df, ticker)
              status = "EXCLUDED" if report.is_excluded else "OK"
              print(f"{ticker}: {status}")

              if report.is_excluded:
                  print(f"  Reason: {report.exclusion_reason}")

          # 期待結果: 全銘柄が "OK" となること
          ```

          【成功基準】
          - 10銘柄全てが "OK"（除外されない）
          - OHLC不整合のある銘柄は warning ログが出力される
        priority: high

    execution_strategy: |
      【実行順序】
      Phase 1（並列）: task_018_1 + task_018_2（設定変更）
      Phase 2: task_018_3（コード修正）
      Phase 3: task_018_4（動作確認）

      【足軽割り当て】
      - 足軽1: task_018_1（config/default.yaml）
      - 足軽2: task_018_2（settings.py）
      - 足軽3: task_018_3（quality_checker.py）
      - 足軽4: task_018_4（動作確認テスト）

    expected_outcome: |
      【期待効果】
      - 除外銘柄: 9/10 → 0/10
      - cmd_011の15年バックテストが正常に機能
      - SPY等の主要銘柄がポートフォリオに含まれるようになる

      【パフォーマンス改善予測】
      修正前: Sharpe -3.18, リターン -0.01%（ほぼ機能せず）
      修正後: Sharpe 0.8-1.2, リターン 10-20%（正常動作）

      【リスク】
      - 低: 閾値0.5%は十分な余裕があり、不正データを見逃すリスクは極めて低い
      - 最大不整合率0.16%に対して3倍以上のマージン

  - id: cmd_019
    timestamp: "2026-01-29T05:00:00"
    command: "15年バックテスト検証＆自律改善サイクル"
    project: auto_allocation_system
    priority: critical
    status: stopped
    stopped_reason: "cmd_022高速化完了により不要。cmd_020で包括的に実施"
    depends_on: [cmd_018]
    context: |
      【背景】
      cmd_018でデータ品質チェック緩和を実施後、システム全体の性能を検証する。
      日次・月次両方のリバランスで15年バックテストを実行し、
      主要指数との比較レポートを作成。その後、自律的に改善案を提示・実装する。

      【目標】
      - Sharpe Ratio >= 1.0
      - 年率リターン >= SPY（約13.7%）
      - MDD < SPY（約33.7%）
      - 70%以上の年でSPYを上回る

      【比較対象ベンチマーク】
      1. SPY（S&P 500）- Buy & Hold
      2. 60/40 Portfolio（SPY 60% / TLT 40%）
      3. QQQ（NASDAQ 100）- Buy & Hold
      4. AGG（米国総合債券）- Buy & Hold

    tasks:
      - task_id: task_019_1
        description: |
          15年月次バックテストを実行せよ

          【実行コマンド】
          ```bash
          cd multi-asset-portfolio
          python -m src.main --backtest \
            --universe SPY,QQQ,IWM,EFA,EEM,TLT,GLD,SLV,USO,VNQ \
            --start 2010-01-01 \
            --end 2024-12-31 \
            --rebalance monthly \
            --capital 10000 \
            --cost-bps 10
          ```

          【出力ファイル】
          - results/backtest_15year_monthly_v2.json

          【収集指標】
          - 最終資産額、累計リターン、年率リターン
          - Sharpe Ratio、Sortino Ratio
          - MDD、Calmar Ratio
          - 年別リターン（15年分）
          - 月別勝率、平均ターンオーバー
        priority: critical

      - task_id: task_019_2
        description: |
          15年日次バックテストを実行せよ

          【実行コマンド】
          ```bash
          cd multi-asset-portfolio
          python -m src.main --backtest \
            --universe SPY,QQQ,IWM,EFA,EEM,TLT,GLD,SLV,USO,VNQ \
            --start 2010-01-01 \
            --end 2024-12-31 \
            --rebalance daily \
            --capital 10000 \
            --cost-bps 10
          ```

          【出力ファイル】
          - results/backtest_15year_daily_v2.json

          【注意】
          - 日次は処理時間が長い（数時間の見込み）
          - バックグラウンド実行を推奨
          - 進捗を定期的に報告
        priority: critical

      - task_id: task_019_3
        description: |
          ベンチマーク計算スクリプトを実行せよ

          【計算対象】
          1. SPY Buy & Hold（2010-2024）
          2. 60/40 Portfolio（SPY 60% / TLT 40%、月次リバランス）
          3. QQQ Buy & Hold
          4. AGG Buy & Hold

          【出力ファイル】
          - results/benchmark_comparison_v2.json

          【収集指標（各ベンチマーク）】
          - 最終資産額、累計リターン、年率リターン
          - Sharpe Ratio、Sortino Ratio
          - MDD、Calmar Ratio
          - 年別リターン
          - ボラティリティ
        priority: critical

      - task_id: task_019_4
        description: |
          包括的パフォーマンスレポートを作成せよ

          【レポート内容】

          1. エグゼクティブサマリー
             - 主要指標の一覧表（ポートフォリオ vs 各ベンチマーク）
             - 目標達成状況（Sharpe >= 1.0, SPY超過等）

          2. リターン分析
             - 年別リターン比較表（15年分）
             - 累積リターンチャート（データのみ、描画は別途）
             - SPY超過年数 / 総年数

          3. リスク分析
             - ドローダウン分析（最大DD、回復期間）
             - ボラティリティ比較
             - リスク調整後リターン（Sharpe, Sortino, Calmar）

          4. 期間別分析
             - 上昇相場（2013-2017, 2019-2021）でのパフォーマンス
             - 下落相場（2018, 2022）でのパフォーマンス
             - 高ボラ期間（2020 COVID）でのパフォーマンス

          5. 日次 vs 月次比較
             - リターン差、Sharpe差
             - 取引コスト影響
             - 推奨リバランス頻度

          【出力ファイル】
          - results/performance_report_v2.md
          - results/performance_report_v2.json
        priority: critical

      - task_id: task_019_5
        description: |
          パフォーマンス分析に基づく改善案を自律的に提示せよ

          【分析観点】

          1. リターン不足の場合
             - シグナル強度の調整
             - アグレッシブな戦略の追加
             - レバレッジの検討（ETFベース）

          2. リスク過大の場合
             - ボラティリティターゲットの引き下げ
             - 現金比率の引き上げ
             - ヘッジ戦略の強化

          3. Sharpe不足の場合
             - シグナルの精度向上
             - 取引コスト削減（リバランス頻度調整）
             - 低相関アセットの追加

          4. 特定期間の弱さ
             - レジーム検出の改善
             - 下落相場対応の強化
             - セクターローテーションの最適化

          【出力形式】
          ```yaml
          improvement_proposals:
            - id: proposal_001
              category: "return_enhancement"
              title: "モメンタムシグナル強度の引き上げ"
              expected_impact: "年率+2-3%"
              risk: "MDD増加の可能性"
              implementation:
                file: "config/default.yaml"
                changes: |
                  signals.momentum.weight: 0.3 → 0.4
              priority: high

            - id: proposal_002
              ...
          ```

          【出力ファイル】
          - results/improvement_proposals.yaml
        priority: high

      - task_id: task_019_6
        description: |
          改善案の中から最も効果的なものを選択し実装せよ

          【選択基準】
          1. 期待効果 / リスク比が最も高いもの
          2. 実装コストが低いもの（設定変更のみ等）
          3. バックテストで検証可能なもの

          【実装手順】
          1. 改善案を1つ選択
          2. 設定ファイルまたはコードを修正
          3. 短期バックテスト（2020-2024の5年）で効果確認
          4. 効果があれば採用、なければロールバック

          【出力ファイル】
          - results/improvement_implementation_log.yaml
        priority: high

      - task_id: task_019_7
        description: |
          改善後の15年バックテストを再実行せよ（月次）

          【目的】
          改善案実装後のパフォーマンスを検証

          【実行コマンド】
          task_019_1 と同様

          【出力ファイル】
          - results/backtest_15year_monthly_v3.json

          【比較項目】
          - v2（改善前）vs v3（改善後）の差分
          - 各指標の改善幅
        priority: high

      - task_id: task_019_8
        description: |
          最終レポートを作成せよ

          【レポート内容】

          1. 改善サイクル結果サマリー
             | 指標 | 改善前(v2) | 改善後(v3) | 変化 |
             |------|-----------|-----------|------|
             | Sharpe | x.xx | x.xx | +x.xx |
             | 年率リターン | x.xx% | x.xx% | +x.xx% |
             | MDD | -x.xx% | -x.xx% | +x.xx% |

          2. 実装した改善案
             - 内容、変更箇所、効果

          3. 未実装の改善案（次回検討）
             - 理由、期待効果

          4. 次のアクション推奨
             - さらなる改善の方向性
             - 本番運用に向けた準備事項

          【出力ファイル】
          - results/final_report_cmd019.md
          - dashboard.md への反映
        priority: high

    execution_strategy: |
      【実行順序】
      Phase 1（並列）: task_019_1（月次BT）+ task_019_2（日次BT）+ task_019_3（ベンチマーク）
      Phase 2: task_019_4（レポート作成）← Phase 1 完了待ち
      Phase 3: task_019_5（改善案提示）← task_019_4 完了待ち
      Phase 4: task_019_6（改善実装）← task_019_5 完了待ち
      Phase 5: task_019_7（再検証BT）← task_019_6 完了待ち
      Phase 6: task_019_8（最終レポート）← task_019_7 完了待ち

      【足軽割り当て】
      - 足軽1: task_019_1（月次BT）
      - 足軽2: task_019_2（日次BT）※バックグラウンド実行
      - 足軽3: task_019_3（ベンチマーク計算）
      - 足軽4: task_019_4（レポート作成）
      - 足軽5: task_019_5（改善案提示）
      - 足軽6: task_019_6（改善実装）
      - 足軽7: task_019_7（再検証BT）
      - 足軽8: task_019_8（最終レポート）

      【依存関係】
      cmd_018 完了後に開始すること

    expected_outcome: |
      【期待効果】

      検証完了後の目標:
      - Sharpe Ratio: >= 1.0
      - 年率リターン: >= 13.7%（SPY超過）
      - MDD: < 33.7%（SPY未満）
      - SPY超過年数: >= 11/15年（70%以上）

      改善サイクル後の目標:
      - Sharpe Ratio: 1.0 → 1.2+
      - 年率リターン: 13.7% → 15-20%
      - MDD: 維持または改善

      【成果物】
      - results/backtest_15year_monthly_v2.json
      - results/backtest_15year_daily_v2.json
      - results/benchmark_comparison_v2.json
      - results/performance_report_v2.md
      - results/improvement_proposals.yaml
      - results/improvement_implementation_log.yaml
      - results/backtest_15year_monthly_v3.json
      - results/final_report_cmd019.md

  - id: cmd_020
    timestamp: "2026-01-29T05:30:00"
    command: "全上場企業・通貨ペア包括バックテスト"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_019]
    context: |
      【背景】
      cmd_019までは限定的なユニバース（10銘柄）でテストしていた。
      本コマンドでは、米国・日本の全取引可能銘柄および通貨ペアを含む
      包括的なバックテストを実施する。

      【対象ユニバース】

      1. 米国株式
         - S&P 500 全構成銘柄（約500銘柄）
         - NASDAQ 100 構成銘柄（重複除く追加分）

      2. 日本株式
         - 日経225 全構成銘柄（225銘柄）

      3. ETF（全カテゴリ約60銘柄）
         - Equity, Bonds, Sector, Commodity, International, Alternative

      4. 通貨ペア（Forex）11ペア
         - Major: USDJPY, EURUSD, GBPUSD, AUDUSD, USDCHF, USDCAD, NZDUSD
         - Cross: EURJPY, GBPJPY, EURGBP, AUDJPY

      【推定銘柄数】合計: 約800-900銘柄

    tasks:
      - task_id: task_020_1
        description: |
          config/universe_full.yaml を新規作成し、全銘柄を定義せよ

          【実装内容】

          1. S&P 500 全銘柄リストの取得
             ```python
             import pandas as pd
             sp500 = pd.read_html(
                 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
             )[0]['Symbol'].tolist()
             ```

          2. 日経225 全銘柄リストの取得
             - Wikipedia または日経公式から取得
             - .T サフィックス付きで保存

          3. config/universe_full.yaml の構造
             ```yaml
             universe:
               us_stocks:
                 enabled: true
                 tickers: [AAPL, MSFT, ... 約500銘柄]

               japan_stocks:
                 enabled: true
                 suffix: ".T"
                 tickers: ["7203", "6758", ... 225銘柄]

               etfs:
                 enabled: true
                 # 全カテゴリ含む

               forex:
                 enabled: true
                 pairs: [USDJPY=X, EURUSD=X, ...]
             ```

          【出力ファイル】
          - config/universe_full.yaml
          - scripts/fetch_universe_lists.py
        priority: critical

      - task_id: task_020_2
        description: |
          大量銘柄データ取得のバッチフェッチャーを実装せよ

          【実装内容】
          src/data/batch_fetcher.py を新規作成

          ```python
          class BatchDataFetcher:
              def __init__(
                  self,
                  max_concurrent: int = 10,
                  rate_limit_per_sec: float = 2.0,
                  retry_count: int = 3,
                  cache_dir: str = "cache/price_data"
              ):
                  ...

              async def fetch_all(
                  self,
                  tickers: list[str],
                  start_date: str,
                  end_date: str
              ) -> dict[str, pl.DataFrame]:
                  """並列取得 + キャッシュ + リトライ"""
                  ...
          ```

          【機能】
          - 並列取得（同時接続数制限）
          - レート制限
          - 自動リトライ
          - Parquetキャッシュ
          - 進捗レポート
        priority: critical

      - task_id: task_020_3
        description: |
          メモリ効率的なストリーミングバックテストエンジンを実装せよ

          【実装内容】
          src/backtest/streaming_engine.py を新規作成

          ```python
          class StreamingBacktestEngine:
              def __init__(self, chunk_size: int = 100):
                  ...

              def run(
                  self,
                  universe_file: str,
                  start_date: str,
                  end_date: str,
                  rebalance_freq: str
              ) -> StreamingBacktestResult:
                  """
                  銘柄をチャンク分割して処理
                  中間結果をディスクに保存
                  """
                  ...
          ```

          【機能】
          - チャンク単位処理（メモリ節約）
          - 中間結果のディスク保存
          - 進捗表示
        priority: critical

      - task_id: task_020_4
        description: |
          全銘柄のデータ取得を実行せよ

          【実行手順】
          1. S&P 500 データ取得（2010-2024、約2時間）
          2. 日経225 データ取得（約30分）
          3. ETF データ取得（約10分）
          4. Forex データ取得（約5分）

          【出力】
          - cache/price_data/*.parquet
          - results/data_fetch_report.json
        priority: critical

      - task_id: task_020_5
        description: |
          全銘柄の品質チェックとフィルタリングを実行せよ

          【フィルタリング基準】
          - 最低2年のデータ
          - 平均出来高 >= 100,000
          - 欠損率 <= 5%
          - OHLC整合性 <= 0.5%

          【出力】
          - results/quality_filter_report.json
        priority: critical

      - task_id: task_020_6
        description: |
          全ポートフォリオで月次15年バックテストを実行せよ

          【実行コマンド】
          python -m src.main --backtest \
            --universe-file config/universe_full.yaml \
            --start 2010-01-01 --end 2024-12-31 \
            --rebalance monthly --capital 100000 \
            --streaming

          【推定時間】4-8時間

          【出力】
          - results/backtest_full_monthly.json
        priority: critical

      - task_id: task_020_7
        description: |
          全ポートフォリオで週次15年バックテストを実行せよ

          【推定時間】8-16時間（バックグラウンド実行）

          【出力】
          - results/backtest_full_weekly.json
        priority: high

      - task_id: task_020_8
        description: |
          ベンチマークとの包括的比較レポートを作成せよ

          【比較対象】
          - SPY, 60/40, QQQ, VTI, EWJ, Global 60/40

          【レポート内容】
          - エグゼクティブサマリー
          - アセットクラス別分析
          - 地域別分析（米国/日本/国際）
          - セクター分析
          - 期間別分析
          - リバランス頻度比較

          【出力】
          - results/full_portfolio_report.md
          - results/full_portfolio_report.json
        priority: critical

      - task_id: task_020_9
        description: |
          全ポートフォリオ結果に基づく改善提案を作成せよ

          【分析観点】
          - アセットクラス最適化
          - 地域配分最適化
          - セクター配分最適化
          - 銘柄選択最適化
          - リスク管理改善

          【出力】
          - results/full_portfolio_improvements.yaml
        priority: high

      - task_id: task_020_10
        description: |
          最終包括レポートを作成しダッシュボードを更新せよ

          【レポート内容】
          - cmd_001〜cmd_020 の全体サマリー
          - パフォーマンス推移（5銘柄→800銘柄）
          - 分散効果分析
          - 本番運用への推奨事項
          - 今後の課題

          【出力】
          - results/final_comprehensive_report.md
          - dashboard.md 更新
        priority: critical

    execution_strategy: |
      【実行順序】
      Phase 1（並列）: task_020_1 + task_020_2 + task_020_3
      Phase 2: task_020_4（データ取得）
      Phase 3: task_020_5（品質フィルタ）
      Phase 4（並列）: task_020_6 + task_020_7（バックテスト）
      Phase 5: task_020_8（比較レポート）
      Phase 6: task_020_9（改善提案）
      Phase 7: task_020_10（最終レポート）

      【依存関係】cmd_019 完了後に開始

      【推定処理時間】合計: 約15-35時間

    expected_outcome: |
      【期待効果】
      - Sharpe Ratio: 1.0-1.2 → 1.2-1.5
      - MDD: -20% → -15%以下
      - 地域分散・アセットクラス分散による安定性向上

      【対象銘柄数】
      - 米国株: 400-500銘柄（フィルタ後）
      - 日本株: 180-200銘柄
      - ETF: 50-55銘柄
      - Forex: 11ペア
      - 合計: 約650-770銘柄

  - id: cmd_021
    timestamp: "2026-01-29T06:00:00"
    command: "バックテスト高速化 - 緊急実装"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: []
    context: |
      【緊急】バックテスト処理時間短縮。即座に実装開始。

      【現状】10銘柄日次15年: 5-6時間 / 800銘柄月次15年: 推定8-16時間
      【目標】10銘柄日次15年: 5-10分 / 800銘柄月次15年: 30-60分

    tasks:
      - task_id: task_021_1
        description: |
          src/backtest/incremental_signal.py 新規作成【最重要】

          SignalState: dequeで価格バッファ保持、update()で1価格追加
          IncrementalSignalEngine: 全銘柄のシグナル状態管理
          対応: momentum, roc, zscore, rsi, bollinger

          効果: O(N×T) → O(N) で10-50倍高速化
        priority: critical

      - task_id: task_021_2
        description: |
          src/backtest/vectorized_compute.py 新規作成

          compute_all_momentum_vectorized(): 全銘柄×全期間一括計算
          compute_all_volatility_vectorized(): ボラティリティ一括
          compute_all_zscore_vectorized(): Zスコア一括
          compute_covariance_ewm(): 指数加重共分散

          効果: 5-20倍高速化
        priority: critical

      - task_id: task_021_3
        description: |
          src/backtest/signal_precompute.py 新規作成

          SignalPrecomputer: 全シグナルを事前計算しParquet保存
          precompute_all(): ベクトル化計算→保存
          load_signal(): キャッシュから読み込み
          get_signal_at_date(): 特定日のシグナル取得

          効果: バックテスト時5-10倍高速化
        priority: critical

      - task_id: task_021_4
        description: |
          src/backtest/covariance_cache.py 新規作成

          IncrementalCovarianceEstimator: 共分散のインクリメンタル更新
          update(): 指数加重で共分散行列を更新
          get_correlation(): 相関行列を取得

          効果: 3-5倍高速化
        priority: critical

      - task_id: task_021_5
        description: |
          src/backtest/fast_engine.py 新規作成

          FastBacktestEngine: 高速バックテストエンジン
          - 事前計算シグナル使用
          - インクリメンタル共分散
          - ベクトル化ポートフォリオ計算

          _run_fast_simulation(): 高速ループ実装
        priority: critical

      - task_id: task_021_6
        description: |
          src/backtest/engine.py 更新

          BacktestConfig追加: use_fast_mode, precompute_signals等
          run()でfast_mode判定、FastBacktestEngineへ切り替え
        priority: critical

      - task_id: task_021_7
        description: |
          main.py に--fast, --precompute, --no-cacheオプション追加
        priority: high

      - task_id: task_021_8
        description: |
          高速化効果検証: 10銘柄で従来vs高速比較
          成功基準: 10倍以上高速化、結果誤差0.1%未満
        priority: high

    execution_strategy: |
      【緊急並列実行】依存なし、即座に開始

      Phase 1（全並列）:
      足軽1→task_021_1, 足軽2→task_021_2
      足軽3→task_021_3, 足軽4→task_021_4

      Phase 2: 足軽5→task_021_5, 足軽6→task_021_6

      Phase 3: 足軽7→task_021_7, 足軽8→task_021_8

    expected_outcome: |
      処理時間: 5-6時間→5-10分（60倍）、8-16時間→30-60分（20倍）

  - id: cmd_022
    timestamp: "2026-01-29T06:30:00"
    command: "バックテスト超高速化 - アルゴリズム最適化とJIT/GPU"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_021]
    context: |
      cmd_021に加え、さらなる高速化を実現。
      目標: 800銘柄・月次・15年を3-5分で完了（100倍以上高速化）

    tasks:
      - task_id: task_022_1
        description: |
          ensemble.py RegimeDetector O(n²)→O(n)修正【最重要】
          for i in range(len) → rolling().rank(pct=True)
          効果: シグナル計算40-60%高速化
        priority: critical

      - task_id: task_022_2
        description: |
          src/backtest/numba_compute.py 新規作成
          @njit(parallel=True)でmomentum_batch, volatility_batch, covariance実装
          効果: 計算5-10倍高速化
        priority: critical

      - task_id: task_022_3
        description: |
          src/backtest/gpu_compute.py 新規作成（オプション）
          CuPyでcovariance_gpu, matrix_multiply_gpu実装
          GPU未搭載環境はCPUフォールバック
        priority: high

      - task_id: task_022_4
        description: |
          hrp.py クラスタリングキャッシュ追加
          相関行列ハッシュでlinkage結果を再利用
          効果: HRP計算60-80%高速化
        priority: critical

      - task_id: task_022_5
        description: |
          src/backtest/score_vectorized.py 新規作成
          ProcessPool廃止、NumPy一括計算
          効果: スコア計算50-70%高速化
        priority: critical

      - task_id: task_022_6
        description: |
          mean_reversion.py rollingキャッシュ追加
          _get_rolling_mean/stdでキャッシュ再利用
          効果: MeanReversion系30-50%高速化
        priority: high

      - task_id: task_022_7
        description: |
          fast_engine.py Numba/GPU統合
          use_gpu, use_numbaフラグで切り替え
        priority: critical

      - task_id: task_022_8
        description: |
          最終検証: 元実装vsCmd021vsCmd022
          成功基準: 累積50-100倍高速化
        priority: high

    execution_strategy: |
      cmd_021完了後に開始
      Phase1並列: task_022_1,2,3,4
      Phase2並列: task_022_5,6
      Phase3: task_022_7,8

    expected_outcome: |
      10銘柄日次15年: 5-6時間→30秒-1分（300倍）
      800銘柄月次15年: 8-16時間→3-5分（100倍）

  - id: cmd_023
    timestamp: "2026-01-29T08:30:00"
    command: "超高速バックテストエンジン Phase 2 - VectorBTアーキテクチャ統合"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_022]
    context: |
      【背景】
      cmd_021/022で累積11.2倍の高速化を達成。
      更なる高速化として、業界最速VectorBT（1000倍高速化実証済み）の
      アーキテクチャを参考に、追加最適化を実施する。

      【目標】
      - 現状（cmd_022完了）: 800銘柄月次15年 = 30-60分
      - Phase 1完了後: 3-6分（50-100倍）
      - Phase 2完了後: 30-90秒（200-500倍）
      - 最終目標: 10-30秒（500-2000倍）

      【参考資料】
      - VectorBT: 100万注文を70-100msで処理
      - Polars: Pandasの5-10倍高速
      - DuckDB: 列型圧縮で5-20倍高速
      - Ray: GIL回避で5-50倍並列化
      - JAX on GPU: 共分散計算10-30倍高速

    tasks:
      - task_id: task_023_1
        description: |
          Numba JIT全面適用【最重要・即効性高】

          【対象ファイル】
          1. src/backtest/score_vectorized.py
             - _normalize_scores() のループをNumba化
          2. src/backtest/simulator.py
             - rebalance() のtrade計算ループ
          3. src/backtest/result.py
             - _calculate_max_drawdown() をNumba化
          4. src/meta/evaluator.py
             - 評価ループのNumba化

          【実装パターン】
          ```python
          from numba import njit, prange

          @njit(parallel=True, cache=True, fastmath=True)
          def calculate_max_drawdown_numba(portfolio_values: np.ndarray) -> float:
              n = len(portfolio_values)
              running_max = portfolio_values[0]
              max_dd = 0.0
              for i in prange(1, n):
                  if portfolio_values[i] > running_max:
                      running_max = portfolio_values[i]
                  dd = (portfolio_values[i] - running_max) / running_max
                  if dd < max_dd:
                      max_dd = dd
              return max_dd
          ```

          【効果】8-12倍高速化（該当処理）
        priority: critical

      - task_id: task_023_2
        description: |
          Polars全面移行【Pandas完全置換】

          【対象ファイル】
          1. src/backtest/engine.py - データ読み込み・処理
          2. src/backtest/fast_engine.py - 価格データ処理
          3. src/data/fetcher.py - データ取得後の処理
          4. src/signals/*.py - シグナル計算

          【実装方針】
          ```python
          import polars as pl

          # Pandas → Polars 変換
          def process_prices_polars(prices: pl.DataFrame) -> pl.DataFrame:
              return prices.lazy().with_columns([
                  pl.col('close').pct_change().alias('returns'),
                  pl.col('close').rolling_mean(window_size=20).alias('sma20'),
                  pl.col('close').rolling_std(window_size=20).alias('std20'),
              ]).collect()

          # 遅延評価で最適化
          result = (
              prices.lazy()
              .filter(pl.col('date') >= start_date)
              .group_by('symbol')
              .agg([
                  pl.col('returns').mean().alias('mean_return'),
                  pl.col('returns').std().alias('volatility'),
              ])
              .collect()
          )
          ```

          【注意】
          - 既存APIとの互換性維持（Pandas出力オプション）
          - 段階的移行（まずengine.pyから）

          【効果】5-10倍高速化、メモリ30-50%削減
        priority: critical

      - task_id: task_023_3
        description: |
          DuckDB統合データレイヤー

          【新規ファイル】src/data/unified_data_layer.py

          【実装】
          ```python
          import duckdb
          import polars as pl
          from pathlib import Path

          class UnifiedDataLayer:
              """
              DuckDB + Polars統合データレイヤー
              - インメモリ列型ストレージ
              - SQL + DataFrame API両対応
              - 自動キャッシュ管理
              """
              def __init__(self, cache_dir: str = '.cache/duckdb'):
                  self.con = duckdb.connect(':memory:')
                  self.cache_dir = Path(cache_dir)
                  self.cache_dir.mkdir(parents=True, exist_ok=True)
                  self._init_schema()

              def _init_schema(self):
                  self.con.execute('''
                      CREATE TABLE IF NOT EXISTS prices (
                          symbol VARCHAR,
                          date DATE,
                          open DOUBLE,
                          high DOUBLE,
                          low DOUBLE,
                          close DOUBLE,
                          adj_close DOUBLE,
                          volume BIGINT
                      )
                  ''')
                  self.con.execute('CREATE INDEX IF NOT EXISTS idx_symbol_date ON prices(symbol, date)')

              def load_universe(self, universe: list[str], start: str, end: str) -> pl.DataFrame:
                  """効率的にユニバース全体をロード"""
                  # Parquet行グループフィルタリング
                  for symbol in universe:
                      parquet_path = self.cache_dir / f'{symbol}.parquet'
                      if parquet_path.exists():
                          self.con.execute(f'''
                              INSERT INTO prices
                              SELECT * FROM read_parquet('{parquet_path}')
                              WHERE date BETWEEN '{start}' AND '{end}'
                          ''')

                  return self.con.execute('''
                      SELECT * FROM prices ORDER BY symbol, date
                  ''').pl()

              def get_returns_matrix(self, start: str, end: str) -> pl.DataFrame:
                  """リターン行列を効率的に抽出"""
                  return self.con.execute(f'''
                      SELECT date, symbol,
                             (close / LAG(close) OVER (PARTITION BY symbol ORDER BY date) - 1) as return
                      FROM prices
                      WHERE date BETWEEN '{start}' AND '{end}'
                      ORDER BY date, symbol
                  ''').pl().pivot(index='date', columns='symbol', values='return')
          ```

          【効果】5-20倍高速化（複雑なクエリで特に効果大）
        priority: critical

      - task_id: task_023_4
        description: |
          Ray分散バックテスト

          【新規ファイル】src/backtest/distributed_engine.py

          【実装】
          ```python
          import ray
          from typing import List, Dict, Any
          import numpy as np

          @ray.remote
          class BacktestWorker:
              """Ray Actor: 個別資産のバックテスト実行"""
              def __init__(self, config: dict):
                  self.config = config
                  self._init_engine()

              def _init_engine(self):
                  from .fast_engine import FastBacktestEngine
                  self.engine = FastBacktestEngine(self.config)

              def run_subset(self, symbols: List[str], prices: np.ndarray,
                            start: str, end: str) -> Dict[str, Any]:
                  """サブセットのバックテスト実行"""
                  return self.engine.run(prices, start, end, symbols=symbols)

          class DistributedBacktestEngine:
              """Ray分散バックテストエンジン"""
              def __init__(self, config: dict, n_workers: int = None):
                  ray.init(ignore_reinit_error=True)
                  self.n_workers = n_workers or ray.available_resources().get('CPU', 4)
                  self.workers = [BacktestWorker.remote(config) for _ in range(int(self.n_workers))]

              def run(self, universe: List[str], prices: np.ndarray,
                     start: str, end: str) -> Dict[str, Any]:
                  """全資産を並列バックテスト"""
                  # ユニバースを分割
                  chunk_size = len(universe) // len(self.workers) + 1
                  chunks = [universe[i:i+chunk_size] for i in range(0, len(universe), chunk_size)]

                  # 並列実行
                  futures = [
                      worker.run_subset.remote(chunk, prices, start, end)
                      for worker, chunk in zip(self.workers, chunks)
                  ]

                  # 結果集約
                  results = ray.get(futures)
                  return self._merge_results(results)

              def _merge_results(self, results: List[Dict]) -> Dict[str, Any]:
                  """分散結果をマージ"""
                  merged = {'portfolio_values': [], 'metrics': {}}
                  for r in results:
                      merged['portfolio_values'].extend(r.get('portfolio_values', []))
                  return merged
          ```

          【メモリ管理】
          - 各ワーカー: max 1GB（32GB RAM環境で最大32並列）
          - ray.init(object_store_memory=10**10) で10GB共有メモリ

          【効果】5-50倍高速化（コア数依存、16コアで8-16倍）
        priority: critical

      - task_id: task_023_5
        description: |
          ストリーミング共分散（Welfordアルゴリズム）

          【更新ファイル】src/backtest/covariance_cache.py

          【実装】
          ```python
          import numpy as np
          from numba import njit

          @njit(cache=True)
          def update_streaming_covariance(
              mean: np.ndarray,
              M2: np.ndarray,
              n: int,
              new_data: np.ndarray
          ) -> tuple:
              """
              Welfordのオンラインアルゴリズムで共分散を逐次更新
              O(T×n²) → O(n²) per update
              """
              n_new = n + 1
              delta = new_data - mean
              mean_new = mean + delta / n_new
              delta2 = new_data - mean_new
              M2_new = M2 + np.outer(delta, delta2)
              return mean_new, M2_new, n_new

          class StreamingCovarianceEstimator:
              """
              オンライン共分散推定器
              - メモリ効率: O(n²) 固定
              - 計算効率: 新データ追加 O(n²)
              """
              def __init__(self, n_assets: int):
                  self.n_assets = n_assets
                  self.mean = np.zeros(n_assets)
                  self.M2 = np.zeros((n_assets, n_assets))
                  self.n = 0

              def update(self, returns: np.ndarray):
                  """新しいリターンデータで更新"""
                  self.mean, self.M2, self.n = update_streaming_covariance(
                      self.mean, self.M2, self.n, returns
                  )

              def update_batch(self, returns_batch: np.ndarray):
                  """バッチ更新"""
                  for returns in returns_batch:
                      self.update(returns)

              @property
              def covariance(self) -> np.ndarray:
                  if self.n < 2:
                      return np.zeros_like(self.M2)
                  return self.M2 / (self.n - 1)

              @property
              def correlation(self) -> np.ndarray:
                  cov = self.covariance
                  std = np.sqrt(np.diag(cov))
                  std[std == 0] = 1  # ゼロ除算防止
                  return cov / np.outer(std, std)
          ```

          【fast_engine.pyへの統合】
          ```python
          def _update_covariance_streaming(self, new_returns):
              if not hasattr(self, '_streaming_cov'):
                  self._streaming_cov = StreamingCovarianceEstimator(self.n_assets)
              self._streaming_cov.update(new_returns)
              return self._streaming_cov.covariance
          ```

          【効果】3-5倍高速化（特にWalk-Forward検証で効果大）
        priority: high

      - task_id: task_023_6
        description: |
          メモリ最適化（アライメント+プール）

          【更新ファイル】src/backtest/memory_optimizer.py（新規）

          【実装】
          ```python
          import numpy as np
          from typing import Dict, Tuple

          class MemoryPool:
              """
              NumPy配列の事前確保プール
              - アロケーションオーバーヘッド削減
              - キャッシュライン最適化（64バイト境界）
              """
              def __init__(self, max_assets: int = 1000, max_days: int = 5000):
                  self.max_assets = max_assets
                  self.max_days = max_days
                  self._pools: Dict[Tuple[int, ...], np.ndarray] = {}

              def get_aligned_array(self, shape: Tuple[int, ...], dtype=np.float64) -> np.ndarray:
                  """64バイトアライメント済み配列を取得"""
                  key = (shape, dtype)
                  if key not in self._pools:
                      # 64バイト境界にアライン
                      size = int(np.prod(shape))
                      buffer = np.empty(size + 8, dtype=dtype)
                      offset = (64 - buffer.ctypes.data % 64) // buffer.itemsize
                      aligned = buffer[offset:offset+size].reshape(shape)
                      self._pools[key] = np.ascontiguousarray(aligned)
                  return self._pools[key].copy()

              def get_returns_matrix(self, n_assets: int, n_days: int) -> np.ndarray:
                  """リターン行列用の最適化配列"""
                  return self.get_aligned_array((n_days, n_assets))

              def get_covariance_matrix(self, n_assets: int) -> np.ndarray:
                  """共分散行列用の最適化配列"""
                  return self.get_aligned_array((n_assets, n_assets))

          # グローバルプール（シングルトン）
          _memory_pool = None

          def get_memory_pool() -> MemoryPool:
              global _memory_pool
              if _memory_pool is None:
                  _memory_pool = MemoryPool()
              return _memory_pool

          # 使用例
          def optimized_covariance(returns: np.ndarray) -> np.ndarray:
              pool = get_memory_pool()
              n_assets = returns.shape[1]
              cov = pool.get_covariance_matrix(n_assets)
              # ... 計算
              return cov
          ```

          【fast_engine.pyへの統合】
          - 配列確保をMemoryPoolから取得
          - np.ascontiguousarray()で連続メモリ保証

          【効果】1.5-2倍高速化（キャッシュヒット率向上）
        priority: high

      - task_id: task_023_7
        description: |
          VectorBTアーキテクチャ統合【最終最適化】

          【新規ファイル】src/backtest/vectorbt_engine.py

          【VectorBTの核心技術】
          1. 完全ベクトル化: forループ完全排除
          2. Numba JIT: 全計算パスをコンパイル
          3. チャンキング: メモリ効率的な分割処理
          4. マルチスレッド: GIL解放関数の並列実行

          【実装】
          ```python
          import numpy as np
          from numba import njit, prange
          from typing import Tuple

          @njit(parallel=True, cache=True, fastmath=True)
          def vectorized_backtest_core(
              prices: np.ndarray,        # (n_days, n_assets)
              signals: np.ndarray,       # (n_days, n_assets)
              weights: np.ndarray,       # (n_days, n_assets)
              initial_capital: float,
              transaction_cost: float,
              rebalance_mask: np.ndarray # (n_days,) bool
          ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
              """
              完全ベクトル化バックテストコア
              100万注文を70-100msで処理（VectorBT実証済み）
              """
              n_days, n_assets = prices.shape
              portfolio_values = np.zeros(n_days)
              positions = np.zeros(n_assets)
              cash = initial_capital

              # 初期ポジション
              portfolio_values[0] = initial_capital

              for t in prange(1, n_days):
                  # 価格変動による評価額更新
                  price_returns = prices[t] / prices[t-1] - 1
                  position_values = positions * prices[t-1] * (1 + price_returns)

                  # リバランス判定
                  if rebalance_mask[t]:
                      total_value = cash + np.sum(position_values)
                      target_positions = weights[t] * total_value / prices[t]

                      # 取引コスト計算
                      trades = target_positions - positions
                      cost = np.sum(np.abs(trades) * prices[t]) * transaction_cost

                      # ポジション更新
                      cash = total_value - np.sum(target_positions * prices[t]) - cost
                      positions = target_positions
                  else:
                      # ポジション維持
                      cash = cash  # 変更なし

                  # ポートフォリオ価値計算
                  portfolio_values[t] = cash + np.sum(positions * prices[t])

              # メトリクス計算
              returns = np.diff(portfolio_values) / portfolio_values[:-1]
              sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)

              return portfolio_values, returns, sharpe

          class VectorBTStyleEngine:
              """VectorBTスタイルの超高速バックテストエンジン"""
              def __init__(self, config: dict):
                  self.config = config
                  self.chunk_size = config.get('chunk_size', 10000)

              def run(self, prices: np.ndarray, signals: np.ndarray,
                     start_idx: int = 0, end_idx: int = None) -> dict:
                  """チャンク分割 + 並列実行"""
                  if end_idx is None:
                      end_idx = len(prices)

                  # 重みを事前計算（シグナルから）
                  weights = self._compute_weights_vectorized(signals)

                  # リバランスマスク生成
                  rebalance_mask = self._generate_rebalance_mask(len(prices))

                  # コア実行
                  pv, returns, sharpe = vectorized_backtest_core(
                      prices[start_idx:end_idx],
                      signals[start_idx:end_idx],
                      weights[start_idx:end_idx],
                      self.config.get('initial_capital', 100000),
                      self.config.get('transaction_cost', 0.001),
                      rebalance_mask[start_idx:end_idx]
                  )

                  return {
                      'portfolio_values': pv,
                      'returns': returns,
                      'sharpe': sharpe,
                      'max_drawdown': self._calculate_max_drawdown_fast(pv)
                  }

              @staticmethod
              @njit(cache=True)
              def _calculate_max_drawdown_fast(portfolio_values: np.ndarray) -> float:
                  running_max = portfolio_values[0]
                  max_dd = 0.0
                  for i in range(1, len(portfolio_values)):
                      if portfolio_values[i] > running_max:
                          running_max = portfolio_values[i]
                      dd = (portfolio_values[i] - running_max) / running_max
                      if dd < max_dd:
                          max_dd = dd
                  return max_dd
          ```

          【既存エンジンとの統合】
          - fast_engine.py に use_vectorbt_mode フラグ追加
          - CLI: --vectorbt オプション

          【効果】10-100倍高速化（VectorBT実証：1000倍可能）
        priority: critical

      - task_id: task_023_8
        description: |
          統合検証・ベンチマーク

          【検証スクリプト】scripts/benchmark_engines.py

          【テスト条件】
          ```python
          # テストケース
          test_cases = [
              {'name': 'small', 'n_assets': 10, 'n_days': 252, 'desc': '10銘柄1年'},
              {'name': 'medium', 'n_assets': 100, 'n_days': 2520, 'desc': '100銘柄10年'},
              {'name': 'large', 'n_assets': 500, 'n_days': 3780, 'desc': '500銘柄15年'},
              {'name': 'xlarge', 'n_assets': 800, 'n_days': 180, 'desc': '800銘柄月次15年'},
          ]

          # エンジン比較
          engines = [
              ('original', OriginalBacktestEngine),
              ('cmd021', FastBacktestEngine),  # cmd_021
              ('cmd022', FastBacktestEngine),  # cmd_022 (numba/gpu)
              ('cmd023', VectorBTStyleEngine), # cmd_023
              ('distributed', DistributedBacktestEngine),  # Ray分散
          ]
          ```

          【出力】
          - results/benchmark_cmd023.json
          - results/benchmark_cmd023.md（マークダウンレポート）

          【成功基準】
          | ケース | 現状(cmd022) | 目標(cmd023) | 改善倍率 |
          |--------|-------------|--------------|---------|
          | small  | 0.25秒 | 0.01秒 | 25倍 |
          | medium | 5秒 | 0.2秒 | 25倍 |
          | large  | 60秒 | 2秒 | 30倍 |
          | xlarge | 30分 | 30秒 | 60倍 |

          【検証項目】
          1. 処理時間比較
          2. メモリ使用量比較
          3. 結果精度検証（Sharpe誤差 < 0.1%）
          4. CPU/GPUリソース使用率

        priority: high

    execution_strategy: |
      【Phase 1: 即時実装】並列実行
      - task_023_1 (Numba JIT) → 足軽1
      - task_023_2 (Polars移行) → 足軽2
      - task_023_6 (メモリ最適化) → 足軽3

      【Phase 2: 基盤構築】task_023_1,2完了後
      - task_023_3 (DuckDB統合) → 足軽4
      - task_023_5 (ストリーミング共分散) → 足軽5

      【Phase 3: 分散化】task_023_3完了後
      - task_023_4 (Ray分散) → 足軽6

      【Phase 4: 最終統合】全タスク完了後
      - task_023_7 (VectorBTアーキテクチャ) → 足軽7
      - task_023_8 (統合検証) → 足軽8

    expected_outcome: |
      【累積高速化効果】
      - cmd_022完了時: 11.2倍
      - cmd_023 Phase1完了: 50-100倍（5-10倍追加）
      - cmd_023 Phase2完了: 200-500倍（4-5倍追加）
      - cmd_023 Phase3完了: 500-2000倍（2-4倍追加）
      - cmd_023最終: 1000-5000倍

      【実測目標】
      - 800銘柄月次15年: 30分 → 30秒以下
      - 10銘柄日次15年: 1分 → 1秒以下
      - パラメータスイープ100パターン: 50時間 → 30分

      【品質保証】
      - Sharpe Ratio誤差: 0.1%未満
      - リターン誤差: 0.1%未満
      - メモリ使用量: 50%削減

  - id: cmd_024
    timestamp: "2026-01-29T09:00:00"
    command: "スキル設計書作成 - 採用12スキルの仕様策定"
    project: multi-agent-shogun
    priority: high
    status: pending
    context: |
      【背景】
      殿の命により、29件のスキル化候補を精査。
      汎用性・再利用性・独立性の観点から12件を採用決定。

      【採用スキル一覧】
      Tier 1（即時）:
        - walk-forward-backtest: Walk-Forward方式バックテスト
        - parallel-backtest-engine: 並列高速バックテスト
        - bayesian-hyperopt: ベイズ最適化フレームワーク

      Tier 2（次期）:
        - multi-source-data-fetcher: 並列データ取得
        - portfolio-simulator: ポートフォリオシミュレーター
        - kelly-position-sizing: Kelly公式ポジションサイジング

      Tier 3（後期）:
        - parameter-grid-search: パラメータグリッドサーチ
        - performance-analysis-framework: パフォーマンス分析
        - sector-signal-toolkit: セクターシグナル生成
        - market-calendar-toolkit: 取引カレンダー管理

      Tier 4（統合）:
        - dynamic-params-suite: 動的パラメータ統合（#20,21,23,24,27統合）
        - hierarchical-ensemble-suite: 階層的アンサンブル統合（#18,19統合）

      【却下スキル】11件
      #1,4,5,6,9,12,13,25,26,28,29 - プロジェクト固有または標準機能で代替可能

      【設計書フォーマット】
      各スキルについて以下を定義:
      1. スキル名・説明
      2. 入力パラメータ
      3. 出力形式
      4. 依存関係
      5. 使用例
      6. 実装ファイルパス

    tasks:
      - task_id: task_024_1
        description: |
          Tier 1 スキル設計書作成（3件）

          【対象スキル】
          1. walk-forward-backtest
          2. parallel-backtest-engine
          3. bayesian-hyperopt

          【出力先】skills/designs/tier1/

          【walk-forward-backtest 設計】
          ```yaml
          name: walk-forward-backtest
          description: |
            Walk-Forward方式のバックテストを実行。
            未来データ漏洩を防止し、真のアウトオブサンプル性能を評価。

          parameters:
            - name: train_period
              type: int
              default: 252
              description: 学習期間（日数）
            - name: test_period
              type: int
              default: 63
              description: テスト期間（日数）
            - name: step_size
              type: int
              default: 21
              description: ステップサイズ（日数）
            - name: embargo_days
              type: int
              default: 5
              description: データリーク防止期間

          outputs:
            - name: results
              type: List[BacktestResult]
              description: 各フォールドの結果
            - name: aggregate_metrics
              type: dict
              description: 集計メトリクス（Sharpe, MDD等）

          usage: |
            from skills import walk_forward_backtest
            results = walk_forward_backtest(
                strategy=my_strategy,
                data=price_data,
                train_period=504,
                test_period=126
            )
          ```

          【parallel-backtest-engine 設計】
          - マルチプロセス/スレッド並列実行
          - キャッシュ機構統合
          - 進捗コールバック対応

          【bayesian-hyperopt 設計】
          - Optuna/skopt統合
          - 時系列CV対応
          - 早期停止・過学習ペナルティ

        priority: critical

      - task_id: task_024_2
        description: |
          Tier 2 スキル設計書作成（3件）

          【対象スキル】
          1. multi-source-data-fetcher
          2. portfolio-simulator
          3. kelly-position-sizing

          【出力先】skills/designs/tier2/

          【multi-source-data-fetcher 設計】
          ```yaml
          name: multi-source-data-fetcher
          description: |
            複数データソースから並列でデータ取得。
            リトライ、レート制限、キャッシュ対応。

          parameters:
            - name: sources
              type: List[DataSource]
              description: データソース一覧
            - name: symbols
              type: List[str]
              description: 取得対象シンボル
            - name: start_date
              type: str
              description: 開始日
            - name: end_date
              type: str
              description: 終了日
            - name: parallel_workers
              type: int
              default: 4
              description: 並列ワーカー数
            - name: retry_count
              type: int
              default: 3
              description: リトライ回数
            - name: rate_limit
              type: float
              default: 1.0
              description: 秒間リクエスト数

          outputs:
            - name: data
              type: Dict[str, DataFrame]
              description: シンボル別データ
            - name: errors
              type: List[FetchError]
              description: 取得エラー一覧
          ```

          【portfolio-simulator 設計】
          - 取引コスト（スリッページ、手数料）
          - ターンオーバー制約
          - マージン/レバレッジ対応

          【kelly-position-sizing 設計】
          - フルKelly / Fractional Kelly
          - 最大ポジション制約
          - ボラティリティスケーリング

        priority: high

      - task_id: task_024_3
        description: |
          Tier 3 スキル設計書作成（4件）

          【対象スキル】
          1. parameter-grid-search
          2. performance-analysis-framework
          3. sector-signal-toolkit
          4. market-calendar-toolkit

          【出力先】skills/designs/tier3/

          【parameter-grid-search 設計】
          - グリッド/ランダム/ベイズ切り替え
          - 並列実行対応
          - 結果可視化

          【performance-analysis-framework 設計】
          - リターン帰属分析
          - ドローダウン分析
          - レジーム別パフォーマンス
          - ファクターエクスポージャー

          【sector-signal-toolkit 設計】
          - セクターモメンタム
          - 相対強度
          - ブレッドス（騰落銘柄比率）
          - セクターローテーション

          【market-calendar-toolkit 設計】
          - 複数市場対応（US/JP/EU/Asia）
          - 祝日管理
          - 取引時間管理
          - タイムゾーン変換

        priority: high

      - task_id: task_024_4
        description: |
          Tier 4 統合スキル設計書作成（2件）

          【対象スキル】
          1. dynamic-params-suite（5スキル統合）
          2. hierarchical-ensemble-suite（2スキル統合）

          【出力先】skills/designs/tier4/

          【dynamic-params-suite 設計】
          統合元: #20, 21, 23, 24, 27
          ```yaml
          name: dynamic-params-suite
          description: |
            市場レジームに応じた動的パラメータ計算スイート。
            シグナル/共分散/アロケーション設定を自動調整。

          modules:
            - signal_params: RSI/Bollinger/ZScore動的調整
            - covariance_params: EWMA半減期/相関調整
            - allocation_params: w_asset_max/delta_max調整
            - regime_adaptive: 5レジーム×パラメータマッピング
            - integration: use_dynamic切替+フォールバック

          parameters:
            - name: regime_detector
              type: RegimeDetector
              description: レジーム検出器
            - name: lookback_window
              type: int
              default: 60
              description: パラメータ計算ルックバック
            - name: use_dynamic
              type: bool
              default: true
              description: 動的計算の有効化

          outputs:
            - name: signal_config
              type: SignalConfig
            - name: cov_config
              type: CovarianceConfig
            - name: alloc_config
              type: AllocationConfig
          ```

          【hierarchical-ensemble-suite 設計】
          統合元: #18, 19
          - 3レイヤー階層（Trend/Reversion/Macro）
          - スタッキングメタモデル
          - レジーム適応重み
          - ベイズ最適化統合

        priority: high

      - task_id: task_024_5
        description: |
          スキルディレクトリ構造作成

          【作成構造】
          ```
          skills/
          ├── designs/
          │   ├── tier1/
          │   │   ├── walk-forward-backtest.yaml
          │   │   ├── parallel-backtest-engine.yaml
          │   │   └── bayesian-hyperopt.yaml
          │   ├── tier2/
          │   │   ├── multi-source-data-fetcher.yaml
          │   │   ├── portfolio-simulator.yaml
          │   │   └── kelly-position-sizing.yaml
          │   ├── tier3/
          │   │   ├── parameter-grid-search.yaml
          │   │   ├── performance-analysis-framework.yaml
          │   │   ├── sector-signal-toolkit.yaml
          │   │   └── market-calendar-toolkit.yaml
          │   └── tier4/
          │       ├── dynamic-params-suite.yaml
          │       └── hierarchical-ensemble-suite.yaml
          ├── implementations/
          │   └── (実装後に配置)
          └── README.md
          ```

          【README.md 内容】
          - スキル一覧と概要
          - 使用方法
          - 開発ガイドライン

        priority: high

      - task_id: task_024_6
        description: |
          dashboard.md スキルセクション更新

          【更新内容】
          「スキル化候補」セクションを以下に変更:

          ```markdown
          ### スキル化 最終決定【12件採用】

          #### ✅ 採用スキル
          | Tier | スキル名 | 状態 | 設計書 |
          |------|----------|------|--------|
          | 1 | walk-forward-backtest | 設計中 | designs/tier1/ |
          | 1 | parallel-backtest-engine | 設計中 | designs/tier1/ |
          | 1 | bayesian-hyperopt | 設計中 | designs/tier1/ |
          | 2 | multi-source-data-fetcher | 設計中 | designs/tier2/ |
          | 2 | portfolio-simulator | 設計中 | designs/tier2/ |
          | 2 | kelly-position-sizing | 設計中 | designs/tier2/ |
          | 3 | parameter-grid-search | 設計中 | designs/tier3/ |
          | 3 | performance-analysis-framework | 設計中 | designs/tier3/ |
          | 3 | sector-signal-toolkit | 設計中 | designs/tier3/ |
          | 3 | market-calendar-toolkit | 設計中 | designs/tier3/ |
          | 4 | dynamic-params-suite | 設計中 | designs/tier4/ |
          | 4 | hierarchical-ensemble-suite | 設計中 | designs/tier4/ |

          #### ❌ 却下スキル（11件）
          #1,4,5,6,9,12,13,25,26,28,29 - プロジェクト固有または標準機能で代替可能
          ```

        priority: high

    execution_strategy: |
      【並列実行】
      - task_024_1 (Tier1設計) → 足軽1
      - task_024_2 (Tier2設計) → 足軽2
      - task_024_3 (Tier3設計) → 足軽3
      - task_024_4 (Tier4設計) → 足軽4
      - task_024_5 (ディレクトリ作成) → 足軽5
      - task_024_6 (dashboard更新) → 足軽6

      全タスク並列実行可能

    expected_outcome: |
      - 12件のスキル設計書完成
      - skills/ディレクトリ構造整備
      - dashboard.md更新完了
      - 次フェーズ（実装）への準備完了

  - id: cmd_025
    timestamp: "2026-01-29T10:00:00"
    command: "バックテストエンジン機能統合 - 本番パイプラインとの同期"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_020]
    context: |
      【根本原因分析】
      cmd_020の月次BT(Sharpe 0.913)と週次BT(Sharpe 0.261)の差異を調査した結果、
      以下の根本的問題が判明した：

      1. 月次BTは102銘柄、週次BTは828銘柄で実行されていた
         → 月次スクリプトのカラム検出バグにより726銘柄が除外
         → 「銘柄数削減で改善」という誤った結論に至りかけた

      2. バックテストエンジン(fast_engine.py)とパイプライン(pipeline.py)の機能乖離
         → 23の機能がバックテストで未使用
         → 本番で使用される最適化ロジックがバックテストに反映されない
         → バックテスト結果が本番パフォーマンスを正確に予測できない

      【真の問題】
      バックテストエンジンに以下の重要機能が欠落：
      - TransactionCostOptimizer: 期待リターン vs 取引コストの最適化
      - DrawdownProtection: MDD制御
      - RegimeDetector: 市場環境認識
      - VIX Cash Allocation: パニック時防衛
      - 配当の扱い: トータルリターン計算

      【目標】
      バックテストエンジンに本番パイプラインの機能を統合し、
      正確なパフォーマンス予測を可能にする。

      【教訓と再発防止】
      1. 表面的な数値比較ではなく、根本原因を調査する
      2. バックテストと本番パイプラインの機能差異を定期的に監査
      3. 「なぜ」を5回繰り返して真因を特定する

    tasks:
      - task_id: task_025_1
        description: |
          TransactionCostOptimizer統合【根本対策1】

          【問題】
          バックテストエンジン(fast_engine.py)に取引コスト最適化ロジックがない。
          pipeline.pyでは使用されているが、バックテストでは未使用。
          結果: バックテスト結果が本番と乖離する。

          【既存コード参照】
          src/allocation/transaction_cost_optimizer.py - 完成済み
          - optimize(): max (期待リターン - リスクペナルティ - コストペナルティ)
          - 式: U = μ'w - (λ/2)*w'Σw - κ*TC(w, w_current)
          - 期待リターンvs取引コストの最適トレードオフを計算

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             - TransactionCostOptimizerをインポート
             - リバランス時に期待リターン計算
             - 取引コストとの比較でポジション変更判断
             ```python
             from src.allocation.transaction_cost_optimizer import TransactionCostOptimizer

             def _execute_rebalance(self, date, target_weights, current_weights):
                 optimizer = TransactionCostOptimizer(
                     expected_returns=self._calc_expected_returns(date),
                     covariance=self._calc_covariance(date),
                     transaction_cost_rate=0.001  # 10bps
                 )
                 optimized = optimizer.optimize(current_weights, target_weights)
                 return optimized
             ```

          2. src/backtest/engine.py も同様に修正

          3. 検証: 取引コスト考慮前後のパフォーマンス比較

          【期待効果】
          - バックテストと本番の一致度向上
          - 無駄なリバランス削減によるコスト低減
        priority: critical

      - task_id: task_025_2
        description: |
          DrawdownProtection統合【根本対策2】

          【問題】
          pipeline.pyにはDrawdownProtectionがあるが、バックテストでは未使用。
          MDD -31.49%は制御されていない。

          【既存コード参照】
          src/risk/drawdown_protection.py - 完成済み
          - check_and_protect(): 現在のドローダウンを監視
          - get_reduction_factor(): ポジション削減係数を計算
          - pipeline.py L2890-2920で使用

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             ```python
             from src.risk.drawdown_protection import DrawdownProtection

             def _apply_risk_controls(self, date, weights, portfolio_value):
                 dd_protector = DrawdownProtection(config=self.risk_config)
                 current_dd = self._calculate_drawdown(portfolio_value)

                 reduction = dd_protector.get_reduction_factor(current_dd)
                 if reduction < 1.0:
                     weights = weights * reduction
                     cash_weight = 1.0 - weights.sum()
                     weights['CASH'] = cash_weight

                 return weights
             ```

          2. 各リバランス時点でドローダウンチェック追加

          3. 検証: MDD制御効果の測定

          【期待効果】
          - MDD -31%→ -25%以下（目標達成）
          - バックテストでのリスク制御の正確な反映
        priority: critical

      - task_id: task_025_3
        description: |
          RegimeDetector統合【根本対策3】

          【問題】
          pipeline.pyにはRegimeDetector（市場環境認識）があるが、バックテストでは未使用。
          2020年のV字回復を捉えられなかった原因。

          【既存コード参照】
          src/signals/regime_detector.py - 完成済み
          - detect_regime(): bull/bear/neutral/crisis判定
          - get_regime_adjustments(): レジーム別配分調整
          - pipeline.py L1850-1900で使用

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             ```python
             from src.signals.regime_detector import RegimeDetector

             def _get_regime_adjusted_weights(self, date, base_weights):
                 regime = self.regime_detector.detect_regime(
                     self.price_data, date
                 )
                 adjustments = self.regime_detector.get_regime_adjustments(regime)

                 adjusted = base_weights.copy()
                 for asset, factor in adjustments.items():
                     if asset in adjusted:
                         adjusted[asset] *= factor

                 return self._normalize_weights(adjusted)
             ```

          2. レジーム変化時のリバランストリガー追加

          3. 検証: レジーム認識の効果測定

          【期待効果】
          - 急回復局面での適応力向上
          - 市場環境に応じた適切な配分
        priority: critical

      - task_id: task_025_4
        description: |
          VIX動的キャッシュ配分統合【根本対策4】

          【問題】
          pipeline.pyにはVIXベースのキャッシュ配分機能があるが、バックテストでは未使用。
          パニック時の防衛ロジックが欠落。

          【既存コード参照】
          src/signals/vix_signal.py - 完成済み
          - get_cash_allocation(): VIX水準に応じたキャッシュ配分
          - pipeline.py L2100-2150で使用

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             ```python
             from src.signals.vix_signal import VIXSignal

             def _apply_vix_adjustment(self, date, weights):
                 vix = self._get_vix(date)
                 vix_signal = VIXSignal(config=self.vix_config)

                 cash_target = vix_signal.get_cash_allocation(vix)

                 if cash_target > 0:
                     # ポジション削減
                     scale = 1.0 - cash_target
                     adjusted = {k: v * scale for k, v in weights.items()}
                     adjusted['CASH'] = cash_target
                     return adjusted

                 return weights
             ```

          2. VIXデータのバックテスト用キャッシュ機能追加

          3. 検証: VIX配分効果の測定

          【期待効果】
          - パニック時（2020年3月等）の損失軽減
          - MDD改善への寄与
        priority: critical

      - task_id: task_025_5
        description: |
          DynamicWeighter統合【根本対策5】

          【問題】
          pipeline.pyにはDynamicWeighter（シグナル動的重み付け）があるが、
          バックテストでは未使用。
          複数シグナルの最適統合ができていない。

          【既存コード参照】
          src/meta/dynamic_weighter.py - 完成済み
          - calculate_weights(): シグナル別の動的重み計算
          - pipeline.py L2500-2600で使用

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             ```python
             from src.meta.dynamic_weighter import DynamicWeighter

             def _combine_signals(self, date, signals_dict):
                 weighter = DynamicWeighter(config=self.weighter_config)

                 # 各シグナルの直近パフォーマンスから重みを計算
                 weights = weighter.calculate_weights(
                     signals_dict,
                     lookback=self.signal_lookback
                 )

                 # 重み付き統合
                 combined = sum(
                     sig * weights[name]
                     for name, sig in signals_dict.items()
                 )

                 return combined
             ```

          2. シグナル履歴の追跡機能追加

          3. 検証: 動的重み付け効果の測定

          【期待効果】
          - シグナル統合の精度向上
          - 適応的な戦略配分
        priority: high

      - task_id: task_025_6
        description: |
          配当・トータルリターン対応【根本対策6】

          【問題】
          バックテストでは価格リターンのみを使用。
          pipeline.pyでは配当を考慮したトータルリターンを計算。
          長期バックテストで年1-2%の乖離が発生。

          【既存コード参照】
          src/data/dividend_handler.py - 完成済み
          - get_total_return(): 配当込みリターン計算
          - adjust_for_splits(): 株式分割調整
          - pipeline.py L800-850で使用

          【実装内容】
          1. src/backtest/fast_engine.py 修正
             ```python
             from src.data.dividend_handler import DividendHandler

             def _calculate_period_return(self, ticker, start_date, end_date):
                 div_handler = DividendHandler()

                 # 配当込みトータルリターン計算
                 price_return = self._get_price_return(ticker, start_date, end_date)
                 dividend_return = div_handler.get_dividend_yield(
                     ticker, start_date, end_date
                 )

                 return price_return + dividend_return
             ```

          2. 配当データのキャッシュ機能追加

          3. 検証: トータルリターン計算の精度確認

          【期待効果】
          - 長期リターンの正確な測定
          - 特に高配当銘柄の適切な評価
        priority: high

      - task_id: task_025_7
        description: |
          Pipeline/Backtest同期監査システム【再発防止】

          【問題】
          pipeline.pyとbacktest engineの機能乖離が発生したが、
          検出手段がなかった。今後も乖離が発生する可能性。

          【実装内容】
          1. scripts/audit_pipeline_backtest_parity.py 新規作成
             ```python
             class PipelineBacktestAuditor:
                 """パイプラインとバックテストの機能同期を監査"""

                 REQUIRED_FEATURES = [
                     'TransactionCostOptimizer',
                     'DrawdownProtection',
                     'RegimeDetector',
                     'VIXSignal',
                     'DynamicWeighter',
                     'DividendHandler',
                     'CorrelationBreakDetector',
                     'KellyAllocator',
                 ]

                 def audit(self):
                     pipeline_features = self._scan_pipeline()
                     backtest_features = self._scan_backtest()

                     missing = set(pipeline_features) - set(backtest_features)
                     if missing:
                         return AuditResult(
                             status='FAIL',
                             missing_features=list(missing),
                             message=f'{len(missing)}個の機能がバックテストで未実装'
                         )

                     return AuditResult(status='PASS')
             ```

          2. tests/test_pipeline_backtest_parity.py 新規作成
             - CIで自動実行されるパリティテスト
             - 新機能追加時に両方で実装されているか確認

          3. pre-commit hook設定
             - pipeline.py変更時に自動監査実行

          【期待効果】
          - 機能乖離の早期発見
          - 再発防止の自動化
        priority: critical

      - task_id: task_025_8
        description: |
          統合検証・機能統合バックテスト

          【検証内容】
          1. 全機能統合後の15年バックテスト実行
          2. 統合前後の比較分析
          3. パイプラインとの一致度確認

          【検証スクリプト】scripts/verify_integration.py
          ```python
          test_configs = [
              {'name': 'baseline', 'features': []},
              {'name': 'phase1', 'features': ['TransactionCostOptimizer', 'DrawdownProtection']},
              {'name': 'phase2', 'features': ['phase1', 'RegimeDetector', 'VIXSignal']},
              {'name': 'full', 'features': ['all']},
          ]

          # パイプラインとの一致度テスト
          pipeline_result = run_pipeline_backtest(period='2023-01-01:2024-01-01')
          engine_result = run_engine_backtest(period='2023-01-01:2024-01-01')
          assert abs(pipeline_result.sharpe - engine_result.sharpe) < 0.05
          ```

          【成功基準】
          | 指標 | 現状 | 目標 |
          |------|------|------|
          | パイプライン一致度 | 不明 | >95% |
          | Sharpe | 0.913 | ≥1.0 |
          | MDD | -31.49% | ≥-25% |

          【出力】
          - results/integration_comparison.xlsx
          - results/integration_report.md
          - dashboard.md更新

        priority: critical

    execution_strategy: |
      【Phase 1: コア機能統合】並列実行
      - task_025_1 (TransactionCostOptimizer) → 足軽1
      - task_025_2 (DrawdownProtection) → 足軽2
      - task_025_3 (RegimeDetector) → 足軽3
      - task_025_4 (VIXSignal) → 足軽4

      【Phase 2: 補助機能統合】Phase 1完了後
      - task_025_5 (DynamicWeighter) → 足軽5
      - task_025_6 (配当・トータルリターン) → 足軽6

      【Phase 3: 再発防止】Phase 2と並行
      - task_025_7 (監査システム) → 足軽7

      【Phase 4: 統合検証】全タスク完了後
      - task_025_8 (統合検証) → 足軽8

    expected_outcome: |
      【Phase 1完了時】
      - バックテストに4つのコア機能統合完了
      - パイプラインとの一致度: 70%→85%

      【Phase 2完了時】
      - 全機能統合完了
      - パイプラインとの一致度: 85%→95%+

      【Phase 3完了時】
      - 自動監査システム稼働
      - 再発防止体制確立

      【全Phase完了時】
      - バックテスト結果が本番を正確に予測
      - Sharpe ≥1.0（本番と一致した数値）
      - MDD ≤-25%（リスク制御が正確に反映）

      【成果物】
      - 機能統合されたバックテストエンジン
      - パイプライン/バックテスト監査システム
      - 統合検証レポート

      【根本原因への対処】
      本修正は「症状」への対処ではなく「根本原因」への対処である：
      - 旧: 銘柄数削減でSharpe改善を狙う（症状への対処）
      - 新: バックテストに本番機能を統合（根本原因への対処）

      【教訓】
      1. パフォーマンス差異の原因を「なぜ5回」で深掘りする
      2. 表面的な数値改善に飛びつかない
      3. バックテストと本番の機能同期を維持する仕組みを作る

  - id: cmd_026
    timestamp: "2026-01-29T10:30:00"
    command: "バックテストエンジン統合修正 - インターフェース整合と再発防止"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_025]
    context: |
      【発見された真の問題】
      cmd_025の調査で「23機能欠落」と報告されたが、さらに深掘りした結果、
      真の問題はインターフェース不整合による統合バグであった。

      【問題の構造】
      1. engine.py が FastBacktestEngine を呼び出そうとする
      2. パラメータ不一致（BacktestConfig vs FastBacktestConfig）
      3. シグネチャ不一致（run(universe) vs run(prices, asset_names, weights_func)）
      4. TypeError発生 → except でキャッチ → Standard Mode にフォールバック
      5. **結果: FastBacktestEngineは常に使われていない**

      【具体的なバグ箇所】
      engine.py L425-429:
      ```python
      fast_engine = FastBacktestEngine(self.config, self._settings)  # 型不一致
      return fast_engine.run(universe)  # シグネチャ不一致
      ```

      FastBacktestEngine実装:
      ```python
      def __init__(self, config: FastBacktestConfig, ...):  # Settings不要
      def run(self, prices: DataFrame, ...):  # universeではなくprices
      ```

      【追加問題】
      - BacktestResultが2箇所で定義（engine.py L176, result.py L58）
      - 5つのエンジン（engine, fast_engine, ray_engine, streaming_engine, vectorbt_engine）
        が全て異なるインターフェース
      - エンジン切り替えが機能していない

      【なぜこの問題が起きたか - 5 Whys】
      1. なぜFastBacktestEngineが使われなかった？→ 呼び出しがTypeErrorで失敗
      2. なぜTypeErrorが発生？→ インターフェースが不一致
      3. なぜインターフェースが不一致？→ 各エンジンが独立開発され統合されなかった
      4. なぜ統合されなかった？→ 共通インターフェースの設計がなかった
      5. なぜ設計がなかった？→ エンジン追加時のレビュープロセスがなかった

      【再発防止策】
      1. 共通インターフェース（BacktestEngineBase）の定義
      2. 全エンジンの準拠
      3. 統合テストの追加（CI必須）
      4. エンジン追加時のチェックリスト

    tasks:
      - task_id: task_026_1
        description: |
          BacktestEngineBase 共通インターフェース定義

          【目的】
          全バックテストエンジンが準拠すべき共通インターフェースを定義。
          今後のエンジン追加時も統一されたAPIを強制する。

          【実装内容】
          1. src/backtest/base.py 新規作成
             ```python
             from abc import ABC, abstractmethod
             from dataclasses import dataclass
             from typing import Protocol
             import pandas as pd

             @dataclass
             class UnifiedBacktestConfig:
                 """全エンジン共通の設定"""
                 start_date: str
                 end_date: str
                 initial_capital: float
                 rebalance_frequency: str  # 'daily', 'weekly', 'monthly'
                 transaction_cost_bps: float = 10.0
                 slippage_bps: float = 5.0

             @dataclass
             class UnifiedBacktestResult:
                 """全エンジン共通の結果"""
                 total_return: float
                 annual_return: float
                 sharpe_ratio: float
                 max_drawdown: float
                 volatility: float
                 daily_returns: pd.Series
                 portfolio_values: pd.Series
                 trades: list  # 取引履歴

             class BacktestEngineBase(ABC):
                 """全バックテストエンジンの基底クラス"""

                 @abstractmethod
                 def run(
                     self,
                     universe: list[str],
                     prices: dict[str, pd.DataFrame],
                     config: UnifiedBacktestConfig,
                 ) -> UnifiedBacktestResult:
                     """バックテスト実行"""
                     pass

                 @abstractmethod
                 def validate_inputs(
                     self,
                     universe: list[str],
                     prices: dict[str, pd.DataFrame],
                 ) -> bool:
                     """入力検証"""
                     pass
             ```

          2. src/backtest/__init__.py に公開

          【検証】
          - 型チェック（mypy）通過
          - インターフェーステスト作成

        priority: critical

      - task_id: task_026_2
        description: |
          engine.py → BacktestEngineBase 準拠

          【目的】
          メインのBacktestEngineを共通インターフェースに準拠させる。

          【実装内容】
          1. engine.py 修正
             ```python
             from src.backtest.base import BacktestEngineBase, UnifiedBacktestConfig, UnifiedBacktestResult

             class BacktestEngine(BacktestEngineBase):
                 def run(
                     self,
                     universe: list[str],
                     prices: dict[str, pd.DataFrame],
                     config: UnifiedBacktestConfig,
                 ) -> UnifiedBacktestResult:
                     # 既存ロジックを新インターフェースでラップ
                     ...
             ```

          2. engine.py内のBacktestResult定義を削除
             → result.pyのUnifiedBacktestResultを使用

          3. FastBacktestEngine呼び出し修正（L425-429）
             → 共通インターフェース経由で呼び出し

          【検証】
          - 既存テスト通過
          - 新インターフェーステスト通過

        priority: critical

      - task_id: task_026_3
        description: |
          fast_engine.py → BacktestEngineBase 準拠

          【目的】
          FastBacktestEngineを共通インターフェースに準拠させる。

          【実装内容】
          1. fast_engine.py 修正
             ```python
             from src.backtest.base import BacktestEngineBase, UnifiedBacktestConfig, UnifiedBacktestResult

             class FastBacktestEngine(BacktestEngineBase):
                 def run(
                     self,
                     universe: list[str],
                     prices: dict[str, pd.DataFrame],
                     config: UnifiedBacktestConfig,
                 ) -> UnifiedBacktestResult:
                     # 内部でPolars/Numba最適化を使用
                     # 外部インターフェースは統一
                     ...
             ```

          2. 旧run(prices, asset_names, weights_func)は内部メソッドに移動

          【検証】
          - engine.pyからの呼び出し成功
          - 速度劣化なし確認

        priority: critical

      - task_id: task_026_4
        description: |
          残り3エンジン → BacktestEngineBase 準拠

          【対象】
          - ray_engine.py (分散処理)
          - streaming_engine.py (ストリーミング)
          - vectorbt_engine.py (超高速)

          【実装内容】
          各エンジンを BacktestEngineBase に準拠させる。
          内部実装は維持し、外部インターフェースのみ統一。

          【検証】
          - 全エンジンが同一インターフェースで呼び出し可能
          - エンジン切り替えが正常動作

        priority: high

      - task_id: task_026_5
        description: |
          エンジンファクトリ＆自動選択

          【目的】
          状況に応じて最適なエンジンを自動選択する仕組み。

          【実装内容】
          1. src/backtest/factory.py 新規作成
             ```python
             class BacktestEngineFactory:
                 @staticmethod
                 def create(
                     mode: str = 'auto',
                     config: UnifiedBacktestConfig = None,
                 ) -> BacktestEngineBase:
                     """
                     mode:
                       - 'auto': 銘柄数/期間から自動選択
                       - 'standard': BacktestEngine
                       - 'fast': FastBacktestEngine
                       - 'ray': RayBacktestEngine（大規模並列）
                       - 'vectorbt': VectorBTEngine（超高速）
                     """
                     if mode == 'auto':
                         return cls._auto_select(config)
                     ...

                 @staticmethod
                 def _auto_select(config) -> BacktestEngineBase:
                     # 銘柄数 > 500 かつ GPU利用可能 → vectorbt
                     # 銘柄数 > 100 → fast
                     # それ以外 → standard
                     ...
             ```

          2. engine.py のエンジン切り替えロジックをファクトリに移行

          【検証】
          - auto選択が正常動作
          - 各モード指定が正常動作

        priority: high

      - task_id: task_026_6
        description: |
          統合テスト＆CIパイプライン

          【目的】
          エンジン統合の破損を自動検出する仕組み。

          【実装内容】
          1. tests/test_engine_integration.py 新規作成
             ```python
             import pytest
             from src.backtest.base import BacktestEngineBase
             from src.backtest.factory import BacktestEngineFactory

             # 全エンジンが共通インターフェースに準拠
             @pytest.mark.parametrize("mode", ["standard", "fast", "ray", "vectorbt"])
             def test_engine_interface_compliance(mode):
                 engine = BacktestEngineFactory.create(mode=mode)
                 assert isinstance(engine, BacktestEngineBase)

             # 全エンジンで同一結果（許容誤差内）
             @pytest.mark.parametrize("mode", ["standard", "fast", "ray", "vectorbt"])
             def test_engine_result_consistency(mode, sample_data):
                 engine = BacktestEngineFactory.create(mode=mode)
                 result = engine.run(**sample_data)
                 assert abs(result.sharpe_ratio - EXPECTED_SHARPE) < 0.01

             # エンジン切り替えが正常動作
             def test_engine_switching():
                 for mode in ["standard", "fast", "ray", "vectorbt"]:
                     engine = BacktestEngineFactory.create(mode=mode)
                     result = engine.run(**sample_data)
                     assert result is not None
             ```

          2. .github/workflows/test.yml 更新
             - エンジン統合テストを必須に
             - PR時に自動実行

          【検証】
          - CI通過
          - 全テストグリーン

        priority: critical

      - task_id: task_026_7
        description: |
          エンジン追加チェックリスト＆ドキュメント

          【目的】
          今後のエンジン追加時に統合問題を防止。

          【実装内容】
          1. docs/ENGINE_INTEGRATION.md 新規作成
             ```markdown
             # バックテストエンジン統合ガイド

             ## 新エンジン追加チェックリスト

             - [ ] BacktestEngineBase を継承
             - [ ] run() シグネチャが統一インターフェースに準拠
             - [ ] UnifiedBacktestResult を返却
             - [ ] validate_inputs() 実装
             - [ ] test_engine_integration.py にテスト追加
             - [ ] BacktestEngineFactory に登録
             - [ ] CI通過確認

             ## 禁止事項

             - 独自のBacktestResult定義（UnifiedBacktestResultを使用）
             - 独自のrun()シグネチャ（内部メソッドは自由）
             - ファクトリ未登録のエンジン
             ```

          2. pre-commit hook追加
             - src/backtest/に新ファイル追加時、チェックリスト確認を促す

          【検証】
          - ドキュメント読みやすさ確認
          - hook動作確認

        priority: high

      - task_id: task_026_8
        description: |
          最終検証＆レグレッションテスト

          【検証内容】
          1. 全エンジンの統合テスト実行
          2. 既存バックテスト結果との比較（レグレッション）
          3. パフォーマンス測定（速度劣化なし確認）
          4. エンジン切り替えの動作確認

          【成功基準】
          | 項目 | 基準 |
          |------|------|
          | インターフェース準拠 | 5エンジン全て準拠 |
          | 結果一貫性 | Sharpe差異 < 0.01 |
          | 速度劣化 | < 5% |
          | CI通過 | 全テストグリーン |

          【出力】
          - results/engine_integration_report.md
          - dashboard.md更新

        priority: critical

    execution_strategy: |
      【Phase 1: 基盤構築】並列実行
      - task_026_1 (共通インターフェース定義) → 足軽1
      - task_026_6 (統合テスト作成) → 足軽2

      【Phase 2: エンジン準拠】task_026_1完了後、並列実行
      - task_026_2 (engine.py準拠) → 足軽3
      - task_026_3 (fast_engine.py準拠) → 足軽4
      - task_026_4 (残り3エンジン準拠) → 足軽5,6,7

      【Phase 3: 統合】Phase 2完了後
      - task_026_5 (ファクトリ＆自動選択) → 足軽3
      - task_026_7 (チェックリスト＆ドキュメント) → 足軽4

      【Phase 4: 検証】全タスク完了後
      - task_026_8 (最終検証) → 足軽8

    expected_outcome: |
      【成果物】
      1. BacktestEngineBase - 全エンジン共通インターフェース
      2. 5エンジン全てが準拠
      3. BacktestEngineFactory - 自動エンジン選択
      4. 統合テスト＆CI設定
      5. エンジン追加チェックリスト

      【再発防止効果】
      - インターフェース不整合は型エラーで即座に検出
      - CIで統合テストが自動実行
      - 新エンジン追加時はチェックリスト必須

      【技術的負債の解消】
      - BacktestResult重複定義の解消
      - エンジン切り替えバグの修正
      - 5エンジンの統一API

      【教訓の制度化】
      1. 「なぜ5回」の深掘り → 真の問題発見
      2. 共通インターフェース → 統合問題の予防
      3. 統合テスト＆CI → 自動検出
      4. チェックリスト → プロセスの標準化

  - id: cmd_027
    timestamp: "2026-01-29T19:15:00"
    command: "設計品質改善 - 技術的負債解消と再発防止の仕組み化"
    project: auto_allocation_system
    priority: high
    status: pending
    depends_on: []
    context: |
      【背景】
      cmd_026の調査過程で、バックテストエンジン以外にも多数の設計問題が発見された。
      これらを放置すると、同様の統合問題が他の箇所でも発生するリスクがある。

      【発見された問題（15件）】
      HIGH優先度（3件）:
      - A1: FallbackMode重複定義（settings.py / fallback.py）
      - A3: パイプライン肥大化（3583行のモノリシック設計）
      - T1: 統合テスト欠如（3ファイルのみ）

      MEDIUM優先度（10件）:
      - A2: 動的パラメータモジュール重複（dynamic_threshold.py / dynamic_thresholds.py）
      - A4: バージョン付きモジュール混在（*_v2.py）
      - Q1: パラメータ定義の散在（6箇所の*_params.py）
      - Q2: ハードコード値（main.py, engine.py）
      - Q3: 型定義不整合（FallbackMode/DegradationLevel混在）
      - T2: 単体テストカバレッジ不足（37シグナルクラス）
      - O1: エラーハンドリング不完全
      - O2: ログ・監視不足
      - P1: メモリ効率不明確
      - P2: キャッシュ機構分散（3つの独立キャッシュ）

      【目標】
      1. HIGH優先度の問題を解消
      2. 同様の問題が再発しない仕組みを構築
      3. コード品質を自動で監視・強制する仕組みを導入

      【再発防止の仕組み化方針】
      1. 静的解析ツール（型チェック、重複検出）の導入
      2. アーキテクチャテスト（依存関係、モジュールサイズ）
      3. pre-commitフックによる自動チェック
      4. 設計レビューチェックリスト

    tasks:
      - task_id: task_027_1
        description: |
          FallbackMode/DegradationLevel型定義の統一【A1】

          【問題】
          settings.py:44-49 と fallback.py:31-38 で FallbackMode が重複定義。
          fallback.py には NONE が追加されており、定義が異なる。

          【実装内容】
          1. src/config/settings.py を単一の真実の源(SSOT)とする
             ```python
             # settings.py - 統一定義
             class FallbackMode(str, Enum):
                 NONE = "none"
                 HOLD_PREVIOUS = "hold_previous"
                 EQUAL_WEIGHT = "equal_weight"
                 CASH = "cash"
             ```

          2. src/orchestrator/fallback.py から FallbackMode 定義を削除
             ```python
             # fallback.py
             from src.config.settings import FallbackMode  # インポートに変更
             ```

          3. DegradationLevel は fallback.py に残す（フォールバック専用）

          4. 全ファイルのインポートパスを更新

          【検証】
          - mypy 型チェック通過
          - 既存テスト全通過

        priority: critical

      - task_id: task_027_2
        description: |
          動的パラメータモジュール統合【A2】

          【問題】
          dynamic_threshold.py (740行) と dynamic_thresholds.py が並存。
          同様の機能（リバランス閾値、VIX閾値、ケリー基準）が重複実装。

          【実装内容】
          1. 2ファイルの機能を分析・比較

          2. src/analysis/dynamic_parameters.py に統合
             ```python
             # 統合モジュール構成
             class DynamicParameterBase(ABC):
                 """動的パラメータの基底クラス"""
                 @abstractmethod
                 def calculate(self, market_data: MarketData) -> float:
                     pass

             class RebalanceThreshold(DynamicParameterBase):
                 """リバランス閾値計算"""
                 pass

             class VIXThreshold(DynamicParameterBase):
                 """VIX閾値計算"""
                 pass

             class KellyCriterion(DynamicParameterBase):
                 """ケリー基準計算"""
                 pass
             ```

          3. 旧ファイルを削除、インポートパスを更新

          【検証】
          - 機能の同等性確認
          - パフォーマンス劣化なし

        priority: high

      - task_id: task_027_3
        description: |
          パイプライン責務分割 Phase 1【A3】

          【問題】
          pipeline.py が 3583行のモノリシック設計。
          8つの責務が1ファイルに混在。

          【実装内容 - Phase 1: ステップ抽出】
          1. src/orchestrator/steps/ ディレクトリ作成

          2. 最も独立性の高い3ステップを抽出
             ```
             steps/
             ├── __init__.py
             ├── base.py (PipelineStepBase)
             ├── data_fetch.py (データ取得・品質チェック)
             ├── anomaly_check.py (異常検出)
             └── output.py (結果出力・ログ)
             ```

          3. PipelineStepBase 基底クラス定義
             ```python
             class PipelineStepBase(ABC):
                 @abstractmethod
                 def execute(self, context: PipelineContext) -> StepResult:
                     pass

                 @abstractmethod
                 def validate(self, context: PipelineContext) -> bool:
                     pass
             ```

          4. pipeline.py から呼び出しを変更（委譲パターン）

          【検証】
          - 既存テスト全通過
          - パイプライン動作に変化なし

          【注意】
          Phase 1 では3ステップのみ抽出。残りは task_027_7 で対応。

        priority: high

      - task_id: task_027_4
        description: |
          統合テスト拡充【T1】

          【問題】
          tests/integration/ に3ファイルのみ。
          重要シナリオのテストが欠落。

          【実装内容】
          1. tests/integration/ に以下を追加

          2. test_error_recovery.py
             ```python
             class TestErrorRecovery:
                 def test_data_fetch_failure_fallback(self):
                     """データ取得失敗時のフォールバック"""
                     pass

                 def test_signal_generation_failure_fallback(self):
                     """シグナル生成失敗時のフォールバック"""
                     pass

                 def test_allocation_failure_fallback(self):
                     """配分計算失敗時のフォールバック"""
                     pass
             ```

          3. test_fallback_transitions.py
             ```python
             class TestFallbackTransitions:
                 def test_none_to_hold_previous(self):
                     """NONE→HOLD_PREVIOUS遷移"""
                     pass

                 def test_degradation_level_escalation(self):
                     """劣化レベルのエスカレーション"""
                     pass

                 def test_recovery_from_fallback(self):
                     """フォールバックからの復帰"""
                     pass
             ```

          4. test_cache_behavior.py
             ```python
             class TestCacheBehavior:
                 def test_cache_hit(self):
                     """キャッシュヒット時の動作"""
                     pass

                 def test_cache_invalidation(self):
                     """キャッシュ無効化"""
                     pass

                 def test_cache_consistency(self):
                     """複数キャッシュ間の一貫性"""
                     pass
             ```

          5. test_performance_regression.py
             ```python
             class TestPerformanceRegression:
                 def test_pipeline_execution_time(self):
                     """パイプライン実行時間の閾値チェック"""
                     pass

                 def test_memory_usage(self):
                     """メモリ使用量の閾値チェック"""
                     pass
             ```

          【検証】
          - 全テスト通過
          - カバレッジ向上確認

        priority: critical

      - task_id: task_027_5
        description: |
          静的解析・品質チェック自動化【仕組み化1】

          【目的】
          型定義重複、未使用コード、循環依存を自動検出。
          同様の問題の再発を防止。

          【実装内容】
          1. pyproject.toml に静的解析ツール設定追加
             ```toml
             [tool.mypy]
             strict = true
             warn_redundant_casts = true
             warn_unused_ignores = true
             disallow_untyped_defs = true

             [tool.ruff]
             select = ["E", "F", "I", "N", "W", "UP", "ANN", "B", "C4", "SIM"]
             line-length = 100

             [tool.vulture]
             min_confidence = 80
             ```

          2. scripts/lint.sh 作成
             ```bash
             #!/bin/bash
             set -e
             echo "Running mypy..."
             mypy src/
             echo "Running ruff..."
             ruff check src/
             echo "Running vulture (dead code)..."
             vulture src/ --min-confidence 80
             echo "All checks passed!"
             ```

          3. .github/workflows/lint.yml 作成
             - PR時に自動実行
             - 失敗時はマージブロック

          【検証】
          - ローカルで lint.sh 通過
          - CI で自動実行確認

        priority: critical

      - task_id: task_027_6
        description: |
          アーキテクチャテスト導入【仕組み化2】

          【目的】
          モジュール依存関係、ファイルサイズ、重複定義を自動検出。
          設計劣化を早期に発見。

          【実装内容】
          1. tests/architecture/ ディレクトリ作成

          2. tests/architecture/test_dependencies.py
             ```python
             import ast
             from pathlib import Path

             class TestArchitectureDependencies:
                 def test_no_circular_imports(self):
                     """循環インポートがないこと"""
                     # importlibで依存グラフ構築、サイクル検出
                     pass

                 def test_layer_dependencies(self):
                     """レイヤー依存関係の遵守"""
                     # data → signals → allocation → orchestrator
                     ALLOWED = {
                         'orchestrator': ['allocation', 'signals', 'data', 'config'],
                         'allocation': ['signals', 'data', 'config'],
                         'signals': ['data', 'config'],
                         'data': ['config'],
                     }
                     pass
             ```

          3. tests/architecture/test_code_quality.py
             ```python
             class TestCodeQuality:
                 MAX_FILE_LINES = 1000
                 MAX_FUNCTION_LINES = 100

                 def test_file_size_limits(self):
                     """ファイルサイズ上限チェック"""
                     for py_file in Path('src').rglob('*.py'):
                         lines = len(py_file.read_text().splitlines())
                         assert lines <= self.MAX_FILE_LINES, \
                             f"{py_file}: {lines} lines exceeds {self.MAX_FILE_LINES}"

                 def test_no_duplicate_class_definitions(self):
                     """同名クラスの重複定義がないこと"""
                     class_locations = {}
                     for py_file in Path('src').rglob('*.py'):
                         tree = ast.parse(py_file.read_text())
                         for node in ast.walk(tree):
                             if isinstance(node, ast.ClassDef):
                                 if node.name in class_locations:
                                     pytest.fail(
                                         f"Duplicate class {node.name}: "
                                         f"{class_locations[node.name]} and {py_file}"
                                     )
                                 class_locations[node.name] = py_file
             ```

          4. CI に組み込み（pytest tests/architecture/）

          【検証】
          - 現在の問題を検出できること
          - 修正後にテスト通過

        priority: critical

      - task_id: task_027_7
        description: |
          パイプライン責務分割 Phase 2【A3続き】

          【前提】task_027_3 完了後

          【実装内容 - Phase 2: 残りステップ抽出】
          1. 残り5ステップを抽出
             ```
             steps/
             ├── signal_generation.py (信号生成)
             ├── strategy_evaluation.py (戦略評価)
             ├── weighting.py (ウェイト計算)
             ├── allocation.py (資産配分)
             └── fallback.py (フォールバック処理)
             ```

          2. pipeline.py を 400行以下に削減
             - 実行フロー制御のみ
             - 各ステップへの委譲

          3. PipelineContext でステップ間のデータ受け渡し
             ```python
             @dataclass
             class PipelineContext:
                 config: PipelineConfig
                 market_data: Optional[MarketData] = None
                 signals: Optional[Dict[str, Signal]] = None
                 weights: Optional[PortfolioWeights] = None
                 result: Optional[PipelineResult] = None
             ```

          【検証】
          - pipeline.py が 400行以下
          - 既存テスト全通過
          - パフォーマンス劣化なし

        priority: high

      - task_id: task_027_8
        description: |
          pre-commitフック＆設計レビューチェックリスト【仕組み化3】

          【目的】
          コミット前に品質チェックを強制。
          レビュー時の確認事項を標準化。

          【実装内容】
          1. .pre-commit-config.yaml 作成
             ```yaml
             repos:
               - repo: local
                 hooks:
                   - id: mypy
                     name: mypy type check
                     entry: mypy src/
                     language: system
                     types: [python]
                     pass_filenames: false

                   - id: ruff
                     name: ruff linter
                     entry: ruff check src/
                     language: system
                     types: [python]

                   - id: architecture-test
                     name: architecture tests
                     entry: pytest tests/architecture/ -q
                     language: system
                     pass_filenames: false

                   - id: file-size-check
                     name: check file size
                     entry: python scripts/check_file_size.py
                     language: system
                     types: [python]
             ```

          2. docs/DESIGN_REVIEW_CHECKLIST.md 作成
             ```markdown
             # 設計レビューチェックリスト

             ## 新規モジュール追加時
             - [ ] 既存モジュールとの責務重複がないか
             - [ ] 同名クラス/関数が他にないか
             - [ ] 適切なレイヤーに配置されているか
             - [ ] 1000行以下か
             - [ ] 型ヒントが完備されているか
             - [ ] テストが追加されているか

             ## 既存モジュール修正時
             - [ ] 破壊的変更がないか
             - [ ] インターフェース変更時は呼び出し元を全て更新したか
             - [ ] 型定義を変更した場合、重複定義がないか

             ## Enum/定数追加時
             - [ ] 既存の同名Enumがないか
             - [ ] 単一の真実の源(SSOT)に定義されているか
             ```

          3. PR テンプレート (.github/PULL_REQUEST_TEMPLATE.md)
             ```markdown
             ## チェックリスト
             - [ ] 静的解析（mypy, ruff）通過
             - [ ] アーキテクチャテスト通過
             - [ ] 設計レビューチェックリスト確認済み
             ```

          【検証】
          - pre-commit hook 動作確認
          - ドキュメント完備

        priority: high

      - task_id: task_027_9
        description: |
          ハードコード値の設定ファイル移行【Q2】

          【問題】
          main.py:298 - DEFAULT_BACKTEST_UNIVERSE がハードコード
          engine.py:69 - DEFAULT_WORKERS がハードコード

          【実装内容】
          1. config/default.yaml に追加
             ```yaml
             backtest:
               default_universe:
                 - AAPL
                 - MSFT
                 - GOOGL
                 - AMZN
                 - META

             system:
               default_workers: 4
               max_workers: 8
             ```

          2. src/config/settings.py に対応フィールド追加
             ```python
             @dataclass
             class SystemConfig:
                 default_workers: int = 4
                 max_workers: int = 8

             @dataclass
             class BacktestSettings:
                 default_universe: list[str] = field(default_factory=list)
             ```

          3. main.py, engine.py から参照を変更

          4. 環境変数での上書き対応
             - PORTFOLIO_DEFAULT_WORKERS
             - PORTFOLIO_BACKTEST_UNIVERSE (カンマ区切り)

          【検証】
          - 設定ファイルから値が読み込まれること
          - 環境変数で上書きできること

        priority: medium

      - task_id: task_027_10
        description: |
          キャッシュ機構統合【P2】

          【問題】
          3つの独立したキャッシュが存在:
          - src/backtest/cache.py
          - src/data/cache.py
          - src/backtest/covariance_cache.py
          キャッシュ無効化戦略が不統一。

          【実装内容】
          1. src/utils/cache_manager.py 作成
             ```python
             class UnifiedCacheManager:
                 """統一キャッシュマネージャー"""

                 def __init__(self, max_memory_mb: int = 1024):
                     self.caches: Dict[str, LRUCache] = {}
                     self.max_memory_mb = max_memory_mb
                     self.stats = CacheStats()

                 def get_cache(self, name: str, maxsize: int = 1000) -> LRUCache:
                     """名前付きキャッシュを取得"""
                     if name not in self.caches:
                         self.caches[name] = LRUCache(maxsize=maxsize)
                     return self.caches[name]

                 def invalidate_all(self):
                     """全キャッシュ無効化"""
                     for cache in self.caches.values():
                         cache.clear()

                 def get_stats(self) -> CacheStats:
                     """統計情報取得"""
                     return self.stats

             # シングルトンインスタンス
             cache_manager = UnifiedCacheManager()
             ```

          2. 既存キャッシュを統一マネージャー経由に変更
             ```python
             # 旧: 独自キャッシュ
             signal_cache = {}

             # 新: 統一マネージャー経由
             signal_cache = cache_manager.get_cache('signals', maxsize=1000)
             ```

          3. キャッシュ統計のログ出力
             ```python
             logger.info(f"Cache stats: {cache_manager.get_stats()}")
             ```

          【検証】
          - キャッシュ動作に変化なし
          - 統計情報が取得できること

        priority: medium

      - task_id: task_027_11
        description: |
          最終検証＆品質ゲート確認

          【検証内容】
          1. 全静的解析ツール通過
             - mypy strict mode
             - ruff (全ルール)
             - vulture (dead code)

          2. アーキテクチャテスト通過
             - 循環依存なし
             - レイヤー依存関係遵守
             - 重複クラス定義なし
             - ファイルサイズ上限遵守

          3. 統合テスト通過
             - 新規追加テスト全通過
             - 既存テスト全通過

          4. パイプライン動作確認
             - 分割後も同一結果
             - パフォーマンス劣化なし

          【成功基準】
          | 項目 | 基準 |
          |------|------|
          | mypy | エラー0 |
          | ruff | 警告0 |
          | アーキテクチャテスト | 全通過 |
          | 統合テスト | 全通過 |
          | pipeline.py行数 | ≤400行 |
          | 重複クラス定義 | 0件 |

          【出力】
          - results/design_quality_report.md
          - dashboard.md更新

        priority: critical

    execution_strategy: |
      【Phase 1: 型統一＆テスト基盤】並列実行
      - task_027_1 (FallbackMode統一) → 足軽1
      - task_027_4 (統合テスト拡充) → 足軽2
      - task_027_5 (静的解析自動化) → 足軽3
      - task_027_6 (アーキテクチャテスト) → 足軽4

      【Phase 2: モジュール整理】Phase 1完了後
      - task_027_2 (動的パラメータ統合) → 足軽5
      - task_027_3 (パイプライン分割Phase1) → 足軽6

      【Phase 3: 仕組み化＆継続改善】Phase 2完了後
      - task_027_7 (パイプライン分割Phase2) → 足軽6
      - task_027_8 (pre-commit＆チェックリスト) → 足軽7
      - task_027_9 (ハードコード移行) → 足軽3
      - task_027_10 (キャッシュ統合) → 足軽4

      【Phase 4: 最終検証】全タスク完了後
      - task_027_11 (最終検証) → 足軽8

    expected_outcome: |
      【成果物】
      1. 統一された型定義（FallbackMode, DegradationLevel）
      2. 統合された動的パラメータモジュール
      3. 責務分割されたパイプライン（400行以下）
      4. 統合テスト拡充（+4ファイル）
      5. 静的解析・アーキテクチャテストのCI組み込み
      6. pre-commitフック設定
      7. 設計レビューチェックリスト
      8. 統一キャッシュマネージャー

      【再発防止の仕組み】
      | 問題 | 防止策 | 検出タイミング |
      |------|--------|---------------|
      | 型定義重複 | アーキテクチャテスト | CI（PR時） |
      | ファイル肥大化 | ファイルサイズチェック | pre-commit |
      | 循環依存 | 依存関係テスト | CI（PR時） |
      | 品質劣化 | 静的解析（mypy, ruff） | pre-commit |
      | 統合問題 | 統合テスト | CI（PR時） |
      | レビュー漏れ | チェックリスト | PRテンプレート |

      【品質指標の改善】
      - 重複クラス定義: 2件 → 0件
      - pipeline.py行数: 3583行 → 400行以下
      - 統合テスト: 3ファイル → 7ファイル
      - 静的解析カバレッジ: 部分的 → 100%

      【教訓の制度化】
      今回の問題から学んだ教訓を仕組みとして定着:
      1. 「型定義は1箇所に」→ アーキテクチャテストで強制
      2. 「モジュールは小さく」→ ファイルサイズ上限チェック
      3. 「統合は常にテスト」→ CI必須の統合テスト
      4. 「レビューは標準化」→ チェックリスト＆PRテンプレート

  - id: cmd_028
    timestamp: "2026-01-29T19:45:00"
    command: "15年包括バックテスト - 全銘柄・全頻度検証と改善提案"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_026, cmd_027]
    context: |
      【背景】
      cmd_026（エンジン統合）とcmd_027（設計品質改善）の完了により、
      バックテストエンジンが正しく動作し、コード品質も担保された状態となる。
      この状態で、全銘柄を対象とした包括的な15年バックテストを実施し、
      システムの真のパフォーマンスを測定する。

      【目的】
      1. 統合されたエンジンで正確なバックテスト結果を取得
      2. 日次/週次/月次の3頻度で比較分析
      3. 全銘柄（828銘柄）を対象に検証
      4. 改善策を提案し、次の開発サイクルへつなげる

      【期間】
      2010-01-01 ～ 2024-12-31（約15年）

      【対象銘柄】
      config/universe.yaml に定義された全銘柄（828銘柄）

      【リバランス頻度】
      - 日次（daily）
      - 週次（weekly）
      - 月次（monthly）

      【ベンチマーク】
      - SPY (S&P 500)
      - QQQ (NASDAQ 100)
      - 60/40 Portfolio (SPY/TLT)
      - AGG (米国債券)

      【成功基準】
      | 指標 | 目標 |
      |------|------|
      | Sharpe Ratio | ≥ 1.0 |
      | 年率リターン | > SPY |
      | 最大ドローダウン | ≤ -25% |
      | 60/40超過Sharpe | > 0.998 |

    tasks:
      - task_id: task_028_1
        description: |
          日次バックテスト実行（全828銘柄）

          【実行内容】
          1. 統合エンジンで日次リバランスBT実行
             ```python
             from src.backtest.factory import BacktestEngineFactory
             from src.backtest.base import UnifiedBacktestConfig

             config = UnifiedBacktestConfig(
                 start_date='2010-01-01',
                 end_date='2024-12-31',
                 initial_capital=1_000_000,
                 rebalance_frequency='daily',
                 transaction_cost_bps=10.0,
                 slippage_bps=5.0,
             )

             engine = BacktestEngineFactory.create(mode='auto')
             result = engine.run(universe=all_828_tickers, prices=price_data, config=config)
             ```

          2. 結果を保存
             - results/backtest_daily_15y.parquet
             - results/backtest_daily_15y_summary.yaml

          3. 基本指標計算
             - 年率リターン
             - Sharpe Ratio
             - Sortino Ratio
             - 最大ドローダウン
             - Calmar Ratio
             - 勝率（対SPY年別）

          【出力】
          - results/backtest_daily_15y.parquet
          - results/backtest_daily_15y_summary.yaml

        priority: critical

      - task_id: task_028_2
        description: |
          週次バックテスト実行（全828銘柄）

          【実行内容】
          1. 統合エンジンで週次リバランスBT実行
             ```python
             config = UnifiedBacktestConfig(
                 start_date='2010-01-01',
                 end_date='2024-12-31',
                 initial_capital=1_000_000,
                 rebalance_frequency='weekly',
                 transaction_cost_bps=10.0,
                 slippage_bps=5.0,
             )

             engine = BacktestEngineFactory.create(mode='auto')
             result = engine.run(universe=all_828_tickers, prices=price_data, config=config)
             ```

          2. 結果を保存
             - results/backtest_weekly_15y.parquet
             - results/backtest_weekly_15y_summary.yaml

          3. 基本指標計算（task_028_1と同様）

          【出力】
          - results/backtest_weekly_15y.parquet
          - results/backtest_weekly_15y_summary.yaml

        priority: critical

      - task_id: task_028_3
        description: |
          月次バックテスト実行（全828銘柄）

          【実行内容】
          1. 統合エンジンで月次リバランスBT実行
             ```python
             config = UnifiedBacktestConfig(
                 start_date='2010-01-01',
                 end_date='2024-12-31',
                 initial_capital=1_000_000,
                 rebalance_frequency='monthly',
                 transaction_cost_bps=10.0,
                 slippage_bps=5.0,
             )

             engine = BacktestEngineFactory.create(mode='auto')
             result = engine.run(universe=all_828_tickers, prices=price_data, config=config)
             ```

          2. 結果を保存
             - results/backtest_monthly_15y.parquet
             - results/backtest_monthly_15y_summary.yaml

          3. 基本指標計算（task_028_1と同様）

          【出力】
          - results/backtest_monthly_15y.parquet
          - results/backtest_monthly_15y_summary.yaml

        priority: critical

      - task_id: task_028_4
        description: |
          ベンチマーク計算

          【実行内容】
          1. 各ベンチマークの15年パフォーマンス計算
             - SPY: Buy-and-Hold
             - QQQ: Buy-and-Hold
             - 60/40: SPY 60% / TLT 40%（月次リバランス）
             - AGG: Buy-and-Hold

          2. 同一期間・同一指標で計算
             ```python
             benchmarks = {
                 'SPY': calculate_benchmark('SPY', '2010-01-01', '2024-12-31'),
                 'QQQ': calculate_benchmark('QQQ', '2010-01-01', '2024-12-31'),
                 '60/40': calculate_6040_benchmark('2010-01-01', '2024-12-31'),
                 'AGG': calculate_benchmark('AGG', '2010-01-01', '2024-12-31'),
             }
             ```

          3. 結果を保存
             - results/benchmarks_15y.yaml

          【出力】
          - results/benchmarks_15y.yaml

        priority: high

      - task_id: task_028_5
        description: |
          頻度別比較分析

          【実行内容】
          1. 日次/週次/月次の比較表作成
             ```markdown
             | 指標 | 日次 | 週次 | 月次 | SPY | 60/40 |
             |------|------|------|------|-----|-------|
             | 年率リターン | X% | Y% | Z% | A% | B% |
             | Sharpe Ratio | X | Y | Z | A | B |
             | 最大DD | X% | Y% | Z% | A% | B% |
             | 取引回数 | X | Y | Z | - | - |
             | 推定取引コスト | X% | Y% | Z% | - | - |
             ```

          2. 取引コスト影響分析
             - 頻度別の推定年間取引コスト
             - ネットリターン計算
             - コスト調整後Sharpe

          3. 最適頻度の特定
             - コスト考慮後のベスト頻度
             - リスク調整後のベスト頻度

          【出力】
          - results/frequency_comparison.md
          - results/frequency_comparison.xlsx

        priority: high

      - task_id: task_028_6
        description: |
          年別・市場環境別分析

          【実行内容】
          1. 年別パフォーマンス表
             ```markdown
             | 年 | 戦略(月次) | SPY | 差分 | 勝敗 |
             |----|----------|-----|------|------|
             | 2010 | X% | Y% | Z% | 勝/負 |
             ...
             ```

          2. 市場環境別分析
             - 上昇相場（2013-2014, 2017, 2019, 2021）
             - 下落相場（2011, 2018, 2022）
             - 高ボラティリティ（2020 COVID）
             - 金利上昇期（2022-2024）

          3. レジーム別Sharpe計算
             ```python
             regimes = {
                 'bull': ['2013', '2014', '2017', '2019', '2021'],
                 'bear': ['2011', '2018', '2022'],
                 'high_vol': ['2020'],
                 'rate_hike': ['2022', '2023', '2024'],
             }
             for regime, years in regimes.items():
                 sharpe = calculate_sharpe_for_years(result, years)
             ```

          【出力】
          - results/yearly_performance.md
          - results/regime_analysis.md

        priority: high

      - task_id: task_028_7
        description: |
          改善提案の策定

          【実行内容】
          1. パフォーマンスギャップ分析
             - 目標未達の指標を特定
             - 原因を分析

          2. 改善提案の策定
             各提案に以下を含める:
             ```yaml
             improvement:
               id: IMP-XXX
               title: "改善提案タイトル"
               target_metric: "改善対象指標"
               current_value: X
               target_value: Y
               expected_improvement: Z
               implementation:
                 - step1
                 - step2
               risk: "実装リスク"
               priority: high/medium/low
             ```

          3. 提案カテゴリ
             - シグナル改善（モメンタム、ミーンリバージョン等）
             - リスク管理改善（VIX、ドローダウン制御等）
             - 配分アルゴリズム改善（HRP、リスクパリティ等）
             - コスト最適化（リバランス頻度、閾値等）

          【出力】
          - results/improvement_proposals.yaml
          - results/improvement_proposals.md

        priority: critical

      - task_id: task_028_8
        description: |
          包括レポート作成

          【実行内容】
          1. エグゼクティブサマリー
             - 主要指標一覧
             - 目標達成状況
             - 推奨設定

          2. 詳細分析セクション
             - パフォーマンス詳細
             - 頻度別比較
             - 年別分析
             - 市場環境別分析

          3. 改善提案セクション
             - 優先度順の改善提案
             - 実装ロードマップ
             - 期待効果

          4. 技術詳細セクション
             - 使用エンジン
             - パラメータ設定
             - データソース

          【出力形式】
          - results/full_portfolio_report_v2.md（Markdown）
          - results/full_portfolio_report_v2.xlsx（Excel）
          - results/full_portfolio_report_v2.pdf（PDF、可能であれば）

          【dashboard.md更新】
          - 主要結果をダッシュボードに反映
          - 改善提案を「要対応」セクションに追加

        priority: critical

    execution_strategy: |
      【Phase 1: バックテスト実行】並列実行
      - task_028_1 (日次BT) → 足軽1
      - task_028_2 (週次BT) → 足軽2
      - task_028_3 (月次BT) → 足軽3
      - task_028_4 (ベンチマーク) → 足軽4

      【Phase 2: 分析】Phase 1完了後、並列実行
      - task_028_5 (頻度別比較) → 足軽5
      - task_028_6 (年別・環境別分析) → 足軽6

      【Phase 3: 提案＆レポート】Phase 2完了後
      - task_028_7 (改善提案) → 足軽7
      - task_028_8 (包括レポート) → 足軽8

    expected_outcome: |
      【成果物】
      1. 日次/週次/月次バックテスト結果（各.parquet + .yaml）
      2. ベンチマーク比較データ
      3. 頻度別比較レポート
      4. 年別・市場環境別分析レポート
      5. 改善提案（優先度付き）
      6. 包括レポート（Markdown + Excel）

      【期待される洞察】
      1. 最適なリバランス頻度の特定
      2. 市場環境別の戦略有効性
      3. 目標達成に必要な改善点
      4. 次の開発サイクルへの入力

      【次のアクション】
      レポート結果に基づき、cmd_029以降で改善提案を実装。
      改善サイクルを継続し、目標達成を目指す。

      【品質保証】
      - cmd_026完了により、エンジン統合問題は解消済み
      - cmd_027完了により、設計品質は担保済み
      - 正確なバックテスト結果が期待できる

  - id: cmd_029
    timestamp: "2026-01-29T20:00:00"
    command: "統一バックテスト規格の策定と再実行"
    project: auto_allocation_system
    priority: critical
    status: pending
    depends_on: [cmd_026, cmd_027]
    context: |
      【背景】
      cmd_028の調査で、日次/週次/月次バックテストが全く異なる条件で
      実行されていることが判明した。比較として無効である。

      【発見された問題】
      | 項目 | 日次 | 週次 | 月次 |
      |------|------|------|------|
      | エンジン | VectorBT | 独自実装 | Streaming |
      | 銘柄数 | 490 | 828 | 102 |
      | 初期資本 | $1M | $1M | $100K |
      | スリッページ | 5bps | あり | なし |
      | 戦略 | 汎用 | モメンタム | エンジン依存 |
      | データ取得 | yfinance | キャッシュ | キャッシュ |

      【目標】
      1. 統一バックテスト規格（Backtest Standard）を策定
      2. 規格に基づくスクリプトを作成
      3. 統一条件で日次/週次/月次を再実行
      4. 正確な比較分析を実施

      【統一規格の要件】
      - 同一エンジン（VectorBTStyleEngine）
      - 同一ユニバース（全銘柄）
      - 同一初期資本（$1,000,000）
      - 同一取引コスト（10bps + 5bps slippage）
      - 同一期間（2010-01-01 ～ 2024-12-31）
      - 同一データソース（統一キャッシュ）
      - 同一戦略（パイプライン準拠）

    tasks:
      - task_id: task_029_1
        description: |
          統一バックテスト規格書（Backtest Standard）の策定

          【目的】
          全バックテストが準拠すべき統一規格を定義。
          今後のバックテストは全てこの規格に従う。

          【成果物】
          docs/BACKTEST_STANDARD.md
          ```markdown
          # バックテスト統一規格 v1.0

          ## 1. 必須パラメータ

          ### 1.1 共通設定（変更不可）
          | パラメータ | 値 | 理由 |
          |-----------|-----|------|
          | initial_capital | 1,000,000 USD | 機関投資家規模 |
          | transaction_cost_bps | 10 | 市場平均 |
          | slippage_bps | 5 | 流動性考慮 |
          | currency | USD | 基準通貨 |

          ### 1.2 期間設定
          | パラメータ | 標準値 | 許容範囲 |
          |-----------|--------|----------|
          | start_date | 2010-01-01 | 2000-01-01以降 |
          | end_date | 2024-12-31 | start_date以降 |
          | min_history_days | 252 | 1年以上 |

          ### 1.3 リバランス頻度
          | 頻度 | 値 | 年間回数 |
          |------|-----|----------|
          | daily | 'daily' | ~252 |
          | weekly | 'weekly' | ~52 |
          | monthly | 'monthly' | 12 |

          ## 2. エンジン規格

          ### 2.1 使用エンジン
          - 本番バックテスト: BacktestEngineFactory.create(mode='auto')
          - 比較検証: 全エンジンで同一結果（Sharpe差異 < 0.01）

          ### 2.2 エンジン選択基準
          | 条件 | 選択エンジン |
          |------|-------------|
          | universe > 500 & period > 10y | vectorbt |
          | universe > 100 | fast |
          | その他 | standard |

          ## 3. ユニバース規格

          ### 3.1 標準ユニバース
          - ファイル: config/universe_standard.yaml
          - 銘柄数: 828（固定）
          - 更新頻度: 四半期

          ### 3.2 銘柄選定基準
          | 基準 | 値 |
          |------|-----|
          | min_market_cap | $1B |
          | min_daily_volume | $10M |
          | min_price | $5 |
          | max_sector_weight | 15% |

          ## 4. データ規格

          ### 4.1 データソース優先順位
          1. ローカルキャッシュ（cache/price_data/*.parquet）
          2. yfinance API（キャッシュミス時）

          ### 4.2 データ品質基準
          | 基準 | 閾値 |
          |------|------|
          | missing_rate | < 5% |
          | ohlc_inconsistency | < 1% |
          | price_spike | < 50% |

          ### 4.3 キャッシュ管理
          - 形式: Parquet
          - 更新: 日次（市場終了後）
          - 保持期間: 無期限

          ## 5. 戦略規格

          ### 5.1 シグナル生成
          - パイプライン準拠（src/orchestrator/pipeline.py）
          - 全シグナルを使用（モメンタム、ミーンリバージョン等）

          ### 5.2 配分計算
          - アルゴリズム: HRP + リスクパリティ
          - 最大単一銘柄: 20%
          - 最小現金: 5%

          ## 6. 結果規格

          ### 6.1 必須指標
          | 指標 | 計算方法 |
          |------|----------|
          | total_return | (final - initial) / initial |
          | annual_return | (1 + total_return)^(1/years) - 1 |
          | sharpe_ratio | mean(excess_return) / std(return) * sqrt(252) |
          | max_drawdown | min(cumulative_return / peak - 1) |
          | volatility | std(return) * sqrt(252) |
          | sortino_ratio | mean(excess_return) / std(downside_return) * sqrt(252) |
          | calmar_ratio | annual_return / abs(max_drawdown) |

          ### 6.2 出力形式
          - サマリー: YAML
          - 詳細: Parquet
          - レポート: Markdown + Excel

          ## 7. 検証規格

          ### 7.1 必須検証
          - [ ] 全銘柄のデータ取得成功
          - [ ] 取引コスト適用確認
          - [ ] リバランス回数の妥当性
          - [ ] ベンチマーク比較

          ### 7.2 品質ゲート
          | 項目 | 基準 |
          |------|------|
          | データカバレッジ | > 95% |
          | 計算エラー | 0件 |
          | 結果ファイル | 全て生成 |
          ```

        priority: critical

      - task_id: task_029_2
        description: |
          標準ユニバース定義（config/universe_standard.yaml）

          【目的】
          全バックテストで使用する標準ユニバースを定義。

          【実装内容】
          1. config/universe_standard.yaml 作成
             ```yaml
             # 統一バックテスト用標準ユニバース v1.0
             # 最終更新: 2026-01-29
             # 銘柄数: 828

             metadata:
               version: "1.0"
               last_updated: "2026-01-29"
               total_tickers: 828
               selection_criteria:
                 min_market_cap: 1_000_000_000  # $1B
                 min_daily_volume: 10_000_000   # $10M
                 min_price: 5.0
                 max_sector_weight: 0.15

             # 米国大型株（S&P 500構成銘柄）
             us_large_cap:
               - AAPL
               - MSFT
               - GOOGL
               - AMZN
               - META
               # ... (全銘柄リスト)

             # セクターETF
             sector_etfs:
               - XLK  # Technology
               - XLF  # Financials
               - XLV  # Healthcare
               # ...

             # 国際ETF
             international:
               - EFA  # EAFE
               - EEM  # Emerging Markets
               # ...

             # 債券ETF
             bonds:
               - TLT  # 長期国債
               - IEF  # 中期国債
               - AGG  # 総合債券
               # ...

             # コモディティ
             commodities:
               - GLD  # 金
               - SLV  # 銀
               - USO  # 原油
               # ...
             ```

          2. 既存のuniverse_full.yaml, universe_filtered.yamlとの整合性確認

          3. 銘柄リストの完全性検証

        priority: critical

      - task_id: task_029_3
        description: |
          統一データキャッシュの構築

          【目的】
          全828銘柄の15年データをキャッシュに格納。
          バックテスト実行時のデータ取得失敗を防止。

          【実装内容】
          1. scripts/build_price_cache.py 作成
             ```python
             """
             統一バックテスト用価格データキャッシュ構築スクリプト

             全828銘柄の2010-01-01～2024-12-31のデータを
             Parquet形式でキャッシュに保存する。
             """
             import yfinance as yf
             import polars as pl
             from pathlib import Path
             from tqdm import tqdm

             CACHE_DIR = Path("cache/price_data_standard")
             START_DATE = "2010-01-01"
             END_DATE = "2024-12-31"

             def build_cache(universe: list[str]):
                 CACHE_DIR.mkdir(parents=True, exist_ok=True)

                 failed = []
                 for ticker in tqdm(universe, desc="Building cache"):
                     cache_path = CACHE_DIR / f"{ticker}.parquet"
                     if cache_path.exists():
                         continue

                     try:
                         df = yf.download(ticker, start=START_DATE, end=END_DATE)
                         if len(df) < 252:  # 最低1年分
                             failed.append((ticker, "insufficient_data"))
                             continue

                         pl.from_pandas(df).write_parquet(cache_path)
                     except Exception as e:
                         failed.append((ticker, str(e)))

                 # 結果レポート
                 print(f"Success: {len(universe) - len(failed)}")
                 print(f"Failed: {len(failed)}")
                 return failed

             if __name__ == "__main__":
                 universe = load_universe("config/universe_standard.yaml")
                 failed = build_cache(universe)
                 save_failed_report(failed, "results/cache_build_report.yaml")
             ```

          2. キャッシュ構築実行（全828銘柄）

          3. 構築結果レポート作成
             - 成功銘柄数
             - 失敗銘柄リスト
             - データ品質サマリー

        priority: critical

      - task_id: task_029_4
        description: |
          統一バックテストランナー作成

          【目的】
          規格に準拠したバックテストを実行する統一スクリプト。

          【実装内容】
          1. scripts/run_standard_backtest.py 作成
             ```python
             """
             統一バックテストランナー v1.0

             Usage:
               python run_standard_backtest.py --frequency daily
               python run_standard_backtest.py --frequency weekly
               python run_standard_backtest.py --frequency monthly
               python run_standard_backtest.py --frequency all
             """
             import argparse
             from dataclasses import dataclass
             from datetime import datetime
             from pathlib import Path

             from src.backtest.factory import BacktestEngineFactory
             from src.backtest.base import UnifiedBacktestConfig, UnifiedBacktestResult

             # 統一規格パラメータ（変更不可）
             STANDARD_CONFIG = {
                 "initial_capital": 1_000_000.0,
                 "transaction_cost_bps": 10.0,
                 "slippage_bps": 5.0,
                 "start_date": datetime(2010, 1, 1),
                 "end_date": datetime(2024, 12, 31),
             }

             FREQUENCIES = ["daily", "weekly", "monthly"]

             @dataclass
             class BacktestRun:
                 frequency: str
                 config: UnifiedBacktestConfig
                 result: UnifiedBacktestResult = None

             def load_standard_universe() -> list[str]:
                 """標準ユニバースを読み込み"""
                 # config/universe_standard.yaml から読み込み
                 pass

             def load_cached_prices(universe: list[str]) -> dict:
                 """キャッシュから価格データを読み込み"""
                 # cache/price_data_standard/ から読み込み
                 pass

             def run_backtest(frequency: str) -> BacktestRun:
                 """統一規格でバックテスト実行"""
                 config = UnifiedBacktestConfig(
                     **STANDARD_CONFIG,
                     rebalance_frequency=frequency,
                 )

                 universe = load_standard_universe()
                 prices = load_cached_prices(universe)

                 # ファクトリでエンジン自動選択
                 engine = BacktestEngineFactory.create(mode='auto', config=config)

                 # 実行
                 result = engine.run(
                     universe=universe,
                     prices=prices,
                     config=config,
                 )

                 return BacktestRun(frequency=frequency, config=config, result=result)

             def save_results(run: BacktestRun):
                 """結果を規格準拠形式で保存"""
                 output_dir = Path("results/standard_backtest")
                 output_dir.mkdir(parents=True, exist_ok=True)

                 # サマリー（YAML）
                 summary_path = output_dir / f"{run.frequency}_summary.yaml"
                 save_summary_yaml(run, summary_path)

                 # 詳細（Parquet）
                 detail_path = output_dir / f"{run.frequency}_detail.parquet"
                 save_detail_parquet(run, detail_path)

                 # 検証レポート
                 verify_path = output_dir / f"{run.frequency}_verification.yaml"
                 save_verification(run, verify_path)

             def main():
                 parser = argparse.ArgumentParser()
                 parser.add_argument("--frequency", choices=FREQUENCIES + ["all"], required=True)
                 args = parser.parse_args()

                 frequencies = FREQUENCIES if args.frequency == "all" else [args.frequency]

                 for freq in frequencies:
                     print(f"Running {freq} backtest...")
                     run = run_backtest(freq)
                     save_results(run)
                     print(f"  Sharpe: {run.result.sharpe_ratio:.3f}")
                     print(f"  Annual Return: {run.result.annual_return:.2%}")
                     print(f"  Max DD: {run.result.max_drawdown:.2%}")

             if __name__ == "__main__":
                 main()
             ```

          2. 検証機能の実装
             - データカバレッジチェック
             - 取引コスト適用確認
             - リバランス回数検証

        priority: critical

      - task_id: task_029_5
        description: |
          統一バックテスト実行（日次/週次/月次）

          【実行内容】
          1. 統一ランナーで3頻度のバックテスト実行
             ```bash
             python scripts/run_standard_backtest.py --frequency all
             ```

          2. 実行条件の確認
             | 項目 | 値 |
             |------|-----|
             | エンジン | VectorBT（自動選択） |
             | 銘柄数 | 828（標準ユニバース） |
             | 初期資本 | $1,000,000 |
             | 取引コスト | 10bps + 5bps |
             | 期間 | 2010-01-01 ～ 2024-12-31 |

          3. 結果ファイル確認
             - results/standard_backtest/daily_summary.yaml
             - results/standard_backtest/weekly_summary.yaml
             - results/standard_backtest/monthly_summary.yaml

        priority: critical

      - task_id: task_029_6
        description: |
          ベンチマーク計算（統一規格準拠）

          【実行内容】
          1. scripts/calculate_standard_benchmarks.py 作成
             ```python
             """
             統一規格準拠ベンチマーク計算
             """
             BENCHMARKS = {
                 'SPY': {'type': 'buy_and_hold', 'ticker': 'SPY'},
                 'QQQ': {'type': 'buy_and_hold', 'ticker': 'QQQ'},
                 '60/40': {'type': 'balanced', 'equity': 'SPY', 'bond': 'TLT', 'ratio': 0.6},
                 'AGG': {'type': 'buy_and_hold', 'ticker': 'AGG'},
             }

             def calculate_benchmark(name: str, config: dict) -> BenchmarkResult:
                 # 統一規格の期間・初期資本を使用
                 pass
             ```

          2. 同一期間・同一初期資本で計算

          3. 結果を results/standard_backtest/benchmarks.yaml に保存

        priority: high

      - task_id: task_029_7
        description: |
          統一規格準拠の比較分析レポート

          【実行内容】
          1. 頻度別比較表
             ```markdown
             ## 統一規格バックテスト結果（2010-2024, 828銘柄, $1M）

             | 指標 | 日次 | 週次 | 月次 | SPY | 60/40 |
             |------|------|------|------|-----|-------|
             | 年率リターン | X% | Y% | Z% | A% | B% |
             | Sharpe Ratio | X | Y | Z | A | B |
             | 最大DD | X% | Y% | Z% | A% | B% |
             | ボラティリティ | X% | Y% | Z% | A% | B% |
             | Sortino Ratio | X | Y | Z | A | B |
             | Calmar Ratio | X | Y | Z | A | B |
             | 取引回数 | X | Y | Z | - | - |
             | 推定コスト | X% | Y% | Z% | - | - |
             ```

          2. 取引コスト影響分析
             - グロスリターン vs ネットリターン
             - 頻度別コスト比較

          3. 最適頻度の特定
             - コスト調整後Sharpe
             - リスク調整後リターン

          4. 結果を results/standard_backtest/comparison_report.md に保存

        priority: high

      - task_id: task_029_8
        description: |
          統一規格のCI/検証組み込み

          【目的】
          今後のバックテストが統一規格に準拠していることを自動検証。

          【実装内容】
          1. tests/test_backtest_standard_compliance.py 作成
             ```python
             """
             バックテスト統一規格準拠テスト
             """
             import pytest
             from src.backtest.standard import STANDARD_CONFIG, validate_config

             class TestBacktestStandardCompliance:
                 def test_initial_capital(self, backtest_config):
                     assert backtest_config.initial_capital == 1_000_000.0

                 def test_transaction_cost(self, backtest_config):
                     assert backtest_config.transaction_cost_bps == 10.0

                 def test_slippage(self, backtest_config):
                     assert backtest_config.slippage_bps == 5.0

                 def test_universe_coverage(self, backtest_result):
                     assert backtest_result.universe_coverage >= 0.95

                 def test_data_quality(self, backtest_result):
                     assert backtest_result.data_errors == 0
             ```

          2. pre-commit hookでバックテストスクリプト作成時に規格チェック

          3. CI/CDパイプラインに組み込み

        priority: high

      - task_id: task_029_9
        description: |
          不要コード削除 Phase 1（即座に削除可能）

          【目的】
          参照のない不要ファイルを削除し、コードベースを整理。
          今回の「実行条件不統一」問題の根本原因の一部を解消。

          【削除対象】
          1. scripts/test_speedup.py
             - 理由: 参照なし、テスト用一時スクリプト
             - 影響: なし

          2. scripts/test_final_speedup.py
             - 理由: test_speedup.pyと重複
             - 影響: なし

          3. scripts/check_file_size.py
             - 理由: デバッグ用、参照なし
             - 影響: なし

          4. config/universe.yaml（30銘柄版）
             - 理由: universe_full.yamlで置換済み
             - 影響: なし（サンプル用のみ）

          【実行手順】
          1. 削除前に参照チェック
             ```bash
             grep -r "test_speedup" scripts/ src/
             grep -r "check_file_size" scripts/ src/
             grep -r "universe.yaml" scripts/ src/ --include="*.py"
             ```

          2. バックアップ作成
             ```bash
             mkdir -p backup/deleted_$(date +%Y%m%d)
             cp scripts/test_speedup.py backup/deleted_$(date +%Y%m%d)/
             # ...
             ```

          3. 削除実行
             ```bash
             rm scripts/test_speedup.py
             rm scripts/test_final_speedup.py
             rm scripts/check_file_size.py
             rm config/universe.yaml
             ```

          4. テスト実行して影響なしを確認

        priority: high

      - task_id: task_029_10
        description: |
          不要コード削除 Phase 2（重複スクリプト統合後に削除）

          【前提】task_029_4（統一ランナー作成）完了後

          【削除対象】
          1. scripts/run_daily_backtest_15y.py
             - 理由: run_standard_backtest.pyで置換
             - 影響: パイプライン参照があれば修正必要

          2. scripts/run_full_backtest_monthly.py
             - 理由: run_standard_backtest.pyで置換
             - 影響: 同上

          3. scripts/run_full_backtest_weekly.py
             - 理由: run_standard_backtest.pyで置換
             - 影響: 同上

          4. scripts/run_15year_backtest.py
             - 理由: run_standard_backtest.pyで置換
             - 影響: 同上

          5. config/universe_full.yaml
             - 理由: universe_standard.yamlで置換
             - 影響: 参照スクリプトを修正

          6. config/universe_filtered.yaml
             - 理由: universe_standard.yamlで置換
             - 影響: 参照スクリプトを修正

          【実行手順】
          1. 参照チェック
             ```bash
             grep -r "run_daily_backtest" .
             grep -r "run_full_backtest" .
             grep -r "universe_full" .
             grep -r "universe_filtered" .
             ```

          2. 参照箇所を統一ランナー/標準ユニバースに修正

          3. バックアップ作成

          4. 削除実行

          5. CI/テスト実行して影響なしを確認

        priority: high

      - task_id: task_029_11
        description: |
          不要コード削除 Phase 3（エンジン層統廃合）

          【注意】大規模リファクタリング。慎重に実施。

          【削除候補】
          1. src/backtest/engine.py（1937行）
             - 理由: fast_engine.pyで置換可能
             - 前提: BacktestConfigを使用する箇所を全て移行
             - リスク: 高（広範な参照）

          2. src/backtest/numba_compute.py
          3. src/backtest/gpu_compute.py
          4. src/backtest/vectorized_compute.py
             - 理由: compute_backend.pyに統合
             - 前提: 統合モジュール作成完了
             - リスク: 中（計算結果の回帰テスト必須）

          5. src/signals/regime_detector_v2.py
          6. src/signals/sector_rotation_v2.py
             - 理由: v1版で機能カバー
             - 前提: v1版の存在確認
             - リスク: 低

          【実行手順】
          1. 依存関係の完全マッピング
             ```bash
             # 各ファイルの参照を調査
             grep -r "from.*engine import" src/
             grep -r "from.*numba_compute import" src/
             ```

          2. 回帰テスト作成
             - 削除前後で同一結果になることを確認するテスト

          3. 段階的に削除
             - 1ファイルずつ削除→テスト→コミット

          4. 削除後の全体テスト

          【成功基準】
          - 全テスト通過
          - バックテスト結果に変化なし
          - パフォーマンス劣化なし

        priority: medium

      - task_id: task_029_12
        description: |
          最終検証＆レポート

          【検証内容】
          1. 統一規格準拠確認
             - 全バックテストが同一条件で実行されたか
             - 結果ファイルが規格準拠形式か

          2. 結果の妥当性検証
             - Sharpe Ratioの範囲
             - ドローダウンの範囲
             - ベンチマークとの比較

          3. 不要コード削除の確認
             - 削除対象が全て削除されたか
             - 参照エラーがないか
             - テスト全通過

          4. 改善提案の策定
             - 目標未達指標の分析
             - 改善案の優先順位付け

          【成功基準】
          | 項目 | 基準 |
          |------|------|
          | 銘柄カバレッジ | 全頻度で同一（828銘柄） |
          | 設定一致 | 初期資本・コスト・期間が同一 |
          | 結果形式 | 規格準拠 |
          | 削除ファイル数 | Phase1: 4件, Phase2: 6件 |
          | テスト | 全通過 |

          【出力】
          - results/standard_backtest/final_report.md
          - results/standard_backtest/final_report.xlsx
          - results/cleanup_report.md（削除レポート）
          - dashboard.md更新

        priority: critical

    execution_strategy: |
      【Phase 1: 規格策定＆即時削除】並列実行
      - task_029_1 (規格書作成) → 足軽1
      - task_029_2 (標準ユニバース) → 足軽2
      - task_029_9 (不要コード削除Phase1) → 足軽3

      【Phase 2: インフラ構築】Phase 1完了後
      - task_029_3 (データキャッシュ構築) → 足軽3,4,5（並列）
      - task_029_4 (統一ランナー作成) → 足軽6

      【Phase 3: 統合＆削除】Phase 2完了後
      - task_029_10 (不要コード削除Phase2) → 足軽7
      - task_029_5 (バックテスト実行) → 足軽1,2,3（日次/週次/月次）
      - task_029_6 (ベンチマーク計算) → 足軽4

      【Phase 4: 分析＆検証】Phase 3完了後
      - task_029_7 (比較分析レポート) → 足軽5
      - task_029_8 (CI/検証組み込み) → 足軽6

      【Phase 5: 最終整理】Phase 4完了後
      - task_029_11 (不要コード削除Phase3) → 足軽7,8
      - task_029_12 (最終検証) → 足軽1

    expected_outcome: |
      【成果物】
      1. docs/BACKTEST_STANDARD.md - 統一規格書
      2. config/universe_standard.yaml - 標準ユニバース（828銘柄）
      3. cache/price_data_standard/ - 全銘柄キャッシュ
      4. scripts/run_standard_backtest.py - 統一ランナー
      5. results/standard_backtest/ - 統一規格準拠結果
      6. tests/test_backtest_standard_compliance.py - 規格準拠テスト
      7. results/cleanup_report.md - 削除レポート

      【統一規格の効果】
      | 問題 | 対策 | 効果 |
      |------|------|------|
      | エンジン不統一 | ファクトリ経由で自動選択 | 同一エンジン |
      | 銘柄数不一致 | 標準ユニバース | 828銘柄固定 |
      | 初期資本不一致 | 規格で$1M固定 | 比較可能 |
      | データ取得失敗 | 事前キャッシュ構築 | 100%カバレッジ |
      | 設定散在 | 統一ランナー | 一元管理 |

      【不要コード削除の効果】
      | Phase | 削除数 | 効果 |
      |-------|--------|------|
      | Phase1 | 4件 | 即座に混乱解消 |
      | Phase2 | 6件 | 重複スクリプト統合 |
      | Phase3 | 6件+ | エンジン層整理 |

      【再発防止】
      - バックテスト実行時は必ず統一ランナーを使用
      - CI/CDで規格準拠を自動検証
      - 新規スクリプト作成時はチェックリスト確認
      - 不要コードは即座に削除する文化

      【期待される結果】
      統一条件下での正確な比較:
      - 日次/週次/月次の真のパフォーマンス差
      - リバランス頻度の最適解
      - 取引コストの真の影響
      - クリーンなコードベース

  - id: cmd_030
    timestamp: "2026-01-29T21:00:00"
    command: "Tier5スキル設計書を作成せよ"
    project: auto_allocation_system
    priority: high
    status: pending
    context: |
      殿よりスキル化候補の判断が下された。
      21件の候補から9件がTier5として新規採用された。
      設計書を作成し、skills/designs/tier5/に格納せよ。

      【採用スキル（9件）】
      | スキル名 | 提案元 | 説明 |
      |----------|--------|------|
      | numba-jit-accelerate | task_023_1 | 金融計算向けNumba JIT関数群（5-10x高速化） |
      | universe-optimizer | task_025_1 | ユニバースサイズ最適化（流動性/品質/セクター分散） |
      | regime-detector | task_025_6 | 市場レジーム検出（HMM/ボラ/トレンド アンサンブル） |
      | economic-cycle-sector-rotation | task_025_7 | 景気サイクル連動セクターローテーション |
      | streaming-stats | task_023_5 | 大規模データストリーミング統計（メモリ38%削減） |
      | duckdb-data-layer | task_023_3 | DuckDB統合データレイヤー（SQL+Parquet統合） |
      | dividend-calculator | task_025_6 | 配当込みリターン計算（税務/分割対応） |
      | backtest-engine-base | task_026_1 | バックテストエンジン共通インターフェース |
      | architecture-test | task_027_6 | 設計ルール違反自動検出（ASTベース） |

      【設計書フォーマット】
      既存のtier1-4設計書と同一形式で作成:
      - name, description, version
      - inputs, outputs
      - parameters
      - dependencies
      - implementation_notes
      - test_cases
      - estimated_effort

    tasks:
      - task_id: task_030_1
        description: |
          numba-jit-accelerate スキル設計書を作成せよ

          【スキル概要】
          金融計算向けNumba JIT関数群。Sharpe/MDD/Volatility等の計算を5-10x高速化。

          【主要機能】
          1. @jit最適化されたSharpe Ratio計算
          2. @jit最適化されたMax Drawdown計算
          3. @jit最適化されたVolatility計算（ローリング）
          4. @jit最適化されたSortino Ratio計算
          5. @jit最適化されたリターン計算

          【出力】
          skills/designs/tier5/numba-jit-accelerate.yaml

        priority: high

      - task_id: task_030_2
        description: |
          universe-optimizer スキル設計書を作成せよ

          【スキル概要】
          ユニバースサイズ最適化。流動性/品質/セクター分散フィルターを提供。

          【主要機能】
          1. 流動性フィルター（出来高、ビッドアスクスプレッド）
          2. 品質フィルター（データ欠損率、OHLC整合性）
          3. セクター分散フィルター（最大/最小比率制約）
          4. 時価総額フィルター
          5. 銘柄数最適化（Sharpe vs 分散のトレードオフ）

          【出力】
          skills/designs/tier5/universe-optimizer.yaml

        priority: high

      - task_id: task_030_3
        description: |
          regime-detector スキル設計書を作成せよ

          【スキル概要】
          市場レジーム検出。HMM/ボラ/トレンドのアンサンブル検出。

          【主要機能】
          1. HMMベースレジーム検出（2-4状態）
          2. ボラティリティレジーム（低/中/高/極高）
          3. トレンドレジーム（上昇/横ばい/下降）
          4. クロスアセットレジーム（株式/債券/コモディティ相関）
          5. レジーム遷移確率計算

          【出力】
          skills/designs/tier5/regime-detector.yaml

        priority: high

      - task_id: task_030_4
        description: |
          economic-cycle-sector-rotation スキル設計書を作成せよ

          【スキル概要】
          景気サイクル連動セクターローテーション。

          【主要機能】
          1. 景気サイクル判定（拡張/ピーク/後退/底）
          2. セクター別期待リターン計算
          3. 最適セクターウェイト算出
          4. 11セクターETFマッピング
          5. マクロ指標連動調整

          【出力】
          skills/designs/tier5/economic-cycle-sector-rotation.yaml

        priority: high

      - task_id: task_030_5
        description: |
          streaming-stats スキル設計書を作成せよ

          【スキル概要】
          大規模データストリーミング統計。Welfordアルゴリズム等でメモリ38%削減。

          【主要機能】
          1. Welfordオンライン平均/分散
          2. ストリーミング共分散
          3. ストリーミング相関
          4. インクリメンタルPCA
          5. メモリ効率的なローリング統計

          【出力】
          skills/designs/tier5/streaming-stats.yaml

        priority: high

      - task_id: task_030_6
        description: |
          duckdb-data-layer スキル設計書を作成せよ

          【スキル概要】
          DuckDB統合データレイヤー。SQL+Parquet/CSV統合。

          【主要機能】
          1. Parquet/CSV透過的読み込み
          2. SQL風クエリインターフェース
          3. インメモリ/ファイル切り替え
          4. 効率的な時系列結合
          5. キャッシュ管理

          【出力】
          skills/designs/tier5/duckdb-data-layer.yaml

        priority: high

      - task_id: task_030_7
        description: |
          dividend-calculator スキル設計書を作成せよ

          【スキル概要】
          配当込みリターン計算。税務/分割対応。

          【主要機能】
          1. 配当込みトータルリターン計算
          2. 配当再投資シミュレーション
          3. 税務計算（源泉徴収、外国税額控除）
          4. 株式分割/併合調整
          5. 特別配当処理

          【出力】
          skills/designs/tier5/dividend-calculator.yaml

        priority: high

      - task_id: task_030_8
        description: |
          backtest-engine-base スキル設計書を作成せよ

          【スキル概要】
          バックテストエンジン共通インターフェース。

          【主要機能】
          1. BacktestEngineBase抽象クラス
          2. 統一BacktestConfig
          3. 統一BacktestResult
          4. エンジンファクトリ
          5. 準拠テストスイート

          【出力】
          skills/designs/tier5/backtest-engine-base.yaml

        priority: high

      - task_id: task_030_9
        description: |
          architecture-test スキル設計書を作成せよ

          【スキル概要】
          設計ルール違反自動検出。ASTベース。

          【主要機能】
          1. 重複クラス定義検出
          2. ファイルサイズ超過検出
          3. 循環依存検出
          4. 命名規則違反検出
          5. レイヤー違反検出（依存方向）

          【出力】
          skills/designs/tier5/architecture-test.yaml

        priority: high

    execution_strategy: |
      【Phase 1】並列実行（3足軽）
      - task_030_1, task_030_2, task_030_3 → 足軽1,2,3

      【Phase 2】並列実行（3足軽）
      - task_030_4, task_030_5, task_030_6 → 足軽4,5,6

      【Phase 3】並列実行（3足軽）
      - task_030_7, task_030_8, task_030_9 → 足軽7,8,1

    expected_outcome: |
      【成果物】
      skills/designs/tier5/
      ├── numba-jit-accelerate.yaml
      ├── universe-optimizer.yaml
      ├── regime-detector.yaml
      ├── economic-cycle-sector-rotation.yaml
      ├── streaming-stats.yaml
      ├── duckdb-data-layer.yaml
      ├── dividend-calculator.yaml
      ├── backtest-engine-base.yaml
      └── architecture-test.yaml

      【スキル総数】
      Tier1: 3件（設計完了）
      Tier2: 3件（設計完了）
      Tier3: 4件（設計完了）
      Tier4: 2件（設計完了）
      Tier5: 9件（本cmd完了後）
      合計: 21件

  - id: cmd_031
    timestamp: "2026-01-30T00:15:00"
    command: "バックテストエンジン統一インターフェース完全修正"
    project: auto_allocation_system
    priority: critical
    status: pending
    context: |
      【重大問題発覚】
      足軽5号がtask_029_7で報告した問題を将軍が確認。
      バックテストエンジンの統一インターフェースが形式的に定義されているだけで、
      実際には各エンジンが準拠していない。

      【発見された問題（19件）】

      ■ Critical（5件）
      E1: weights_funcが全エンジンで無視される
      E2: rebalance_frequencyがVectorBTConfigに伝搬されない
      E3: weights_funcのシグネチャがbase.pyとfast_engine.pyで不一致
      E4: run_unified()がrun()にパラメータを渡さない
      E5: 月初め判定（実装）vs月末判定（コメント）の不一致

      ■ High（3件）
      E6: n_trades, n_rebalancesが常に0
      E7: transaction_cost計算方法がエンジンごとに異なる
      E8: rebalance_frequencyの正規化がない

      ■ 設定ファイル問題（4件）
      C1: base_currency二重定義（JPY vs USD）
      C2: train_period_days不一致（252 vs 504）
      C3: penalty_* vs *_lambdaフィールド名不一致
      C4: universe型不一致（list vs dict）

      【影響】
      - 日次・月次バックテストが実質Buy-and-hold
      - リバランス頻度指定が機能しない
      - 統計値（n_rebalances等）が不正確
      - 設定が正しく読み込まれない

    tasks:
      - task_id: task_031_1
        description: |
          【Critical】VectorBTStyleEngine.run()でweights_funcを実装せよ

          【現状】
          vectorbt_engine.py:454-459
          ```python
          result = self.run_vectorized(
              prices=prices_matrix,
              signals=None,  # ← 常にNone（均等配分）
              ...
          )
          ```

          【修正内容】
          1. weights_funcからsignalsを生成するメソッド追加
             ```python
             def _generate_signals_from_weights_func(
                 self,
                 weights_func: Callable,
                 universe: List[str],
                 prices: Dict[str, pd.DataFrame],
                 dates: List[datetime],
                 rebalance_mask: np.ndarray,
             ) -> np.ndarray:
             ```

          2. run()メソッドでweights_funcを使用
             ```python
             if weights_func is not None:
                 signals = self._generate_signals_from_weights_func(...)
             else:
                 signals = None  # 均等配分
             ```

          3. リバランス日のみweights_funcを呼び出し、
             それ以外の日は前回のウェイトを維持

          【テスト】
          - weights_func指定時にn_rebalances > 0
          - リバランス日にのみウェイト変更
          - 日次/週次/月次で異なる結果

        priority: critical

      - task_id: task_031_2
        description: |
          【Critical】UnifiedBacktestConfig→VectorBTConfigの伝搬修正

          【現状】
          VectorBTStyleEngine.__init__()で:
          - self.vbt_config = VectorBTConfig() ← デフォルト値
          - unified_configのrebalance_frequencyが無視される

          【修正内容】
          1. __init__()でunified_configからVectorBTConfigを構築
             ```python
             def __init__(self, config=None, unified_config=None):
                 if unified_config:
                     self.vbt_config = VectorBTConfig(
                         initial_capital=unified_config.initial_capital,
                         transaction_cost_bps=unified_config.transaction_cost_bps,
                         slippage_bps=unified_config.slippage_bps,
                         rebalance_frequency=unified_config.rebalance_frequency,
                         ...
                     )
             ```

          2. run()メソッドでconfigパラメータも反映
             ```python
             def run(self, ..., config=None, ...):
                 if config:
                     self._update_config_from_unified(config)
             ```

          【テスト】
          - rebalance_frequency="daily"で毎日リバランス
          - rebalance_frequency="monthly"で月1回リバランス

        priority: critical

      - task_id: task_031_3
        description: |
          【Critical】weights_funcシグネチャの統一

          【現状の不一致】
          - BacktestEngineBase（base.py）:
            weights_func(universe, prices, date, current_weights) -> Dict[str, float]

          - FastBacktestEngine内部:
            weights_func(signals, cov_matrix) -> weights

          【修正内容】
          1. WeightsFuncProtocolを明確化（base.py）
             ```python
             class WeightsFuncProtocol(Protocol):
                 def __call__(
                     self,
                     universe: List[str],
                     prices: Dict[str, pd.DataFrame],
                     date: datetime,
                     current_weights: Dict[str, float],
                 ) -> Dict[str, float]: ...
             ```

          2. FastBacktestEngine.run_unified()でアダプター実装
             ```python
             def _adapt_weights_func(self, external_func, universe, prices):
                 def internal_func(signals, cov_matrix):
                     # external_funcを呼び出してウェイトを取得
                     # signalsとcov_matrixから必要な情報を抽出
                     ...
                 return internal_func
             ```

          3. 全エンジンで同一シグネチャを使用

          【テスト】
          - 同一weights_funcを全エンジンで使用可能
          - TypeErrorが発生しない

        priority: critical

      - task_id: task_031_4
        description: |
          【Critical】FastBacktestEngine.run_unified()→run()パラメータ伝搬

          【現状】
          fast_engine.py:759-771
          ```python
          internal_weights_func = adapted_weights_func
          result = self.run(price_df, weights_func=internal_weights_func)
          # ↑ しかしrun()のシグネチャにweights_funcがない
          ```

          【修正内容】
          1. FastBacktestEngine.run()にweights_funcパラメータ追加
             ```python
             def run(
                 self,
                 prices: pd.DataFrame,
                 weights_func: Optional[Callable] = None,
                 signals_func: Optional[Callable] = None,
                 ...
             ) -> BacktestResult:
             ```

          2. run()内でweights_funcを使用したリバランスロジック実装

          3. run_unified()からrun()へ正しくパラメータを渡す

          【テスト】
          - run_unified()経由でweights_funcが機能
          - リバランスが実行される

        priority: critical

      - task_id: task_031_5
        description: |
          【Critical】リバランス日判定ロジックの統一（月末）

          【現状】
          vectorbt_engine.py:801-809
          ```python
          elif freq == "monthly":
              # コメント: Last trading day of each month
              # 実装: 月初め判定
              if dt.month != prev_month:
                  mask[i] = True
                  prev_month = dt.month
          ```

          【修正内容】
          1. 月末判定ロジックに修正
             ```python
             elif freq == "monthly":
                 # 次の日付が異なる月、または最終日の場合
                 if i == n_days - 1 or dates[i+1].month != dt.month:
                     mask[i] = True
             ```

          2. 全エンジンで同一の判定ロジックを使用
             - base.pyにユーティリティ関数として定義
             ```python
             def create_rebalance_dates(
                 dates: List[datetime],
                 frequency: str,
             ) -> List[datetime]:
             ```

          【テスト】
          - monthly: 各月の最終取引日にリバランス
          - weekly: 各週の最終取引日にリバランス

        priority: critical

      - task_id: task_031_6
        description: |
          【High】n_rebalances, n_trades計算ロジック修正

          【現状】
          - VectorBTResult: n_trades=0が常に固定
          - UnifiedBacktestResult: rebalances/trades Listから計算するが設定されない

          【修正内容】
          1. VectorBTStyleEngineでリバランス回数をカウント
             ```python
             # ウェイト変化を検出
             weight_changes = np.diff(weights, axis=0)
             rebalance_indices = np.where(np.any(np.abs(weight_changes) > 0.001, axis=1))[0]
             n_rebalances = len(rebalance_indices)
             ```

          2. 取引回数の計算
             ```python
             # 閾値以上のウェイト変更を取引とカウント
             n_trades = np.sum(np.abs(weight_changes) > 0.01)
             ```

          3. UnifiedBacktestResultに正しく設定
             ```python
             return UnifiedBacktestResult(
                 ...
                 engine_specific_results={
                     "n_rebalances": n_rebalances,
                     "n_trades": n_trades,
                 },
             )
             ```

          【テスト】
          - daily: n_rebalances ≈ 取引日数
          - monthly: n_rebalances ≈ 月数（180回/15年）
          - n_trades > 0

        priority: high

      - task_id: task_031_7
        description: |
          【High】設定ファイル整合性修正

          【修正対象】
          C1: base_currency統一
          - SystemConfig.base_currency削除
          - DataConfig.base_currencyのみ使用
          - デフォルト値を"USD"に統一

          C2: train_period_days統一
          - settings.pyのデフォルト値をdefault.yamlに合わせる
          - train_period_days: 504, test_period_days: 126

          C3: フィールド名統一
          - default.yamlを修正: turnover_lambda → penalty_turnover等

          C4: universe型統一
          - Settings.universeをUniverseConfig型に変更
          - または default.yamlをlist型に変更

          【テスト】
          - 設定読み込みテスト
          - 値の一致確認

        priority: high

      - task_id: task_031_8
        description: |
          【High】rebalance_frequency正規化

          【修正内容】
          1. UnifiedBacktestConfig.__post_init__()で正規化
             ```python
             def __post_init__(self):
                 # 既存の検証...

                 # rebalance_frequency正規化
                 self.rebalance_frequency = self.rebalance_frequency.lower()
                 if self.rebalance_frequency not in ["daily", "weekly", "monthly"]:
                     raise ValueError(f"Invalid rebalance_frequency: {self.rebalance_frequency}")
             ```

          2. RebalanceFrequency Enumを実際に使用
             ```python
             from enum import Enum

             class RebalanceFrequency(Enum):
                 DAILY = "daily"
                 WEEKLY = "weekly"
                 MONTHLY = "monthly"
             ```

          【テスト】
          - "Monthly", "MONTHLY", "monthly"全てが動作
          - 不正な値でValueError

        priority: high

      - task_id: task_031_9
        description: |
          【Medium】統合テスト追加

          【テストケース】
          1. test_weights_func_is_called
             - weights_funcが実際に呼び出されることを確認
             - モック関数で呼び出し回数をカウント

          2. test_rebalance_frequency_daily
             - daily設定でn_rebalances ≈ 取引日数

          3. test_rebalance_frequency_monthly
             - monthly設定でn_rebalances ≈ 月数

          4. test_different_frequencies_different_results
             - daily/weekly/monthlyで異なる結果

          5. test_n_trades_positive
             - リバランス時にn_trades > 0

          6. test_config_propagation
             - UnifiedBacktestConfigの全パラメータが伝搬

          【出力】
          tests/integration/test_backtest_engine_unified.py

        priority: medium

      - task_id: task_031_10
        description: |
          【Medium】全頻度バックテスト再実行・比較

          【実行内容】
          1. 修正後のrun_standard_backtest.pyで再実行
             - python scripts/run_standard_backtest.py --all

          2. 結果の検証
             - n_rebalances > 0
             - 日次/週次/月次で異なる結果
             - 統計値が妥当な範囲

          3. 比較レポート作成
             - results/std_comparison_v2.md

          【成功基準】
          | 頻度 | n_rebalances | 期待値 |
          |------|--------------|--------|
          | daily | ~3900 | 取引日数 |
          | weekly | ~780 | 週数 |
          | monthly | ~180 | 月数 |

        priority: medium

      - task_id: task_031_11
        description: |
          【Low】報告フロー改善

          【修正内容】
          1. 足軽報告テンプレートにseverityフィールド追加
             ```yaml
             severity: critical  # critical/high/medium/low
             ```

          2. instructions/karo.mdに上申ルール追加
             - severity: criticalの報告は即座に将軍に上申
             - dashboard.mdの「要対応」に自動追加

          3. instructions/ashigaru.mdに報告ルール追加
             - Critical問題発見時はseverity: criticalを必ず設定

          【テスト】
          - 報告テンプレートの確認

        priority: low

    execution_strategy: |
      【Phase 1: Critical修正】並列実行
      - task_031_1 (weights_func実装) → 足軽1
      - task_031_2 (Config伝搬) → 足軽2
      - task_031_3 (シグネチャ統一) → 足軽3
      - task_031_4 (run_unified修正) → 足軽4
      - task_031_5 (月末判定) → 足軽5

      【Phase 2: High修正】Phase 1完了後
      - task_031_6 (統計量計算) → 足軽1
      - task_031_7 (設定整合性) → 足軽2
      - task_031_8 (frequency正規化) → 足軽3

      【Phase 3: 検証】Phase 2完了後
      - task_031_9 (統合テスト) → 足軽4
      - task_031_10 (再実行・比較) → 足軽5
      - task_031_11 (報告フロー) → 足軽6

    expected_outcome: |
      【成果物】
      1. 修正済みバックテストエンジン（5ファイル）
         - src/backtest/vectorbt_engine.py
         - src/backtest/fast_engine.py
         - src/backtest/ray_engine.py
         - src/backtest/streaming_engine.py
         - src/backtest/base.py

      2. 修正済み設定ファイル
         - config/default.yaml
         - src/config/settings.py

      3. 統合テスト
         - tests/integration/test_backtest_engine_unified.py

      4. 再実行結果
         - results/std_daily_v2.json
         - results/std_weekly_v2.json
         - results/std_monthly_v2.json
         - results/std_comparison_v2.md

      【期待される効果】
      | 問題 | 修正後 |
      |------|--------|
      | weights_func無視 | 正しく使用される |
      | n_rebalances=0 | 実際のリバランス回数 |
      | 日次=月次 | 異なる結果 |
      | 設定不一致 | 統一された設定 |

      【検証基準】
      - daily: n_rebalances ≈ 3900
      - weekly: n_rebalances ≈ 780
      - monthly: n_rebalances ≈ 180
      - 各頻度で異なるSharpe/リターン

# =====================================================
# cmd_031 進捗更新指示 (2026-01-29 15:30)
# =====================================================
- instruction_id: instr_031_update_3
  timestamp: "2026-01-29T15:30:00"
  type: dashboard_update
  priority: high
  content: |
    【dashboard.md更新指示】
    
    cmd_031のPhase 2/3進捗を更新せよ:
    
    Phase 2:
    - task_031_6: 🔄作業中（n_rebalances計算）
    - task_031_7: ✅完了（設定整合性C1-C4修正）
    - task_031_8: ✅完了（frequency正規化、7テストPASS）
    
    Phase 3:
    - task_031_9: 🔄作業中（統合テスト）
    - task_031_10: 🔄作業中（全頻度再実行）
    - task_031_11: 🔄作業中（報告フロー改善）
    
    Tier5設計書の状態も確認し更新せよ（cmd_030完了分）

  - id: cmd_032
    timestamp: "2026-01-30T01:10:00"
    command: "プロジェクト品質改善 - 54件の問題に対応せよ"
    project: auto_allocation_system
    priority: high
    status: pending
    context: |
      【プロジェクト全体調査結果】
      コードベース品質、アーキテクチャ、運用の3観点から54件の改善点を発見。
      高優先13件、中優先24件、低優先17件。

      ■ 高優先問題（13件）
      【コード品質】
      Q1: 非推奨ファイル2つ残存（1,399行重複）
      Q2: 高複雑度関数CC>30（pipeline.py等）
      Q3: スクリプト重複（weekly/monthly 589行）
      Q4: 500行超ファイル99個（64%）
      Q5: テストカバレッジ不足（allocation/risk/strategy）

      【アーキテクチャ】
      A1: meta⇄signals循環依存
      A2: orchestrator全層依存
      A3: config重複定義（YAML+Pydantic）
      A4: 無差別Exception catch（46箇所）
      A5: utils→backtest逆依存

      【運用】
      P1: GitHub Actions未設定
      P2: キャッシュ自動削除なし（87MB肥大化）
      P3: メモリプロファイリング不足

    tasks:
      # ============================================================
      # Phase 1: 即時対応（高優先・小工数）
      # ============================================================
      - task_id: task_032_1
        description: |
          【Q1】非推奨ファイル統合・削除

          【対象】
          - src/analysis/dynamic_threshold.py (762行)
          - src/analysis/dynamic_thresholds.py (637行)
          → 合計1,399行の重複

          【作業内容】
          1. 両ファイルの機能を確認
          2. src/meta/dynamic_params.py に統合済みか確認
          3. 参照箇所を調査（grep "dynamic_threshold"）
          4. 参照を新モジュールに変更
          5. 非推奨ファイルを削除
          6. __init__.py からのエクスポート削除

          【成功基準】
          - 非推奨ファイル2つが削除される
          - 全テストがPASS
          - インポートエラーなし

        priority: high

      - task_id: task_032_2
        description: |
          【P1】GitHub Actions CI/CD構築

          【作業内容】
          1. .github/workflows/ci.yml 作成
             ```yaml
             name: CI
             on: [push, pull_request]
             jobs:
               test:
                 runs-on: ubuntu-latest
                 steps:
                   - uses: actions/checkout@v4
                   - uses: actions/setup-python@v5
                     with:
                       python-version: '3.11'
                   - run: pip install -e ".[dev]"
                   - run: make lint
                   - run: make test
               
               performance:
                 runs-on: ubuntu-latest
                 steps:
                   - run: pytest tests/ -m "slow" --timeout=300
             ```

          2. .github/workflows/release.yml 作成（オプション）

          【成功基準】
          - push時に自動テスト実行
          - lint/test/performanceジョブが定義

        priority: high

      - task_id: task_032_3
        description: |
          【P2】キャッシュ自動削除実装

          【現状】
          - cleanup_expired()メソッドは存在するが呼び出されていない
          - キャッシュが87MBに肥大化

          【作業内容】
          1. src/backtest/cache.py に定期クリーンアップ追加
             ```python
             def __init__(self, ...):
                 ...
                 self._last_cleanup = time.time()
                 self._cleanup_interval = 3600  # 1時間

             def get(self, key):
                 self._maybe_cleanup()
                 ...

             def _maybe_cleanup(self):
                 if time.time() - self._last_cleanup > self._cleanup_interval:
                     self.cleanup_expired()
                     self._last_cleanup = time.time()
             ```

          2. max_cache_size_mb パラメータ追加（デフォルト500MB）
          3. サイズ超過時の古いエントリ削除

          【成功基準】
          - キャッシュが自動的に管理される
          - 設定可能なサイズ上限

        priority: high

      - task_id: task_032_4
        description: |
          【Q3】スクリプト重複統合

          【対象】
          - scripts/run_full_backtest_weekly.py (382行)
          - scripts/run_full_backtest_monthly.py (207行)

          【作業内容】
          1. 共通ロジックを抽出
          2. scripts/run_full_backtest.py に統合
             ```python
             # 使用例
             python scripts/run_full_backtest.py --frequency weekly
             python scripts/run_full_backtest.py --frequency monthly
             python scripts/run_full_backtest.py --frequency daily
             ```

          3. 旧スクリプトを削除（または非推奨化）

          【成功基準】
          - 1つのスクリプトで全頻度対応
          - 旧スクリプト削除

        priority: high

      - task_id: task_032_5
        description: |
          【A4-1】Exception処理の具体化（Phase 1: 調査）

          【現状】
          - except Exception as e: が46箇所
          - 障害が隠蔽されるリスク

          【作業内容】
          1. 46箇所の例外処理を一覧化
          2. 各箇所で発生しうる具体的な例外を特定
          3. 優先度付け（バックテストエンジン優先）
          4. 修正計画を作成

          【出力】
          - results/exception_handling_audit.md
            - 箇所一覧
            - 推奨される具体的例外
            - 修正優先度

        priority: high

      # ============================================================
      # Phase 2: 構造改善（高優先・中工数）
      # ============================================================
      - task_id: task_032_6
        description: |
          【A4-2】Exception処理の具体化（Phase 2: 修正）

          【作業内容】
          task_032_5の調査結果に基づき、上位10箇所を修正

          【修正パターン】
          ```python
          # Before
          except Exception as e:
              logger.warning(f"Failed: {e}")

          # After
          except (ValueError, TypeError) as e:
              logger.warning(f"Invalid input: {e}")
          except IOError as e:
              logger.error(f"IO error: {e}")
              raise
          except Exception as e:
              logger.exception(f"Unexpected error: {e}")
              raise
          ```

          【成功基準】
          - 上位10箇所で具体的例外処理
          - 全テストPASS

        priority: high

      - task_id: task_032_7
        description: |
          【A3】config統一（YAML⇄Pydantic同期）

          【現状の重複】
          | 設定項目 | config/default.yaml | src/config/settings.py |
          |---------|---------------------|------------------------|
          | data_quality | L23-29 | DataQualityConfig |
          | rebalance | L104-109 | RebalanceConfig |
          | walk_forward | L155-161 | WalkForwardConfig |
          | cost_model | L191-213 | CostModelConfig |

          【作業内容】
          1. 設定の真のソースをYAMLに統一
          2. settings.pyはYAMLから読み込むのみ
          3. デフォルト値の重複を削除
          4. 同期検証テストを追加

          【成功基準】
          - 設定が1箇所で管理される
          - 同期検証テストがPASS

        priority: high

      - task_id: task_032_8
        description: |
          【A1】meta⇄signals循環依存解消

          【現状】
          hierarchical_ensemble.py (L707-744):
          ```python
          from src.signals.trend import (...)
          from src.signals.mean_reversion import (...)
          from src.signals.macro import (...)
          ```
          → meta層がsignals層に密結合

          【作業内容】
          1. SignalRegistryパターンの導入
             ```python
             # signals/registry.py
             class SignalRegistry:
                 _signals: Dict[str, Type[Signal]] = {}

                 @classmethod
                 def register(cls, name: str, signal_class: Type[Signal]):
                     cls._signals[name] = signal_class

                 @classmethod
                 def get(cls, name: str) -> Type[Signal]:
                     return cls._signals[name]
             ```

          2. hierarchical_ensemble.pyをRegistry経由に変更
          3. 直接インポートを削除

          【成功基準】
          - meta層がsignals層を直接インポートしない
          - 全テストPASS

        priority: high

      - task_id: task_032_9
        description: |
          【Q2】高複雑度関数分割

          【対象（CC>30）】
          | CC | ファイル | 関数 |
          |----|----------|------|
          | 36 | pipeline.py | _step_cmd017_integration |
          | 32 | risk_allocation.py | allocate |
          | 32 | pipeline.py | _apply_return_maximization |
          | 31 | base.py | create_rebalance_mask |

          【作業内容】
          1. 各関数を小さなヘルパー関数に分割
          2. 目標CC: 20以下
          3. 単体テスト追加

          【成功基準】
          - 対象関数のCC≤20
          - 全テストPASS

        priority: high

      - task_id: task_032_10
        description: |
          【Q5】テストカバレッジ拡充

          【現状不足】
          - src/allocation/ - 単体テストなし
          - src/risk/ - 単体テストなし
          - src/strategy/ - 単体テストなし

          【作業内容】
          1. tests/unit/test_allocation.py 作成
             - allocator.py のテスト
             - covariance.py のテスト
             - constraints.py のテスト

          2. tests/unit/test_risk.py 作成
             - risk_calculator.py のテスト
             - var_calculator.py のテスト

          3. tests/unit/test_strategy.py 作成
             - evaluator.py のテスト
             - gate_checker.py のテスト

          【成功基準】
          - 各モジュールに最低5テストケース
          - カバレッジ60%以上

        priority: high

      # ============================================================
      # Phase 3: アーキテクチャ整理（中優先）
      # ============================================================
      - task_id: task_032_11
        description: |
          【A5】utils→backtest逆依存解消

          【現状】
          utils/cache.py (L24):
          ```python
          from src.backtest.cache import SignalCache
          ```
          → 下位層(utils)が上位層(backtest)に依存

          【作業内容】
          1. 共通キャッシュインターフェースを定義
             ```python
             # utils/cache_interface.py
             class CacheInterface(Protocol):
                 def get(self, key: str) -> Any: ...
                 def set(self, key: str, value: Any) -> None: ...
             ```

          2. backtest/cache.pyがインターフェースを実装
          3. utils/cache.pyの逆依存を削除

          【成功基準】
          - utils層がbacktest層をインポートしない
          - 全テストPASS

        priority: medium

      - task_id: task_032_12
        description: |
          【A2】orchestrator責務分割

          【現状】
          orchestrator/が複数責務を持つ:
          - データ取得 (data_preparation + data/fetcher)
          - シグナル計算 (signal_generation + signals/)
          - リスク推定 (risk_allocation + allocation/)

          【作業内容】
          1. 各サブモジュールの責務を明確化
          2. 重複ロジックを下位層に移動
          3. orchestrator/pipeline.pyは調整のみに

          【成功基準】
          - 各モジュールの責務が単一
          - ドキュメント化

        priority: medium

      - task_id: task_032_13
        description: |
          【中優先】ログレベル統一ガイドライン

          【作業内容】
          1. docs/LOGGING_GUIDELINES.md 作成
             ```markdown
             # ログレベルガイドライン

             ## ERROR
             - システム継続不可能なエラー
             - 即座に対応が必要

             ## WARNING
             - 予期しない状態だが継続可能
             - フォールバック発動時

             ## INFO
             - 重要な処理の開始/完了
             - 設定変更

             ## DEBUG
             - 詳細なデバッグ情報
             - 本番では無効化
             ```

          2. 主要モジュールのログレベルを統一

          【成功基準】
          - ガイドライン作成
          - 上位10モジュールで適用

        priority: medium

      - task_id: task_032_14
        description: |
          【中優先】未使用モジュール調査・削除

          【潜在的未使用（27個）】
          - src/analysis/parameter_optimizer.py
          - src/backtest/covariance_cache.py
          - src/backtest/long_term_validation.py
          - src/backtest/numba_accelerate.py
          - src/backtest/numba_compute.py
          - 他22個

          【作業内容】
          1. 各モジュールの参照を調査
          2. 実際に未使用のものをリスト化
          3. 削除候補をレビュー
          4. 削除実行

          【成功基準】
          - 未使用モジュール特定
          - 確認後削除

        priority: medium

      - task_id: task_032_15
        description: |
          【中優先】Universe設定整理

          【現状】
          - universe.yaml（基本）
          - universe_standard.yaml（標準）
          - universe_filtered.yaml（フィルタ済み）
          - universe_optimized.yaml（最適化）
          - universe_full.yaml（フル）

          【作業内容】
          1. 各ファイルの用途を確認
          2. 不要なものを削除
          3. 残すものをドキュメント化
          4. デフォルト設定を明確化

          【成功基準】
          - 必要なファイルのみ残る
          - 用途がドキュメント化

        priority: medium

      - task_id: task_032_16
        description: |
          【中優先】キャッシュ実装統一

          【現状】
          - src/backtest/cache.py（バックテスト用）
          - src/data/cache.py（データ用）
          - src/utils/cache.py（汎用）

          【作業内容】
          1. 共通CacheManagerクラスを定義
          2. 各用途向けのラッパーを作成
          3. キャッシュポリシーを統一
          4. メモリ効率を改善

          【成功基準】
          - 1つのCache APIに統一
          - 全テストPASS

        priority: medium

      # ============================================================
      # Phase 4: 最適化・ドキュメント（低優先）
      # ============================================================
      - task_id: task_032_17
        description: |
          【P3】メモリプロファイリング強化

          【作業内容】
          1. psutilを常時使用化
          2. バックテスト各フェーズでメモリ記録
          3. メモリスパイク検出アラート

          【成功基準】
          - メモリ使用量がログに記録
          - 閾値超過時にWARNING

        priority: low

      - task_id: task_032_18
        description: |
          【低優先】README.md拡充

          【現状】26行のみ

          【作業内容】
          1. プロジェクト概要
          2. インストール手順
          3. 使用方法（CLI、API）
          4. アーキテクチャ図
          5. 設定ファイルの説明
          6. トラブルシューティング

          【成功基準】
          - README.md 200行以上
          - 主要な使用方法を網羅

        priority: low

      - task_id: task_032_19
        description: |
          【低優先】パフォーマンスベンチマーク自動化

          【作業内容】
          1. scripts/benchmark.py 作成
          2. CI/CDに組み込み（週次実行）
          3. 結果をresults/benchmark_history.jsonに記録

          【成功基準】
          - 自動ベンチマーク実行
          - 履歴記録

        priority: low

      - task_id: task_032_20
        description: |
          【低優先】500行超ファイルリファクタリング計画

          【対象】99ファイル（64%）

          【作業内容】
          1. 優先度付け（使用頻度×行数）
          2. 上位10ファイルのリファクタリング計画
          3. 段階的実行スケジュール

          【出力】
          - docs/REFACTORING_PLAN.md

        priority: low

    execution_strategy: |
      【Phase 1: 即時対応】並列実行（5タスク）
      - task_032_1 (非推奨削除) → 足軽1
      - task_032_2 (GitHub Actions) → 足軽2
      - task_032_3 (キャッシュ自動削除) → 足軽3
      - task_032_4 (スクリプト統合) → 足軽4
      - task_032_5 (Exception調査) → 足軽5

      【Phase 2: 構造改善】Phase 1完了後、並列実行（5タスク）
      - task_032_6 (Exception修正) → 足軽1
      - task_032_7 (config統一) → 足軽2
      - task_032_8 (循環依存解消) → 足軽3
      - task_032_9 (関数分割) → 足軽4
      - task_032_10 (テスト拡充) → 足軽5,6,7,8（並列）

      【Phase 3: アーキテクチャ整理】Phase 2完了後
      - task_032_11 (逆依存解消) → 足軽1
      - task_032_12 (orchestrator分割) → 足軽2
      - task_032_13 (ログガイドライン) → 足軽3
      - task_032_14 (未使用削除) → 足軽4
      - task_032_15 (Universe整理) → 足軽5
      - task_032_16 (キャッシュ統一) → 足軽6

      【Phase 4: 最適化】Phase 3完了後
      - task_032_17 (メモリ) → 足軽7
      - task_032_18 (README) → 足軽8
      - task_032_19 (ベンチマーク) → 足軽1
      - task_032_20 (リファクタ計画) → 足軽2

    expected_outcome: |
      【成果物】
      1. クリーンなコードベース
         - 非推奨ファイル削除
         - 未使用モジュール削除
         - 重複スクリプト統合

      2. CI/CD基盤
         - GitHub Actions設定
         - 自動テスト実行
         - パフォーマンスベンチマーク

      3. 堅牢なアーキテクチャ
         - 循環依存解消
         - レイヤー違反修正
         - 設定の一元化

      4. 品質改善
         - テストカバレッジ向上
         - 例外処理の具体化
         - ログレベル統一

      5. ドキュメント
         - README拡充
         - ログガイドライン
         - リファクタリング計画

      【期待効果】
      | 指標 | Before | After |
      |------|--------|-------|
      | 非推奨ファイル | 2個 | 0個 |
      | 重複スクリプト | 2個 | 0個 |
      | 無差別Exception | 46箇所 | 10箇所以下 |
      | テストカバレッジ | 部分的 | 60%以上 |
      | CI/CD | なし | あり |
      | キャッシュ管理 | 手動 | 自動 |

# =====================================================
# dashboard更新指示 (2026-01-29 15:35)
# =====================================================
- instruction_id: instr_dashboard_update_032
  timestamp: "2026-01-29T15:35:00"
  type: dashboard_update
  priority: high
  content: |
    【dashboard.md緊急更新指示】
    
    ■ cmd_031: 🏆全完了
    - Phase 1 (Critical 5件): ✅全完了
    - Phase 2 (High 3件): ✅全完了  
    - Phase 3 (Medium 3件): ✅全完了
    - 検証結果: daily=3899, weekly=784, monthly=181 リバランス（全PASS）
    - 統合テスト12件PASS
    
    ■ cmd_032: 🔥Phase 1開始
    - タスク配置完了:
      - task_032_1: 足軽1号（非推奨ファイル統合）
      - task_032_2: 足軽2号（GitHub Actions CI/CD）
      - task_032_3: 足軽3号（キャッシュ自動削除）
      - task_032_4: 足軽4号（スクリプト重複統合）
      - task_032_5: 足軽5号（Exception処理調査）
    
    ■ cmd_030: 部分完了確認中
    - Tier5設計書の完了状況を確認し更新せよ

# =====================================================
# cmd_036: バックテスト高速化（キャッシング・並列化）
# =====================================================
- command_id: cmd_036
  timestamp: "2026-01-29T18:10:00+09:00"
  type: performance_optimization
  priority: critical
  title: "バックテスト高速化 - キャッシング＆並列化"
  
  background: |
    【背景】
    cmd_035でPipeline/Backtest統一を実装し、Sharpeが0.961→2.859へ改善。
    しかし5年バックテスト（777銘柄×61リバランス）の実行時間が数時間規模。
    
    【現状の問題】
    - 各リバランス日でPipeline.run()をフル実行
    - シグナル計算、共分散行列計算が毎回重複
    - シングルスレッド実行

  tasks:
    task_036_1:
      title: "シグナル計算キャッシング"
      description: |
        【目的】
        同一データに対するシグナル計算結果をキャッシュし、重複計算を排除
        
        【対象ファイル】
        - src/orchestrator/signal_generation.py
        - src/signals/*.py
        
        【実装内容】
        1. SignalCacheクラスの作成
           - キー: (symbol, signal_type, lookback_period, date_range_hash)
           - 値: 計算済みシグナル値
           - TTL: セッション単位（バックテスト完了まで）
        
        2. SignalGeneratorへのキャッシュ統合
           - 計算前にキャッシュ確認
           - キャッシュヒット時はスキップ
           - キャッシュミス時は計算してキャッシュに保存
        
        3. メモリ効率
           - LRU方式で上限管理（max 10,000エントリ）
           - 古いエントリから自動削除
        
        【期待効果】
        - 重複計算の90%以上を削減
        - リバランス間での計算再利用
      priority: high

    task_036_2:
      title: "共分散行列キャッシング"
      description: |
        【目的】
        共分散行列計算（O(n²)〜O(n³)）の結果をキャッシュ
        
        【対象ファイル】
        - src/allocation/covariance.py
        - src/orchestrator/risk_allocation.py
        
        【実装内容】
        1. CovarianceCacheクラスの作成
           - キー: (universe_hash, lookback_days, method, end_date)
           - 値: 共分散行列（DataFrame/numpy array）
        
        2. 増分更新オプション
           - 新しい日付のデータのみで更新
           - フル再計算は週1回または閾値超え時のみ
        
        3. RiskEstimatorへの統合
           - estimate_covariance()にキャッシュロジック追加
        
        【期待効果】
        - 共分散計算の80%削減
        - 特に大規模universeで効果大
      priority: high

    task_036_3:
      title: "シグナル計算の並列化"
      description: |
        【目的】
        各銘柄のシグナル計算をマルチプロセスで並列実行
        
        【対象ファイル】
        - src/orchestrator/signal_generation.py
        - src/orchestrator/pipeline.py
        
        【実装内容】
        1. ParallelSignalGeneratorクラスの作成
           - concurrent.futures.ProcessPoolExecutor使用
           - ワーカー数: CPU数 - 1（デフォルト）
        
        2. バッチ処理
           - 銘柄を適切なチャンクに分割
           - 各チャンクを別プロセスで処理
           - 結果をマージ
        
        3. 設定オプション
           ```yaml
           parallel:
             enabled: true
             max_workers: auto  # または具体的な数
             chunk_size: 50
           ```
        
        4. フォールバック
           - 並列化でエラー時はシングルスレッドに切り替え
        
        【期待効果】
        - 8コアCPUで約5〜7倍の高速化
      priority: high

    task_036_4:
      title: "Pipeline軽量モード"
      description: |
        【目的】
        バックテスト用の軽量Pipelineモードを実装
        
        【対象ファイル】
        - src/orchestrator/pipeline.py
        - src/orchestrator/unified_executor.py
        
        【実装内容】
        1. PipelineConfig に lightweight_mode フラグ追加
           ```python
           @dataclass
           class PipelineConfig:
               lightweight_mode: bool = False
               skip_diagnostics: bool = False
               skip_audit_log: bool = False
           ```
        
        2. 軽量モードでスキップする処理
           - 詳細な診断情報の生成
           - Audit logの書き込み
           - 中間結果の保存
           - 非必須のバリデーション
        
        3. UnifiedExecutorで自動有効化
           - run_backtest()時は自動でlightweight_mode=True
        
        【期待効果】
        - オーバーヘッド20〜30%削減
      priority: medium

    task_036_5:
      title: "統合テスト＆ベンチマーク"
      description: |
        【目的】
        高速化の効果を定量的に検証
        
        【実装内容】
        1. ベンチマークスクリプト作成
           - scripts/benchmark_backtest.py
           - 高速化前後の比較
        
        2. テストケース
           - 小規模: 10銘柄 × 1年
           - 中規模: 100銘柄 × 3年
           - 大規模: 777銘柄 × 5年
        
        3. 計測項目
           - 総実行時間
           - リバランス1回あたりの時間
           - メモリ使用量
           - キャッシュヒット率
        
        4. 結果レポート
           - results/benchmark_optimization_report.md
        
        【成功基準】
        - 5年バックテスト: 3時間以内 → 30分以内（10倍高速化）
      priority: medium
      depends_on: [task_036_1, task_036_2, task_036_3, task_036_4]

  execution_strategy: |
    【Phase 1: キャッシング】並列実行
    - task_036_1 (シグナルキャッシュ) → 足軽1号
    - task_036_2 (共分散キャッシュ) → 足軽2号

    【Phase 2: 並列化】Phase 1完了後
    - task_036_3 (並列シグナル計算) → 足軽3号
    - task_036_4 (軽量モード) → 足軽4号

    【Phase 3: 検証】Phase 2完了後
    - task_036_5 (ベンチマーク) → 足軽5号

  expected_outcome: |
    【成果物】
    1. SignalCache クラス
    2. CovarianceCache クラス
    3. ParallelSignalGenerator クラス
    4. Pipeline lightweight_mode
    5. benchmark_backtest.py
    6. benchmark_optimization_report.md

    【期待効果】
    | 項目 | Before | After |
    |------|--------|-------|
    | 5年BT実行時間 | 3時間+ | 30分以下 |
    | リバランス/回 | 3分 | 20秒 |
    | キャッシュヒット率 | N/A | 80%+ |
    | CPU効率 | 12% | 80%+ |

# =====================================================
# cmd_037: 高速化版5年バックテスト実行
# =====================================================
- command_id: cmd_037
  timestamp: "2026-01-29T21:20:00+09:00"
  type: backtest_execution
  priority: critical
  title: "高速化版5年バックテスト実行"
  
  background: |
    【背景】
    cmd_035でPipeline/Backtest統一完了（テストモードでSharpe 0.961→2.859、+197%改善）。
    cmd_036で高速化実装完了（キャッシング、並列化、軽量モード）。
    
    旧バックテストプロセス（3時間21分経過、残り6〜7時間）は停止済み。
    高速化版で再実行し、30分〜1時間での完了を目指す。
    
    【精度確認済み】
    - シグナルキャッシュ: 影響なし（data_hashが十分）
    - 共分散キャッシュ: 軽微（7日間隔でフル再計算）
    - 並列化: 影響なし（計算結果同一）
    - 軽量モード: 影響なし（コアロジック保持）

  tasks:
    task_037_1:
      title: "高速化版5年バックテスト実行"
      description: |
        【目的】
        cmd_036で実装した高速化機能を有効化し、5年バックテストを実行
        
        【実行コマンド】
        ```bash
        cd /Users/souta/Project/stock/multi-agent-shogun/multi-asset-portfolio
        uv run python scripts/run_standard_backtest.py \
          --frequency monthly \
          --start 2020-01-01 \
          --end 2025-01-01 \
          --output results/bt_5y_pipeline_optimized.json
        ```
        
        【確認事項】
        1. 高速化機能が有効になっているか確認
           - config/default.yaml の parallel.enabled: true
           - SignalCache, CovarianceCache が使用されているか
        
        2. 実行時間を計測
           - 開始時刻、終了時刻を記録
           - 高速化前との比較
        
        3. 結果の検証
           - Sharpe Ratio
           - Annual Return
           - Max Drawdown
           - n_rebalances
        
        【成功基準】
        - 実行時間: 1時間以内
        - 結果が正常に出力される
        - キャッシュ統計がログに出力される
      priority: critical

    task_037_2:
      title: "ベンチマーク比較レポート作成"
      description: |
        【目的】
        高速化の効果を定量的にレポート
        
        【レポート内容】
        1. 実行時間比較
           - Before（cmd_035）: 推定10時間+
           - After（cmd_037）: 実測値
           - 高速化率
        
        2. キャッシュ統計
           - シグナルキャッシュ: hits, misses, hit_rate
           - 共分散キャッシュ: hits, misses, hit_rate
        
        3. バックテスト結果
           - Sharpe Ratio
           - Annual Return
           - Max Drawdown
           - Total Return
           - n_rebalances
        
        4. 結論
        
        【出力】
        - results/bt_5y_optimization_benchmark.md
      priority: high
      depends_on: [task_037_1]

  execution_strategy: |
    【Phase 1: バックテスト実行】
    - task_037_1 → 足軽1号（即時実行）

    【Phase 2: レポート作成】task_037_1完了後
    - task_037_2 → 足軽2号

  expected_outcome: |
    【成果物】
    1. results/bt_5y_pipeline_optimized.json
    2. results/bt_5y_optimization_benchmark.md

    【期待効果】
    - 実行時間: 10時間+ → 30分〜1時間（10倍以上高速化）
    - Sharpe向上の確認（テストモードで+197%）

# =====================================================
# cmd_038: 高速化再設計版バックテスト実行
# =====================================================
- command_id: cmd_038
  timestamp: "2026-01-29T21:45:00+09:00"
  type: backtest_execution
  priority: critical
  title: "高速化再設計版（v2.1）バックテスト実行"
  
  background: |
    【背景】
    cmd_036-037の高速化が効果なし（キャッシュヒット率0%）だった原因を特定。
    
    【問題の本質】
    - 毎リバランスでPipeline.run()がAPI経由でデータ取得していた
    - キャッシュはリバランス日ごとにdata_hashが異なるためヒットしない
    
    【v2.1 再設計内容】
    1. skip_data_fetch=True でAPI呼び出しをスキップ
    2. 価格データをPipelineに事前注入（_raw_data）
    3. 不要なキャッシュ機能を削除:
       - signal_cache.py 削除
       - parallel_signal.py 削除
       - covariance_cache.py 削除
       - SignalGenerator/RiskEstimator からキャッシュ参照削除
    
    【期待効果】
    - データ取得時間 → ゼロ（事前注入済み）
    - キャッシュオーバーヘッド → ゼロ（削除済み）
    - 推定: 10時間+ → 30分以内

  tasks:
    task_038_1:
      title: "v2.1高速化版バックテスト実行"
      description: |
        【実行コマンド】
        ```bash
        cd /Users/souta/Project/stock/multi-agent-shogun/multi-asset-portfolio
        uv run python scripts/run_standard_backtest.py \
          --frequency monthly \
          --start 2020-01-01 \
          --end 2025-01-01 \
          --output results/bt_5y_v21_monthly.json
        ```
        
        【確認事項】
        1. "skip_data_fetch=True" のログ確認
        2. "Price data pre-injected" のログ確認
        3. 実行時間の計測
        
        【成功基準】
        - 実行時間: 30分以内
        - 結果が正常に出力される
      priority: critical
      timeout_minutes: 60

  execution_strategy: |
    - task_038_1 → 足軽1号（即時実行）

  expected_outcome: |
    【成果物】
    - results/bt_5y_v21_monthly.json

    【期待効果】
    - 実行時間: 10時間+ → 30分以内（20倍以上高速化）

# =====================================================
# cmd_012: パフォーマンスチューニング実装
# =====================================================
- command_id: cmd_012
  timestamp: "2026-01-29T22:30:00+09:00"
  type: performance_tuning
  priority: critical
  title: "Sharpe Ratio 1.0達成のためのパフォーマンスチューニング"
  
  background: |
    【現状】
    - Sharpe Ratio: 0.878
    - 年率リターン: 21.77%
    - 銘柄数: 5
    
    【目標】
    - Sharpe >= 1.0
    - MDD < 20%
    - SPYを70%以上の期間で上回る

  tasks:
    # Phase 1（並行実行可能）
    task_012_1:
      title: "アセット拡張（ETF追加）"
      description: |
        config/universe.yaml にETFを追加して低相関アセットによる分散効果を向上。
        
        【追加ETF】
        ```yaml
        # コモディティ拡張
        commodity_extended: [GSG, CPER, URA]
        
        # グロース/小型株
        growth_small: [VBK, IWO, VTWO, SCHA]
        
        # 新興国（国別）
        emerging_specific: [FM, FXI, EWZ, INDA]
        
        # 代替資産
        alternative_extended: [USRT, SCHH, IFRA, BNDX]
        
        # 債券拡張
        bonds_extended: [ANGL, VGIT, SHYG, EMB]
        ```
        
        【期待効果】Sharpe +0.05-0.08
      priority: critical
      
    task_012_2:
      title: "センチメントシグナル確認・強化"
      description: |
        src/signals/sentiment.py を確認し、必要に応じて強化。
        
        【確認対象】
        1. VIXSentimentSignal - VIX水準によるリスクオン/オフ
        2. PutCallRatioSignal - P/Cレシオによる逆張り
        3. MarketBreadthSignal - 上昇銘柄比率
        4. FearGreedCompositeSignal - 複合センチメント
        
        【パラメータ確認】
        - vix_threshold_high: 25
        - vix_threshold_low: 15
        - breadth_threshold: 50%
        
        【期待効果】Sharpe +0.03-0.05
      priority: critical

    # Phase 2（Phase 1完了後、並行実行可能）
    task_012_3:
      title: "マクロ経済シグナル確認・強化"
      description: |
        src/signals/macro.py を確認し、必要に応じて強化。
        
        【確認対象】
        1. YieldCurveSignal - イールドカーブ形状（TLT/SHY比率）
        2. InflationExpectationSignal - インフレ期待（TIP/IEF比率）
        3. CreditSpreadSignal - クレジットスプレッド（HYG/LQD比率）
        4. DollarStrengthSignal - ドル強度（UUP）
        
        【期待効果】Sharpe +0.02-0.04
      priority: high
      
    task_012_4:
      title: "マルチタイムフレーム戦略"
      description: |
        src/signals/trend.py と config/default.yaml を修正してマルチタイムフレーム戦略を実装。
        
        【設計】
        ```yaml
        multi_timeframe:
          enabled: true
          timeframes:
            daily:
              lookbacks: [5, 10, 20]
              weight: 0.30
            weekly:
              lookbacks: [20, 40, 60]
              weight: 0.35
            monthly:
              lookbacks: [60, 120, 252]
              weight: 0.35
          consensus_threshold: 0.6
        ```
        
        【新シグナル】
        - MultiTimeframeMomentumSignal
        - TimeframeConsensusSignal
        
        【期待効果】Sharpe +0.03-0.05
      priority: high

    # Phase 3（task_012_1依存）
    task_012_5:
      title: "動的相関調整"
      description: |
        src/allocation/covariance.py に動的相関調整機能を追加。
        
        【機能】
        1. レジーム条件付き相関推定
        2. クライシス時の相関上方調整（+30%）
        3. 低ボラ時の相関下方調整
        
        【期待効果】Sharpe +0.02-0.04
      priority: high
      depends_on: [task_012_1]

    # Phase 4（task_012_1-5依存）
    task_012_6:
      title: "パラメータ最適化"
      description: |
        config/default.yaml と src/analysis/ でパラメータ最適化を実施。
        
        【最適化対象】
        ```yaml
        signal_params:
          momentum_lookback: [20, 40, 60, 90, 120]
          bollinger_period: [14, 20, 30]
        
        allocation_params:
          w_asset_max: [0.15, 0.20, 0.25]
          smooth_alpha: [0.2, 0.3, 0.4]
        
        meta_params:
          top_n: [5, 7, 10, 12, 15]
          beta: [1.5, 2.0, 2.5, 3.0]
        ```
        
        【過学習防止】
        - 5-fold時系列CV
        - Purge gap 5日
        - OOS Sharpe ± 20%以内
        
        【期待効果】Sharpe +0.02-0.03
      priority: medium
      depends_on: [task_012_1, task_012_2, task_012_3, task_012_4, task_012_5]

    # Phase 5（task_012_5依存）
    task_012_7:
      title: "レジーム適応アロケーション"
      description: |
        src/meta/dynamic_weighter.py を修正してレジーム適応アロケーションを実装。
        
        【レジーム定義】
        ```yaml
        bull_trend:
          conditions: [momentum_20d > 0.05, breadth > 0.6, vix < 20]
          adjustments:
            momentum_weight: 1.3
            cash_target: 0.05
        
        bear_market:
          conditions: [momentum_60d < -0.10, vix > 25]
          adjustments:
            cash_target: 0.30
            defensive_weight: 1.5
        ```
        
        【期待効果】Sharpe +0.02-0.03
      priority: medium
      depends_on: [task_012_5]

    # Phase 6（全タスク依存）
    task_012_8:
      title: "15年バックテスト検証"
      description: |
        2010-01-01 ~ 2025-01-01 の15年バックテストを実施。
        
        【比較対象】
        - SPY Buy-and-Hold
        - 60/40 Portfolio (SPY/TLT)
        - 改善前システム
        
        【サブ期間分析】
        - 2010-2012: GFC回復期
        - 2013-2017: 低ボラ上昇相場
        - 2018-2019: ボラスパイク
        - 2020-2021: COVID
        - 2022-2025: 金利上昇
        
        【成功基準】
        - Sharpe >= 1.0
        - MDD < 20%
        - SPY超過: 70%以上の期間
      priority: medium
      depends_on: [task_012_1, task_012_2, task_012_3, task_012_4, task_012_5, task_012_6, task_012_7]

  execution_strategy: |
    【Phase 1】並行実行
    - task_012_1 → 足軽1号
    - task_012_2 → 足軽2号
    
    【Phase 2】Phase 1完了後、並行実行
    - task_012_3 → 足軽3号
    - task_012_4 → 足軽4号
    
    【Phase 3】task_012_1完了後
    - task_012_5 → 足軽5号
    
    【Phase 4】Phase 1-3完了後
    - task_012_6 → 足軽6号
    
    【Phase 5】task_012_5完了後
    - task_012_7 → 足軽7号
    
    【Phase 6】全タスク完了後
    - task_012_8 → 足軽8号

  expected_outcome: |
    【期待される累積効果】
    - Phase 1-2完了: Sharpe 0.95-1.00
    - Phase 3-5完了: Sharpe 1.00-1.10
    - Phase 6-7完了: Sharpe 1.05-1.15
    - 検証完了: Sharpe >= 1.0 (確定)
    
    【成果物】
    - 拡張されたアセットユニバース
    - 強化されたシグナルモジュール
    - 最適化されたパラメータ設定
    - 15年バックテスト検証レポート

# =====================================================
# cmd_013: バックテスト高速化（精度維持）
# =====================================================
- command_id: cmd_013
  timestamp: "2026-01-29T23:00:00+09:00"
  type: performance_optimization
  priority: critical
  title: "バックテスト実行速度の大幅改善（精度への影響なし）"
  
  background: |
    【現状】
    - 5年バックテストで30分〜数時間
    - シグナル計算が全体の70-80%を占める
    - キャッシュヒット率0%（cmd_036-037で判明）
    - DataFrame変換が4回以上/アセット発生
    
    【目標】
    - 5年バックテストを10分以内に短縮
    - 精度への影響は一切許されない（数値一致必須）
    
    【ボトルネック分析結果】
    | ボトルネック | 時間割合 | 原因 |
    |-------------|----------|------|
    | シグナル生成 | 70-80% | グリッド探索、ベクトル化なし |
    | DataFrame変換 | 15-25% | Polars↔Pandas 重複変換 |
    | 共分散推定 | 5-10% | 固有値分解O(n³) |
    | HRP/NCO配分 | 5-10% | クラスタリング |

  tasks:
    # Phase 1（並列可能）
    task_013_1:
      title: "シグナルキャッシュ効果の修正"
      description: |
        src/backtest/cache.py (lines 156-184) を修正
        
        【問題】
        キャッシュキーが正確な日付範囲を使用 → ローリングウィンドウでヒットしない
        
        【解決策】
        日付アライメント関数を追加:
        ```python
        def align_cache_date(date: datetime, granularity_days: int = 7) -> datetime:
            """週単位でアライメントしてキャッシュヒット率向上"""
            return date - timedelta(days=date.weekday())
        ```
        generate_cache_key()でこの関数を使用
        
        【検証】
        - キャッシュ有無で同一シグナル値確認
        - ヒット率ログ追加
        
        【期待効果】シグナル計算時間 60-80%削減
      priority: critical

    task_013_2:
      title: "シグナル計算のベクトル化"
      description: |
        src/backtest/signal_precompute.py (lines 196-226) を修正
        
        【問題】
        ticker毎に逐次計算
        
        【解決策】
        ```python
        # Before (sequential)
        for ticker in tickers:
            ticker_data = prices.filter(pl.col("ticker") == ticker)
            momentum = (ticker_data["close"].shift(-period) / ticker_data["close"] - 1)

        # After (vectorized)
        result = prices.group_by("ticker").agg([
            (pl.col("close").shift(-period) / pl.col("close") - 1).alias("momentum")
        ])
        ```
        
        【検証】
        結果DataFrameの全値が1e-10以内で一致
        
        【期待効果】5-10倍高速化
      priority: critical

    task_013_3:
      title: "インクリメンタル共分散の統合"
      description: |
        src/allocation/covariance.py (lines 236-292) を修正
        
        【問題】
        IncrementalCovarianceEstimatorが存在するが未活用
        
        【解決策】
        ```python
        def estimate(
            self,
            returns: pd.DataFrame,
            incremental_state: CovarianceState | None = None,
        ) -> CovarianceResult:
            if incremental_state is not None:
                return self._update_incremental(returns, incremental_state)
            else:
                return self._compute_full(returns)
        ```
        
        【検証】
        共分散行列の最大差が1e-8未満
        
        【期待効果】共分散計算 3-5倍高速化
      priority: critical

    # Phase 2（Phase 1完了後、並列可能）
    task_013_4:
      title: "バッチデータ取得の最適化"
      description: |
        src/data/adapters/multi_source_adapter.py (lines 400-449) を修正
        
        【問題】
        個別ticker毎にyfinance API呼び出し
        
        【解決策】
        ```python
        def _fetch_batch_yfinance(self, tickers: List[str], start, end):
            df = yf.download(
                tickers,
                start=start,
                end=end,
                group_by='ticker',
                threads=True,
                progress=False,
            )
            return {ticker: df[ticker] for ticker in tickers if ticker in df.columns}
        ```
        
        【検証】
        個別取得とバッチ取得でデータ一致確認
        
        【期待効果】データ取得時間 40-60%削減
      priority: high

    task_013_5:
      title: "シグナル生成の並列化"
      description: |
        src/orchestrator/signal_generation.py (lines 154-274) を修正
        
        【問題】
        アセット毎に逐次処理
        
        【解決策】
        ```python
        from concurrent.futures import ThreadPoolExecutor, as_completed

        def generate_signals(self, ...):
            with ThreadPoolExecutor(max_workers=self._get_workers()) as executor:
                futures = {
                    executor.submit(self._process_single_asset, symbol, df): symbol
                    for symbol, df in raw_data.items()
                }
                for future in as_completed(futures):
                    symbol = futures[future]
                    signals[symbol] = future.result()
        ```
        
        【検証】
        - 乱数シード固定で再現性確認
        - 複数実行で結果一致
        
        【期待効果】3-4倍高速化
      priority: high

    task_013_6:
      title: "DataFrame変換の削減"
      description: |
        複数ファイルを修正:
        - src/orchestrator/signal_generation.py (lines 158-159)
        - src/backtest/vectorized_compute.py (lines 161, 198, 230)
        - src/utils/dataframe_utils.py (新規作成)
        
        【問題】
        同一データに対してPolars↔Pandas変換が4回以上発生
        
        【解決策】
        ```python
        # src/utils/dataframe_utils.py (新規)
        def ensure_polars(df: Union[pd.DataFrame, pl.DataFrame]) -> pl.DataFrame:
            if isinstance(df, pd.DataFrame):
                return pl.from_pandas(df)
            return df

        def ensure_pandas(df: Union[pd.DataFrame, pl.DataFrame]) -> pd.DataFrame:
            if isinstance(df, pl.DataFrame):
                return df.to_pandas()
            return df
        ```
        内部処理をPolarsに統一、API境界でのみ変換
        
        【検証】
        既存テスト全パス、出力値の数値差なし
        
        【期待効果】パイプラインオーバーヘッド 20-30%削減
      priority: high

    # Phase 3（Phase 2完了後、並列可能）
    task_013_7:
      title: "HRPキャッシュ改善"
      description: |
        src/allocation/hrp.py (lines 169-191) を修正
        
        【問題】
        完全一致ハッシュでキャッシュヒット率低
        
        【解決策】
        ```python
        def _hash_correlation_matrix(self, corr: NDArray, n_assets: int) -> str:
            rounded = np.round(corr, self.config.hash_precision)
            eigenvalues = np.linalg.eigvalsh(corr)
            bucket = np.round(eigenvalues.sum(), 2)
            hash_input = f"{n_assets}_{bucket}_{self.config.linkage_method}_".encode()
            ...
        ```
        
        【検証】
        HRPウェイトの最大差が1e-6未満
        
        【期待効果】HRP配分計算 30-50%高速化
      priority: medium

    task_013_8:
      title: "NCO最適化結果キャッシング"
      description: |
        src/allocation/nco.py (lines 362-397) を修正
        
        【問題】
        クラスタ内最適化を毎回再計算
        
        【解決策】
        ```python
        self._cluster_weights_cache: Dict[str, Dict[str, float]] = {}

        def _optimize_intra_cluster(self, cluster: ClusterInfo) -> None:
            cache_key = self._hash_cluster_cov(cluster)
            if cache_key in self._cluster_weights_cache:
                cluster.weights_intra = self._cluster_weights_cache[cache_key]
                return
            # ... 既存の最適化処理 ...
            self._cluster_weights_cache[cache_key] = cluster.weights_intra
        ```
        
        【検証】
        NCOウェイトが変更前後で一致
        
        【期待効果】NCO配分計算 20-40%高速化
      priority: medium

  execution_strategy: |
    【Phase 1】並列実行
    - task_013_1 → 足軽3号
    - task_013_2 → 足軽4号
    - task_013_3 → 足軽5号
    
    【Phase 2】Phase 1完了後、並列実行
    - task_013_4 → 足軽3号
    - task_013_5 → 足軽4号
    - task_013_6 → 足軽5号
    
    【Phase 3】Phase 2完了後、並列実行
    - task_013_7 → 足軽6号
    - task_013_8 → 足軽7号
    
    注: cmd_012のタスクと並行可能。空いている足軽に配置せよ。

  expected_outcome: |
    【期待される累積効果】
    | フェーズ | 現状時間 | 期待時間 | 高速化率 |
    |----------|----------|----------|----------|
    | 現状 | 30分 | - | 1.0x |
    | Phase 1完了 | - | 10-15分 | 2-3x |
    | Phase 2完了 | - | 5-8分 | 4-6x |
    | Phase 3完了 | - | 3-5分 | 6-10x |
    
    【必須検証】
    - 各タスク完了後に精度テスト実行
    - 最適化前後で数値が1e-10以内で一致すること
    - パフォーマンスベンチマーク実行
    
    【成功基準】
    - 5年バックテスト: 30分 → 10分以内
    - 精度: 全く同一の結果

  - id: cmd_039
    timestamp: "2026-01-29T23:55:00"
    command: "15年バックテスト高速化（精度維持必須）"
    project: multi-asset-portfolio
    priority: critical
    status: pending
    context: |
      殿より「精度が落ちる提案は許されない。並列化を含めて検討せよ」との命あり。
      
      【現状の問題】
      - 15年バックテスト: 60-70時間（非現実的）
      - ボトルネック: spearmanr相関計算（21分/回）
      - 原因: GIL制約、単一スレッド実行、キャッシュ未活用
      
      【調査結果】
      - src/orchestrator/signal_generation.py: ThreadPoolExecutor使用（GIL制約あり）
      - _evaluate_signal_quality(): spearmanr計算がボトルネック
      - パラメータグリッドサーチ: 順序実行（並列化可能）
      
      【精度100%保証の高速化手法】
      | 優先度 | 施策 | 高速化倍率 | 精度 | 難易度 |
      |--------|------|-----------|------|--------|
      | 1 | spearmanrのNumba JIT化 | 5-10倍 | 100% | ★☆☆ |
      | 2 | ThreadPool→ProcessPool切替 | 4-8倍 | 100% | ★★☆ |
      | 3 | 信号品質スコアのメモ化 | 1.3倍 | 100% | ★☆☆ |
      | 4 | ベクトル化spearmanr | 8-15倍 | 100% | ★★☆ |
      
      【目標】
      - Phase 1完了: 60-70時間 → 3-5時間
      - Phase 2完了: → 30分-2時間
      
      【絶対条件】
      - 精度低下は一切許されない
      - 最適化前後で数値が完全一致すること

    tasks:
      - task_id: task_039_1
        description: |
          spearmanrのNumba JIT化を実装せよ
          
          【実装場所】
          src/backtest/numba_compute.py に追加
          
          【実装内容】
          ```python
          @njit
          def rankdata_numba(x):
              """Numba高速ランク付け"""
              n = len(x)
              order = np.argsort(x)
              ranks = np.empty(n, dtype=np.float64)
              ranks[order] = np.arange(1, n + 1, dtype=np.float64)
              return ranks
          
          @njit
          def spearmanr_numba(x, y):
              """Numba高速Spearman相関（精度100%保証）"""
              ranks_x = rankdata_numba(x)
              ranks_y = rankdata_numba(y)
              # Pearson相関をランクに適用
              n = len(x)
              mean_x = np.mean(ranks_x)
              mean_y = np.mean(ranks_y)
              num = np.sum((ranks_x - mean_x) * (ranks_y - mean_y))
              den = np.sqrt(np.sum((ranks_x - mean_x)**2) * np.sum((ranks_y - mean_y)**2))
              return num / den if den > 0 else 0.0
          ```
          
          【検証】
          - scipy.stats.spearmanrと結果が完全一致することを確認
          - テストケース: ランダムデータ1000件で差分 < 1e-10
          
        assigned_to: ashigaru1
        status: pending
        
      - task_id: task_039_2
        description: |
          signal_generation.pyのThreadPoolをProcessPoolに切り替えよ
          
          【変更ファイル】
          src/orchestrator/signal_generation.py
          
          【変更内容】
          1. ThreadPoolExecutor → ProcessPoolExecutor に変更
          2. GIL回避によりCPUバウンド処理が真に並列化される
          
          【注意点】
          - プロセス間通信のオーバーヘッドに注意
          - 大きなオブジェクトの受け渡しを避ける
          - pickle可能なオブジェクトのみ使用
          
          【検証】
          - 既存テストがすべてパスすること
          - 結果が変更前と完全一致すること
          
        assigned_to: ashigaru2
        status: pending
        depends_on: []
        
      - task_id: task_039_3
        description: |
          _evaluate_signal_quality()にメモ化を導入せよ
          
          【変更ファイル】
          src/orchestrator/signal_generation.py
          
          【実装内容】
          ```python
          from functools import lru_cache
          import hashlib
          
          def _hash_array(arr):
              """配列のハッシュを生成"""
              return hashlib.md5(arr.tobytes()).hexdigest()
          
          # キャッシュ付き評価関数
          _quality_cache = {}
          
          def _evaluate_signal_quality_cached(self, scores, prices):
              key = (_hash_array(scores.values), _hash_array(prices.values))
              if key in _quality_cache:
                  return _quality_cache[key]
              result = self._evaluate_signal_quality_impl(scores, prices)
              _quality_cache[key] = result
              return result
          ```
          
          【検証】
          - キャッシュヒット時と非ヒット時で同一結果
          - メモリ使用量が許容範囲内（max 1GB）
          
        assigned_to: ashigaru3
        status: pending
        depends_on: []
        
      - task_id: task_039_4
        description: |
          task_039_1で実装したspearmanr_numbaをsignal_generation.pyに統合せよ
          
          【変更ファイル】
          src/orchestrator/signal_generation.py
          
          【変更内容】
          _evaluate_signal_quality()内のspearmanr呼び出しを
          spearmanr_numbaに置き換える
          
          ```python
          # Before
          from scipy.stats import spearmanr
          corr, _ = spearmanr(valid_scores, valid_returns)
          
          # After
          from src.backtest.numba_compute import spearmanr_numba
          corr = spearmanr_numba(
              valid_scores.values.astype(np.float64),
              valid_returns.values.astype(np.float64)
          )
          ```
          
          【検証】
          - バックテスト結果が変更前と完全一致
          - ベンチマーク: 処理時間を計測し報告
          
        assigned_to: ashigaru1
        status: pending
        depends_on: [task_039_1]
        
      - task_id: task_039_5
        description: |
          Phase 1統合テスト＆ベンチマークを実行せよ
          
          【検証内容】
          1. 精度テスト
             - 1年バックテストを実行
             - 変更前後でSharpe, Return, MDDが完全一致（diff < 1e-10）
          
          2. パフォーマンステスト
             - 5年バックテストの実行時間を計測
             - 目標: 30分 → 10分以内
          
          3. 結果報告
             - 精度比較表
             - 処理時間比較表
             - ボトルネック分析
          
        assigned_to: ashigaru4
        status: pending
        depends_on: [task_039_1, task_039_2, task_039_3, task_039_4]

    assignment_strategy: |
      【Phase 1】並列実行可能
      - task_039_1 → 足軽1号（Numba実装）
      - task_039_2 → 足軽2号（ProcessPool切替）
      - task_039_3 → 足軽3号（メモ化）
      
      【Phase 2】task_039_1完了後
      - task_039_4 → 足軽1号（統合）
      
      【Phase 3】全タスク完了後
      - task_039_5 → 足軽4号（検証）

    expected_outcome: |
      【期待される効果】
      | 施策 | 高速化倍率 |
      |------|-----------|
      | Numba JIT化 | 5-10倍 |
      | ProcessPool切替 | 4-8倍 |
      | メモ化 | 1.3倍 |
      | 合計 | 26-104倍 |
      
      【目標時間】
      - 現状: 60-70時間
      - Phase 1完了後: 3-5時間
      
      【必須条件】
      - 精度低下ゼロ（数値完全一致）
      - 全既存テストパス

  - id: cmd_040
    timestamp: "2026-01-30T01:00:00"
    command: "バックテスト高速化Phase2（キャッシュ活用・インクリメンタル化）"
    project: multi-asset-portfolio
    priority: critical
    status: pending
    context: |
      殿より「軽量モードは不要。それ以外を進めよ」との命あり。
      また「シグナル種類が増えた場合に意図せずキャッシュが引かれないよう注意せよ」との指示。
      
      【現状の問題】
      - 月次15年バックテスト: 約21時間（非現実的）
      - 毎リバランスで全計算を繰り返している
      - 既存キャッシュ機構が活用されていない
      
      【実装する施策】
      1. シグナル事前計算（Precompute）- 効果40-50%
      2. 品質チェックのインクリメンタル化 - 効果20-30%
      3. 共分散の逐次更新活用 - 効果10-20%
      4. ウォームスタート（チェックポイント）機能
      
      【キャッシュ無効化の設計方針】
      - シグナル種類変更検知: registry.list_all()のハッシュを記録
      - パラメータ変更検知: signal_configのハッシュを記録
      - 銘柄リスト変更検知: universeのハッシュを記録
      - 価格データ変更検知: prices_dfのハッシュを記録
      - いずれかが不一致の場合、該当キャッシュを再計算
      
      【期待効果】
      - 初回: 21時間 → 12時間
      - 2回目以降: 21時間 → 30分

    tasks:
      - task_id: task_040_1
        description: |
          シグナル事前計算（Precompute）機能を実装せよ
          
          【実装ファイル】
          src/backtest/signal_precompute.py を拡張
          
          【キャッシュ無効化設計】
          ```python
          @dataclass
          class PrecomputeMetadata:
              created_at: datetime
              signal_registry_hash: str  # registry.list_all()のハッシュ
              signal_config_hash: str    # 各シグナルのパラメータハッシュ
              universe_hash: str         # 銘柄リストのハッシュ
              prices_hash: str           # 価格データのハッシュ
              version: str               # ライブラリバージョン
          
          def is_cache_valid(self, current_metadata: PrecomputeMetadata) -> bool:
              """キャッシュが有効か判定"""
              if self.signal_registry_hash != current_metadata.signal_registry_hash:
                  logger.warning("シグナル種類が変更されました。再計算します。")
                  return False
              if self.signal_config_hash != current_metadata.signal_config_hash:
                  logger.warning("シグナルパラメータが変更されました。再計算します。")
                  return False
              # ... 他のチェック
              return True
          ```
          
          【実装内容】
          1. SignalPrecomputer クラスの拡張
             - precompute_all(): 全シグナルを事前計算
             - get_signals_at_date(): 特定日付のシグナル取得
             - validate_cache(): キャッシュ有効性チェック
          
          2. キャッシュ形式
             - .cache/signals/{signal_name}/{ticker}.parquet
             - .cache/signals/metadata.json（上記メタデータ）
          
          3. unified_executor.py への統合
             - バックテスト開始前にprecompute_all()呼び出し
             - 各リバランスでget_signals_at_date()使用
          
          【検証】
          - シグナル追加時に自動再計算されることを確認
          - キャッシュヒット時は計算がスキップされることを確認
          
        assigned_to: ashigaru1
        status: pending
        
      - task_id: task_040_2
        description: |
          品質チェックのインクリメンタル化を実装せよ
          
          【実装ファイル】
          src/orchestrator/data_preparation.py
          
          【キャッシュ無効化設計】
          ```python
          @dataclass
          class QualityCheckCache:
              date: datetime
              universe_hash: str
              quality_config_hash: str  # 品質基準のハッシュ
              reports: dict[str, QualityReport]
              excluded_assets: list[str]
          ```
          
          【実装内容】
          1. 品質レポートのキャッシュ機構
             - .cache/quality/{date}.pkl
             - メタデータ: universe_hash, config_hash
          
          2. インクリメンタルチェック
             - 前回の除外銘柄は再利用（変化なし）
             - 新規バーのみチェック（約20日分）
             - 新規銘柄は全チェック
          
          3. キャッシュ無効化条件
             - 品質基準（閾値等）変更時
             - 銘柄リスト変更時
          
          【検証】
          - 品質基準変更時に再チェックされることを確認
          - 処理時間が45秒→5秒に短縮されることを確認
          
        assigned_to: ashigaru2
        status: pending
        
      - task_id: task_040_3
        description: |
          共分散の逐次更新を活用せよ
          
          【実装ファイル】
          src/orchestrator/risk_allocation.py
          src/backtest/covariance_cache.py（既存）
          
          【実装内容】
          1. RiskEstimator に共分散キャッシュ統合
             - 前回の推定器状態をロード
             - 新規リターンで逐次更新
             - 状態を保存
          
          2. キャッシュキー設計
             - universe_hash + halflife + date
             - 銘柄数変更時は再計算
          
          3. unified_executor.py への統合
             - バックテスト開始時に初期状態計算
             - 各リバランスで逐次更新
          
          【検証】
          - 処理時間が30秒→3秒に短縮されることを確認
          - 結果が従来手法と一致することを確認（diff < 1e-8）
          
        assigned_to: ashigaru3
        status: pending
        
      - task_id: task_040_4
        description: |
          ウォームスタート（チェックポイント）機能を実装せよ
          
          【実装ファイル】
          src/orchestrator/unified_executor.py
          
          【実装内容】
          1. チェックポイント保存
             ```python
             @dataclass
             class BacktestCheckpoint:
                 rebalance_index: int
                 current_date: datetime
                 equity_curve: list[float]
                 weights_history: list[dict]
                 covariance_state: CovarianceState
                 signal_cache_state: dict
             
             def save_checkpoint(self, checkpoint: BacktestCheckpoint, path: Path):
                 with open(path, 'wb') as f:
                     pickle.dump(checkpoint, f)
             ```
          
          2. チェックポイントからの再開
             ```python
             def resume_from_checkpoint(self, checkpoint_path: Path):
                 checkpoint = self.load_checkpoint(checkpoint_path)
                 # 状態を復元
                 self._rebalance_index = checkpoint.rebalance_index
                 self._equity_curve = checkpoint.equity_curve
                 # ... 続きから実行
             ```
          
          3. CLI対応
             ```bash
             # チェックポイント保存（10リバランスごと）
             python scripts/run_standard_backtest.py --checkpoint-interval 10
             
             # 再開
             python scripts/run_standard_backtest.py --resume checkpoints/cp_80.pkl
             ```
          
          【検証】
          - 中断・再開で結果が一致することを確認
          - チェックポイントファイルサイズが妥当（< 100MB）
          
        assigned_to: ashigaru4
        status: pending
        
      - task_id: task_040_5
        description: |
          統合テスト＆ベンチマークを実行せよ
          
          【検証内容】
          1. キャッシュ無効化テスト
             - シグナル追加時に再計算されることを確認
             - パラメータ変更時に再計算されることを確認
             - 銘柄追加時に該当銘柄のみ計算されることを確認
          
          2. 精度テスト
             - キャッシュ使用時と非使用時で結果が完全一致（diff < 1e-10）
          
          3. パフォーマンステスト
             - 初回実行時間を計測
             - 2回目実行時間を計測（キャッシュヒット）
             - 目標: 初回12時間、2回目30分
          
          4. ウォームスタートテスト
             - 50回目で中断→再開で結果一致
          
        assigned_to: ashigaru5
        status: pending
        depends_on: [task_040_1, task_040_2, task_040_3, task_040_4]

    assignment_strategy: |
      【Phase 1】並列実行可能
      - task_040_1 → 足軽1号（シグナル事前計算）
      - task_040_2 → 足軽2号（品質チェック最適化）
      - task_040_3 → 足軽3号（共分散逐次更新）
      - task_040_4 → 足軽4号（ウォームスタート）
      
      【Phase 2】全完了後
      - task_040_5 → 足軽5号（統合テスト）

    expected_outcome: |
      【期待効果】
      | 項目 | 現在 | 改善後（初回） | 改善後（2回目） |
      |------|------|---------------|----------------|
      | 月次15年 | 21時間 | 12時間 | 30分 |
      | 週次15年 | 90時間 | 50時間 | 2時間 |
      
      【キャッシュ無効化保証】
      - シグナル種類変更: 自動検知・再計算
      - パラメータ変更: 自動検知・再計算
      - 銘柄変更: 自動検知・差分計算
      - 価格データ変更: 自動検知・再計算

  - id: cmd_041
    timestamp: "2026-01-30T01:30:00"
    command: "シグナルキャッシュのインクリメンタル更新対応"
    project: multi-asset-portfolio
    priority: high
    status: pending
    depends_on: [cmd_040]
    context: |
      殿より「1日分の増分があった場合にキャッシュが再計算されない設計とせよ」との命あり。
      
      【現状の問題】
      現在の prices_hash 計算ロジック:
      ```python
      hash_input = f"{prices.shape}_{prices.columns}"  # 行数が変わる
      hash_input += f"_{first_ts}_{last_ts}"           # last_tsが変わる
      hash_input += f"_{close_sum:.6f}"                # close合計が変わる
      ```
      
      問題:
      - 1日でもデータが追加されると全シグナルが再計算される
      - 15年バックテストの各リバランスでデータが増えるため、毎回キャッシュ無効化
      - これがキャッシュが効かない根本原因
      
      【改善方針】
      インクリメンタルキャッシュ設計:
      - キャッシュ済み期間（cached_start_date, cached_end_date）を記録
      - 期間延長のみの場合は差分計算のみ実行
      - シグナル種類・パラメータ・銘柄変更時のみ全再計算
      
      【期待効果】
      - 2回目以降のバックテスト: 全再計算 → 差分計算のみ
      - 日次データ追加時: 1日分のシグナルのみ計算

    tasks:
      - task_id: task_041_1
        description: |
          PrecomputeMetadataにキャッシュ済み期間を追加せよ
          
          【変更ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          @dataclass
          class PrecomputeMetadata:
              # 既存フィールド
              created_at: str
              signal_registry_hash: str
              signal_config_hash: str
              universe_hash: str
              prices_hash: str  # 削除または用途変更
              version: str
              
              # 追加フィールド
              cached_start_date: str  # キャッシュ済み開始日
              cached_end_date: str    # キャッシュ済み終了日
              ticker_count: int       # 銘柄数（整合性チェック用）
          ```
          
          【注意】
          - 既存キャッシュとの後方互換性を維持
          - prices_hashは銘柄変更検知用に簡略化（行数・日付範囲のみ）
          
        assigned_to: ashigaru1
        status: pending
        
      - task_id: task_041_2
        description: |
          インクリメンタルキャッシュ検証ロジックを実装せよ
          
          【変更ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          def validate_cache_incremental(
              self,
              prices: pl.DataFrame,
              config: dict[str, Any] | None = None,
          ) -> tuple[bool, str, datetime | None]:
              """
              インクリメンタルキャッシュ検証
              
              Returns:
                  (can_use_cache, reason, incremental_start_date)
                  - can_use_cache=True, incremental_start_date=None 
                    → 完全キャッシュヒット（計算不要）
                  - can_use_cache=True, incremental_start_date=date 
                    → 差分計算のみ必要
                  - can_use_cache=False 
                    → 全再計算必要
              """
              cached = self._load_precompute_metadata()
              if cached is None:
                  return False, "No cache found", None
              
              # バージョン変更 → 全再計算
              if cached.version != PRECOMPUTE_VERSION:
                  return False, "Version changed", None
              
              # シグナル種類変更 → 全再計算
              current_registry_hash = self._compute_signal_registry_hash()
              if cached.signal_registry_hash != current_registry_hash:
                  return False, "Signal registry changed", None
              
              # パラメータ変更 → 全再計算
              current_config_hash = self._compute_signal_config_hash(config or self.DEFAULT_CONFIG)
              if cached.signal_config_hash != current_config_hash:
                  return False, "Signal config changed", None
              
              # 銘柄変更 → 全再計算
              current_universe_hash = self._compute_universe_hash(
                  prices.select("ticker").unique().to_series().to_list()
              )
              if cached.universe_hash != current_universe_hash:
                  return False, "Universe changed", None
              
              # 期間チェック
              current_start = prices["timestamp"].min()
              current_end = prices["timestamp"].max()
              cached_start = datetime.fromisoformat(cached.cached_start_date)
              cached_end = datetime.fromisoformat(cached.cached_end_date)
              
              # 開始日が変わった場合 → 全再計算（過去データ追加）
              if current_start < cached_start:
                  return False, "Start date extended backward", None
              
              # 終了日が延長された場合 → 差分計算
              if current_end > cached_end:
                  return True, "Incremental update needed", cached_end
              
              # 完全一致 → キャッシュヒット
              return True, "Cache valid", None
          ```
          
        assigned_to: ashigaru1
        status: pending
        depends_on: [task_041_1]
        
      - task_id: task_041_3
        description: |
          差分計算（インクリメンタル更新）を実装せよ
          
          【変更ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          def precompute_incremental(
              self,
              prices: pl.DataFrame,
              start_from: datetime,
              config: dict[str, Any] | None = None,
          ) -> bool:
              """
              差分シグナル計算（増分データのみ）
              
              Args:
                  prices: 全価格データ（差分計算に必要な履歴を含む）
                  start_from: この日付以降のシグナルを計算
                  config: シグナル設定
              
              Returns:
                  True if successful
              """
              config = config or self.DEFAULT_CONFIG
              
              # 差分計算に必要な履歴期間を確保
              # 例: momentum_252 なら252日前からのデータが必要
              max_lookback = max(
                  max(config.get("momentum_periods", [252])),
                  max(config.get("volatility_periods", [60])),
                  max(config.get("sharpe_periods", [252])),
              )
              lookback_start = start_from - timedelta(days=max_lookback + 10)
              
              # 必要な期間のデータを抽出
              prices_subset = prices.filter(
                  pl.col("timestamp") >= lookback_start
              )
              
              # 各シグナルを差分計算
              for period in config.get("momentum_periods", []):
                  self._compute_and_append_momentum(
                      prices_subset, period, start_from
                  )
              # ... 他のシグナルも同様
              
              # メタデータ更新
              self._update_metadata_end_date(prices["timestamp"].max())
              
              return True
          
          def _compute_and_append_momentum(
              self,
              prices: pl.DataFrame,
              period: int,
              start_from: datetime,
          ) -> None:
              """
              既存キャッシュに差分を追記
              """
              # 新規シグナル計算
              new_signals = self._compute_momentum(prices, period)
              
              # start_from以降のみ抽出
              new_signals = new_signals.filter(
                  pl.col("timestamp") > start_from
              )
              
              # 既存Parquetに追記
              cache_path = self._cache_dir / f"momentum_{period}.parquet"
              if cache_path.exists():
                  existing = pl.read_parquet(cache_path)
                  combined = pl.concat([existing, new_signals]).unique(
                      subset=["timestamp", "ticker"]
                  )
                  combined.write_parquet(cache_path, compression="snappy")
              else:
                  new_signals.write_parquet(cache_path, compression="snappy")
          ```
          
        assigned_to: ashigaru2
        status: pending
        depends_on: [task_041_2]
        
      - task_id: task_041_4
        description: |
          precompute_all()をインクリメンタル対応に修正せよ
          
          【変更ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          def precompute_all(
              self,
              prices: pl.DataFrame,
              config: dict[str, Any] | None = None,
              force: bool = False,
          ) -> bool:
              """
              シグナル事前計算（インクリメンタル対応）
              
              Returns:
                  True if any computation was performed
              """
              config = config or self.DEFAULT_CONFIG
              
              if force:
                  logger.info("Force recomputation requested")
                  return self._full_precompute(prices, config)
              
              # インクリメンタルキャッシュ検証
              can_use, reason, incremental_start = self.validate_cache_incremental(
                  prices, config
              )
              
              if not can_use:
                  logger.info(f"Full recomputation needed: {reason}")
                  return self._full_precompute(prices, config)
              
              if incremental_start is not None:
                  logger.info(f"Incremental update from {incremental_start}")
                  return self.precompute_incremental(prices, incremental_start, config)
              
              logger.info("Cache is valid, no computation needed")
              return False
          ```
          
        assigned_to: ashigaru2
        status: pending
        depends_on: [task_041_3]
        
      - task_id: task_041_5
        description: |
          インクリメンタルキャッシュのテストを実装せよ
          
          【テスト内容】
          1. 初回計算 → 全計算実行
          2. 同一データ → キャッシュヒット（計算なし）
          3. 1日追加 → 差分計算のみ
          4. シグナル種類追加 → 全再計算
          5. パラメータ変更 → 全再計算
          6. 銘柄追加 → 全再計算
          7. 過去データ追加 → 全再計算
          
          【精度検証】
          - 差分計算結果が全計算結果と完全一致（diff < 1e-10）
          
          【パフォーマンス検証】
          - 1日追加時の計算時間 < 全計算の1%
          
        assigned_to: ashigaru3
        status: pending
        depends_on: [task_041_4]

    assignment_strategy: |
      【順次実行】依存関係あり
      1. task_041_1 → 足軽1号（メタデータ拡張）
      2. task_041_2 → 足軽1号（検証ロジック）
      3. task_041_3 → 足軽2号（差分計算）
      4. task_041_4 → 足軽2号（統合）
      5. task_041_5 → 足軽3号（テスト）

    expected_outcome: |
      【期待効果】
      | シナリオ | 現在 | 改善後 |
      |----------|------|--------|
      | 初回計算 | 12時間 | 12時間（変わらず） |
      | 同一データ再実行 | 12時間 | 0分（キャッシュヒット） |
      | 1日追加 | 12時間 | 数分（差分のみ） |
      | シグナル追加 | 12時間 | 12時間（全再計算・意図通り） |
      
      【キャッシュ無効化の正しい動作】
      - シグナル種類変更 → 全再計算 ✓
      - パラメータ変更 → 全再計算 ✓
      - 銘柄変更 → 全再計算 ✓
      - 期間延長（未来） → 差分計算 ✓ ← 新機能
      - 期間延長（過去） → 全再計算 ✓

  # cmd_041 追加要件: シグナル単位の差分計算
  - id: cmd_041_addendum
    timestamp: "2026-01-30T02:00:00"
    command: "cmd_041への追加要件: シグナル単位の差分計算"
    project: multi-asset-portfolio
    priority: high
    status: pending
    context: |
      殿より「実装を認める」との命あり。
      
      【調査結果】
      - シグナル間の依存関係を調査した結果、Level 0/1のシグナルは独立している
      - 内部統合（MeanReversionEnsemble等）は外部シグナルのParquetを参照しない
      - Level 2（レジーム適応）はバックテスト実行時に動的計算
      
      【追加要件】
      cmd_041に以下の機能を追加せよ:
      
      1. シグナル単位の差分計算
         - 新規シグナル追加時は、そのシグナルのみ計算
         - 既存シグナルはキャッシュを再利用
         - 全再計算は不要
      
      2. 実装方針
         ```python
         def validate_cache_incremental(...):
             # 既存の検証に加えて、シグナル単位の差分検知
             cached_signals = set(self._metadata.get("signals", []))
             current_signals = set(self._get_required_signals(config))
             
             missing_signals = current_signals - cached_signals
             removed_signals = cached_signals - current_signals  # 削除は無視可
             
             if missing_signals:
                 return True, "New signals added", None, list(missing_signals)
             
             # 期間延長チェック（既存ロジック）
             ...
         
         def precompute_missing_signals(
             self,
             prices: pl.DataFrame,
             missing_signals: list[str],
             config: dict[str, Any],
         ) -> bool:
             """新規シグナルのみ計算"""
             for signal_name in missing_signals:
                 self._compute_signal(prices, signal_name, config)
             
             # メタデータ更新（シグナルリスト追加）
             self._update_metadata_signals(missing_signals)
             return True
         ```
      
      【期待効果】
      | シナリオ | 現在の設計 | 追加後 |
      |----------|-----------|--------|
      | 1シグナル追加 | 全再計算（12時間） | 1シグナルのみ（数分） |
      | パラメータ変更 | 全再計算 | 全再計算（意図通り） |
      
      【注意事項】
      - シグナル間に依存関係がないことは調査済み（Level 0/1は独立）
      - signal_registry_hashの不一致で即全再計算するのではなく、差分を検知

    tasks:
      - task_id: task_041_1a
        description: |
          task_041_2（インクリメンタルキャッシュ検証）に以下を追加せよ
          
          【追加実装】
          validate_cache_incremental()の戻り値を拡張:
          ```python
          def validate_cache_incremental(
              self,
              prices: pl.DataFrame,
              config: dict[str, Any] | None = None,
          ) -> tuple[bool, str, datetime | None, list[str] | None]:
              """
              Returns:
                  (can_use_cache, reason, incremental_start, missing_signals)
                  - missing_signals: 新規追加されたシグナル名のリスト
              """
              ...
              # シグナル差分検知
              cached_signals = set(self._metadata.get("signals", []))
              current_signals = set(self._get_required_signals(config))
              
              missing_signals = current_signals - cached_signals
              
              if missing_signals:
                  logger.info(f"New signals detected: {missing_signals}")
                  return True, "New signals added", None, list(missing_signals)
              
              # 既存の期間チェックロジック
              ...
          ```
          
        assigned_to: ashigaru1
        status: pending
        
      - task_id: task_041_3a
        description: |
          task_041_3（差分計算）に以下を追加せよ
          
          【追加実装】
          新規シグナルのみ計算するメソッド:
          ```python
          def precompute_missing_signals(
              self,
              prices: pl.DataFrame,
              missing_signals: list[str],
              config: dict[str, Any] | None = None,
          ) -> bool:
              """
              新規追加されたシグナルのみ計算
              
              既存シグナルのキャッシュは維持したまま、
              新規シグナルのみをParquetに追加
              """
              config = config or self.DEFAULT_CONFIG
              
              for signal_name in missing_signals:
                  logger.info(f"Computing new signal: {signal_name}")
                  self._compute_and_save_signal(prices, signal_name, config)
              
              # メタデータ更新（シグナルリスト追加）
              current_signals = self._metadata.get("signals", [])
              updated_signals = list(set(current_signals) | set(missing_signals))
              self._metadata["signals"] = updated_signals
              
              # signal_registry_hashも更新
              self._metadata["signal_registry_hash"] = self._compute_signal_registry_hash()
              self._save_metadata()
              
              return True
          ```
          
        assigned_to: ashigaru2
        status: pending
        
      - task_id: task_041_4a
        description: |
          task_041_4（precompute_all統合）を更新せよ
          
          【更新実装】
          ```python
          def precompute_all(...) -> bool:
              ...
              # インクリメンタルキャッシュ検証（拡張版）
              can_use, reason, incremental_start, missing_signals = \
                  self.validate_cache_incremental(prices, config)
              
              if not can_use:
                  logger.info(f"Full recomputation needed: {reason}")
                  return self._full_precompute(prices, config)
              
              # 新規シグナルのみ計算
              if missing_signals:
                  logger.info(f"Computing {len(missing_signals)} new signals")
                  return self.precompute_missing_signals(prices, missing_signals, config)
              
              # 期間延長のみ
              if incremental_start is not None:
                  logger.info(f"Incremental update from {incremental_start}")
                  return self.precompute_incremental(prices, incremental_start, config)
              
              logger.info("Cache is valid, no computation needed")
              return False
          ```
          
        assigned_to: ashigaru2
        status: pending

    expected_outcome: |
      【最終的なキャッシュ動作】
      | シナリオ | 動作 | 計算時間 |
      |----------|------|---------|
      | 初回 | 全計算 | 12時間 |
      | 同一データ | キャッシュヒット | 0分 |
      | 1日追加 | 差分計算（期間） | 数分 |
      | 1シグナル追加 | 差分計算（シグナル） | 数分 |
      | パラメータ変更 | 全再計算 | 12時間 |
      | 銘柄変更 | 全再計算 | 12時間 |

  # cmd_041 追加要件2: 銘柄追加時の差分計算（独立系シグナルのみ）
  - id: cmd_041_addendum2
    timestamp: "2026-01-30T02:15:00"
    command: "cmd_041への追加要件2: 銘柄追加時の差分計算"
    project: multi-asset-portfolio
    priority: high
    status: pending
    context: |
      殿より「独立系シグナルのみキャッシュ再利用する設計で進めよ」との命あり。
      
      【調査結果】
      シグナルは2種類に分類される:
      
      1. 独立系シグナル（他銘柄に依存しない）
         - momentum_*, rsi_*, volatility_*, zscore_*, sharpe_*
         - 各銘柄の価格データのみで計算
         - 銘柄追加時: 新規銘柄のみ計算、既存銘柄はキャッシュ再利用可
      
      2. 相対系シグナル（他銘柄に依存する）
         - sector_relative_*, cross_asset_*, momentum_factor
         - 全銘柄の相対順位・ベンチマーク比較で計算
         - 銘柄追加時: 全銘柄再計算必要
      
      【実装方針】
      銘柄追加時:
      - 独立系シグナル → 新規銘柄のみ計算（既存キャッシュ再利用）
      - 相対系シグナル → 全銘柄再計算

    tasks:
      - task_id: task_041_5a
        description: |
          シグナルを独立系・相対系に分類する機構を実装せよ
          
          【実装ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          # シグナル分類定義
          INDEPENDENT_SIGNALS = {
              # 各銘柄の価格データのみで計算（他銘柄に依存しない）
              "momentum_*",
              "rsi_*", 
              "volatility_*",
              "zscore_*",
              "sharpe_*",
              "atr_*",
              "bollinger_*",
              "stochastic_*",
              "breakout_*",
              "donchian_*",
          }
          
          RELATIVE_SIGNALS = {
              # 他銘柄との比較・ランキングで計算
              "sector_relative_*",
              "cross_asset_*",
              "momentum_factor",
              "sector_momentum",
              "sector_breadth",
              "market_breadth",
              "ranking_*",
          }
          
          def classify_signal(self, signal_name: str) -> str:
              """
              シグナルを分類
              
              Returns:
                  "independent" or "relative"
              """
              for pattern in INDEPENDENT_SIGNALS:
                  if fnmatch.fnmatch(signal_name, pattern):
                      return "independent"
              
              for pattern in RELATIVE_SIGNALS:
                  if fnmatch.fnmatch(signal_name, pattern):
                      return "relative"
              
              # デフォルトは安全側（相対系扱い）
              logger.warning(f"Unknown signal type: {signal_name}, treating as relative")
              return "relative"
          ```
          
        assigned_to: ashigaru1
        status: pending
        
      - task_id: task_041_6a
        description: |
          銘柄追加時の差分計算ロジックを実装せよ
          
          【実装ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          def precompute_for_new_tickers(
              self,
              prices: pl.DataFrame,
              new_tickers: list[str],
              config: dict[str, Any] | None = None,
          ) -> bool:
              """
              新規銘柄追加時の差分計算
              
              - 独立系シグナル: 新規銘柄のみ計算、既存キャッシュに追記
              - 相対系シグナル: 全銘柄再計算
              """
              config = config or self.DEFAULT_CONFIG
              all_signals = self._get_all_signal_names(config)
              
              for signal_name in all_signals:
                  signal_type = self.classify_signal(signal_name)
                  
                  if signal_type == "independent":
                      # 新規銘柄のみ計算してキャッシュに追記
                      logger.info(f"Computing {signal_name} for new tickers only: {new_tickers}")
                      new_ticker_prices = prices.filter(
                          pl.col("ticker").is_in(new_tickers)
                      )
                      self._compute_and_append_signal(
                          new_ticker_prices, signal_name, config
                      )
                  else:
                      # 全銘柄再計算
                      logger.info(f"Recomputing {signal_name} for all tickers (relative signal)")
                      self._compute_and_save_signal(prices, signal_name, config)
              
              # メタデータ更新
              self._update_metadata_tickers(
                  prices["ticker"].unique().to_list()
              )
              
              return True
          
          def _compute_and_append_signal(
              self,
              prices: pl.DataFrame,
              signal_name: str,
              config: dict[str, Any],
          ) -> None:
              """
              既存Parquetに新規銘柄のシグナルを追記
              """
              # 新規シグナル計算
              new_signals = self._compute_signal(prices, signal_name, config)
              
              # 既存Parquetに追記
              cache_path = self._cache_dir / f"{signal_name}.parquet"
              if cache_path.exists():
                  existing = pl.read_parquet(cache_path)
                  combined = pl.concat([existing, new_signals]).unique(
                      subset=["timestamp", "ticker"]
                  )
                  combined.write_parquet(cache_path, compression="snappy")
              else:
                  new_signals.write_parquet(cache_path, compression="snappy")
          ```
          
        assigned_to: ashigaru2
        status: pending
        depends_on: [task_041_5a]
        
      - task_id: task_041_7a
        description: |
          validate_cache_incremental()を銘柄差分対応に拡張せよ
          
          【実装ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          validate_cache_incremental()の戻り値をさらに拡張:
          ```python
          @dataclass
          class CacheValidationResult:
              can_use_cache: bool
              reason: str
              incremental_start: datetime | None = None
              missing_signals: list[str] | None = None
              new_tickers: list[str] | None = None  # 追加
              
          def validate_cache_incremental(
              self,
              prices: pl.DataFrame,
              config: dict[str, Any] | None = None,
          ) -> CacheValidationResult:
              """
              インクリメンタルキャッシュ検証（完全版）
              """
              ...
              # 銘柄差分検知
              cached_tickers = set(self._metadata.get("tickers", []))
              current_tickers = set(prices["ticker"].unique().to_list())
              
              new_tickers = current_tickers - cached_tickers
              removed_tickers = cached_tickers - current_tickers
              
              # 銘柄削除のみの場合はキャッシュ有効（計算不要）
              if removed_tickers and not new_tickers:
                  return CacheValidationResult(
                      can_use_cache=True,
                      reason="Tickers removed only (cache still valid)",
                  )
              
              # 新規銘柄追加の場合
              if new_tickers:
                  return CacheValidationResult(
                      can_use_cache=True,
                      reason=f"New tickers added: {len(new_tickers)}",
                      new_tickers=list(new_tickers),
                  )
              
              # 既存のチェック（シグナル追加、期間延長等）
              ...
          ```
          
        assigned_to: ashigaru1
        status: pending
        depends_on: [task_041_5a]
        
      - task_id: task_041_8a
        description: |
          precompute_all()を銘柄差分対応に更新せよ
          
          【実装ファイル】
          src/backtest/signal_precompute.py
          
          【実装内容】
          ```python
          def precompute_all(...) -> bool:
              ...
              result = self.validate_cache_incremental(prices, config)
              
              if not result.can_use_cache:
                  logger.info(f"Full recomputation needed: {result.reason}")
                  return self._full_precompute(prices, config)
              
              # 新規銘柄追加
              if result.new_tickers:
                  logger.info(f"Computing for {len(result.new_tickers)} new tickers")
                  return self.precompute_for_new_tickers(
                      prices, result.new_tickers, config
                  )
              
              # 新規シグナル追加
              if result.missing_signals:
                  logger.info(f"Computing {len(result.missing_signals)} new signals")
                  return self.precompute_missing_signals(
                      prices, result.missing_signals, config
                  )
              
              # 期間延長
              if result.incremental_start is not None:
                  logger.info(f"Incremental update from {result.incremental_start}")
                  return self.precompute_incremental(
                      prices, result.incremental_start, config
                  )
              
              logger.info("Cache is valid, no computation needed")
              return False
          ```
          
        assigned_to: ashigaru2
        status: pending
        depends_on: [task_041_6a, task_041_7a]

    expected_outcome: |
      【最終的なキャッシュ動作（完全版）】
      
      | シナリオ | 独立系シグナル | 相対系シグナル | 計算時間 |
      |----------|---------------|---------------|---------|
      | 初回 | 全計算 | 全計算 | 12時間 |
      | 同一データ | キャッシュ | キャッシュ | 0分 |
      | 1日追加 | 差分計算 | 差分計算 | 数分 |
      | 1シグナル追加 | 新規のみ | 新規のみ | 数分 |
      | 1銘柄追加 | **新規銘柄のみ** | **全銘柄再計算** | 中程度 |
      | 銘柄削除 | 計算不要 | 計算不要 | 0分 |
      | パラメータ変更 | 全再計算 | 全再計算 | 12時間 |
      
      【シグナル分類】
      - 独立系: momentum, rsi, volatility, zscore, sharpe, atr, bollinger等
      - 相対系: sector_relative, cross_asset, momentum_factor, ranking等

  - id: cmd_042
    timestamp: "2026-01-30T10:00:00"
    command: "学術的シグナル実装Phase1（無料データソース）"
    project: multi-asset-portfolio
    priority: high
    status: pending
    context: |
      殿より、学術的に実績のあるシグナルの調査・実装を命じられた。
      
      【背景】
      コードベースには46シグナルが実装済みだが、学術的に確立された
      いくつかの重要なシグナルが未実装である。
      
      【Phase1: 無料データで実装可能なシグナル】
      以下5シグナルは既存のyfinance価格データ＋無料公開APIで実装可能。
      
      1. Lead-Lag関係シグナル（Oxford研究: 年率20%+）
      2. 52週高値モメンタム（George & Hwang 2004）
      3. 短期リバーサル（Jegadeesh 1990）
      4. インサイダー取引シグナル（SEC EDGAR API: 無料）
      5. ショートインタレストシグナル（FINRA API: 無料）
      
      【Phase2以降（TODO）】
      有料データ（$19/月〜）が必要なシグナルは別途実装予定:
      - PEAD（決算ドリフト）
      - Accruals（会計発生高）
      - Asset Growth（資産成長）
      - Net Issuance（株式発行）
      - Gross Profitability（売上総利益率）
      - Option IV Skew（オプションスキュー）
      
      詳細はdocs/TODO_signals.mdを参照せよ。
      
    tasks:
      - task_id: task_042_1
        description: |
          LeadLagSignalを実装せよ
          
          【学術的根拠】
          - Oxford大学研究: 年率20%+のリターン
          - 流動性の高い銘柄が小型株をリードする傾向
          - Leader銘柄の情報でFollower銘柄の予測精度60%達成
          
          【実装ファイル】
          src/signals/lead_lag.py（新規作成）
          
          【実装内容】
          ```python
          class LeadLagSignal(SignalBase):
              """
              Lead-Lag関係シグナル
              
              銘柄間の時間差相関を検出し、Leader銘柄の動きから
              Follower銘柄の将来リターンを予測する。
              
              学術的根拠:
              - Frazzini & Pedersen, Oxford研究
              - 年率20%+のバックテストリターン
              """
              
              signal_name = "lead_lag"
              
              def __init__(
                  self,
                  lookback: int = 60,          # 相関計算期間
                  lag_range: tuple = (1, 5),   # 検証するラグ日数
                  min_correlation: float = 0.3, # 最小相関閾値
                  top_n_leaders: int = 5,      # 参照するLeader数
              ):
                  self.lookback = lookback
                  self.lag_range = lag_range
                  self.min_correlation = min_correlation
                  self.top_n_leaders = top_n_leaders
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  1. 各銘柄ペアの時間ラグ付き相関を計算
                  2. 統計的に有意なLead-Lag関係を特定
                  3. Follower銘柄にLeader銘柄の方向性シグナルを付与
                  """
                  # 日次リターン計算
                  returns = self._compute_returns(prices)
                  
                  # 時間ラグ付きクロス相関行列
                  lag_correlations = self._compute_lagged_correlations(
                      returns, self.lag_range
                  )
                  
                  # Leader-Follower関係の特定
                  lead_lag_pairs = self._identify_lead_lag_pairs(
                      lag_correlations, self.min_correlation
                  )
                  
                  # Follower銘柄へのシグナル伝播
                  signals = self._propagate_signals(
                      returns, lead_lag_pairs, self.top_n_leaders
                  )
                  
                  return signals
              
              def _compute_lagged_correlations(
                  self,
                  returns: pl.DataFrame,
                  lag_range: tuple,
              ) -> dict:
                  """時間ラグ付き相関を計算"""
                  correlations = {}
                  tickers = returns["ticker"].unique().to_list()
                  
                  for lag in range(lag_range[0], lag_range[1] + 1):
                      for leader in tickers:
                          leader_returns = returns.filter(
                              pl.col("ticker") == leader
                          )["return"].shift(lag)
                          
                          for follower in tickers:
                              if leader == follower:
                                  continue
                              follower_returns = returns.filter(
                                  pl.col("ticker") == follower
                              )["return"]
                              
                              corr = self._correlation(
                                  leader_returns, follower_returns
                              )
                              correlations[(leader, follower, lag)] = corr
                  
                  return correlations
          ```
          
          【パラメータ範囲】
          - lookback: [30, 60, 90, 120]
          - lag_range: [(1, 3), (1, 5), (2, 5)]
          - min_correlation: [0.2, 0.3, 0.4]
          
          【シグナル分類】
          - 種別: "relative"（銘柄追加時は再計算必要）
          
          【テスト】
          tests/signals/test_lead_lag.py を作成し、以下を検証:
          - 既知のLead-Lag関係の検出
          - 計算精度（相関係数の正確性）
          - エッジケース（データ不足時）
          
        priority: high
        assigned_to: ashigaru1
        
      - task_id: task_042_2
        description: |
          FiftyTwoWeekHighMomentumSignalを実装せよ
          
          【学術的根拠】
          - George & Hwang (2004) "The 52-Week High and Momentum Investing"
          - Journal of Finance掲載
          - 従来のモメンタムより予測力が高い
          - 長期リバーサルが発生しない
          
          【実装ファイル】
          src/signals/fifty_two_week_high.py（新規作成）
          
          【実装内容】
          ```python
          class FiftyTwoWeekHighMomentumSignal(SignalBase):
              """
              52週高値モメンタムシグナル
              
              現在価格の52週高値への近接度を測定。
              高値に近い銘柄は上昇継続、遠い銘柄は下落継続の傾向。
              
              学術的根拠:
              - George & Hwang (2004), Journal of Finance
              - アンカリングバイアスに基づく行動ファイナンス理論
              """
              
              signal_name = "fifty_two_week_high_momentum"
              
              def __init__(
                  self,
                  lookback: int = 252,  # 52週 = 252営業日
                  smoothing: int = 5,   # スムージング期間
              ):
                  self.lookback = lookback
                  self.smoothing = smoothing
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  シグナル = 現在価格 / 52週高値
                  
                  - 1.0に近い: 強気シグナル（高値圏）
                  - 0.5以下: 弱気シグナル（高値から乖離）
                  """
                  signals = prices.group_by("ticker").map_groups(
                      lambda df: df.with_columns([
                          (pl.col("close") / pl.col("high").rolling_max(self.lookback))
                          .alias("raw_signal"),
                      ]).with_columns([
                          pl.col("raw_signal")
                          .rolling_mean(self.smoothing)
                          .alias("signal"),
                      ])
                  )
                  
                  # [-1, 1]に正規化
                  signals = signals.with_columns([
                      ((pl.col("signal") - 0.5) * 2).clip(-1, 1).alias("signal")
                  ])
                  
                  return signals.select(["timestamp", "ticker", "signal"])
          ```
          
          【パラメータ範囲】
          - lookback: [126, 189, 252]（6ヶ月、9ヶ月、12ヶ月）
          - smoothing: [1, 3, 5, 10]
          
          【シグナル分類】
          - 種別: "independent"（銘柄追加時は新規銘柄のみ計算）
          
        priority: high
        assigned_to: ashigaru2
        
      - task_id: task_042_3
        description: |
          ShortTermReversalSignalを実装せよ
          
          【学術的根拠】
          - Jegadeesh (1990) "Evidence of Predictable Behavior"
          - 短期（週次/月次）のリターンは反転する傾向
          - マーケットメイカーの在庫調整・流動性供給による
          
          【実装ファイル】
          src/signals/short_term_reversal.py（新規作成）
          
          【実装内容】
          ```python
          class ShortTermReversalSignal(SignalBase):
              """
              短期リバーサルシグナル
              
              直近の勝者は売り、敗者は買いの逆張り戦略。
              流動性供給者のリターンに相当。
              
              学術的根拠:
              - Jegadeesh (1990)
              - Nagel (2012) - 流動性供給リターンとの関連
              
              注意:
              - 取引コストが高いため、大型株に限定推奨
              - 月次より週次の方が効果が強い
              """
              
              signal_name = "short_term_reversal"
              
              def __init__(
                  self,
                  lookback: int = 5,           # リターン計算期間（日）
                  holding_period: int = 5,     # 保有期間（日）
                  use_volume_weight: bool = True,  # 出来高加重
              ):
                  self.lookback = lookback
                  self.holding_period = holding_period
                  self.use_volume_weight = use_volume_weight
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  シグナル = -1 * 過去Nリターン（正規化）
                  
                  - 過去の勝者 → 負のシグナル（売り）
                  - 過去の敗者 → 正のシグナル（買い）
                  """
                  signals = prices.group_by("ticker").map_groups(
                      lambda df: df.with_columns([
                          # 過去リターン
                          (pl.col("close") / pl.col("close").shift(self.lookback) - 1)
                          .alias("past_return"),
                      ])
                  )
                  
                  # クロスセクショナル正規化（Zスコア）
                  signals = signals.group_by("timestamp").map_groups(
                      lambda df: df.with_columns([
                          ((pl.col("past_return") - pl.col("past_return").mean()) 
                           / pl.col("past_return").std())
                          .alias("zscore"),
                      ])
                  )
                  
                  # 反転（負の相関を利用）
                  signals = signals.with_columns([
                      (-1 * pl.col("zscore")).clip(-3, 3).alias("signal")
                  ])
                  
                  # 出来高加重（オプション）
                  if self.use_volume_weight:
                      signals = signals.with_columns([
                          (pl.col("signal") * pl.col("volume").log())
                          .alias("signal")
                      ])
                  
                  return signals.select(["timestamp", "ticker", "signal"])
          ```
          
          【パラメータ範囲】
          - lookback: [3, 5, 10, 20]
          - use_volume_weight: [True, False]
          
          【シグナル分類】
          - 種別: "relative"（クロスセクショナル正規化のため）
          
        priority: high
        assigned_to: ashigaru3
        
      - task_id: task_042_4
        description: |
          InsiderTradingSignalを実装せよ
          
          【学術的根拠】
          - Seyhun研究: インサイダー購入は月次50bp+の異常リターン
          - Lorie & Niederhoffer (1968): 最初の学術的実証
          - 購入シグナルは有効、売却シグナルは弱い
          
          【データソース】
          SEC EDGAR API（完全無料、認証不要）
          - エンドポイント: https://data.sec.gov/submissions/CIK{cik}.json
          - Form 4（インサイダー取引報告）を解析
          
          【実装ファイル】
          src/signals/insider_trading.py（新規作成）
          src/data/sec_edgar.py（新規作成 - データ取得）
          
          【実装内容】
          ```python
          # src/data/sec_edgar.py
          class SECEdgarClient:
              """SEC EDGAR APIクライアント"""
              
              BASE_URL = "https://data.sec.gov"
              
              def __init__(self, user_agent: str = "YourApp/1.0"):
                  self.session = requests.Session()
                  self.session.headers["User-Agent"] = user_agent
              
              def get_insider_transactions(
                  self,
                  cik: str,
                  start_date: datetime | None = None,
              ) -> list[dict]:
                  """
                  Form 4からインサイダー取引を取得
                  
                  Returns:
                      list of {
                          "date": datetime,
                          "insider_name": str,
                          "title": str,  # CEO, CFO, Director等
                          "transaction_type": str,  # P=Purchase, S=Sale
                          "shares": int,
                          "price": float,
                          "value": float,
                      }
                  """
                  url = f"{self.BASE_URL}/submissions/CIK{cik.zfill(10)}.json"
                  response = self.session.get(url)
                  data = response.json()
                  
                  # Form 4をフィルタ
                  filings = data.get("filings", {}).get("recent", {})
                  form4_indices = [
                      i for i, form in enumerate(filings.get("form", []))
                      if form == "4"
                  ]
                  
                  transactions = []
                  for i in form4_indices:
                      # 各Form 4の詳細を取得・解析
                      ...
                  
                  return transactions
          
          # src/signals/insider_trading.py
          class InsiderTradingSignal(SignalBase):
              """
              インサイダー取引シグナル
              
              企業内部者の売買行動から将来リターンを予測。
              購入は強い買いシグナル、売却は弱い売りシグナル。
              
              学術的根拠:
              - Seyhun研究: 月次50bp+の異常リターン
              - 購入シグナルの方が売却より有効
              """
              
              signal_name = "insider_trading"
              
              def __init__(
                  self,
                  lookback_days: int = 90,     # 集計期間
                  min_transactions: int = 2,   # 最小取引数
                  weight_by_value: bool = True, # 金額加重
                  executive_only: bool = False, # 役員のみ
              ):
                  self.lookback_days = lookback_days
                  self.min_transactions = min_transactions
                  self.weight_by_value = weight_by_value
                  self.executive_only = executive_only
                  self.sec_client = SECEdgarClient()
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  シグナル計算:
                  1. 各銘柄のインサイダー取引を取得
                  2. Net Purchase Ratio = (購入額 - 売却額) / 総取引額
                  3. [-1, 1]にクリップ
                  """
                  ...
          ```
          
          【パラメータ範囲】
          - lookback_days: [30, 60, 90, 180]
          - min_transactions: [1, 2, 3]
          - executive_only: [True, False]
          
          【シグナル分類】
          - 種別: "independent"
          
          【注意事項】
          - SEC APIはレート制限あり（10 req/sec推奨）
          - User-Agentヘッダー必須
          - CIK（Central Index Key）とティッカーのマッピング必要
          
        priority: medium
        assigned_to: ashigaru4
        
      - task_id: task_042_5
        description: |
          ShortInterestSignalを実装せよ
          
          【学術的根拠】
          - Rapach et al. (2016): アグリゲートショートインタレストは市場リターン予測
          - Boehmer et al. (2008): 銘柄レベルでも有効
          - 高ショートインタレスト → 将来の低リターン
          
          【データソース】
          FINRA API（完全無料）
          - エンドポイント: https://api.finra.org/data/group/otcMarket/name/EquityShortInterest
          - 2週間ごとに更新
          - 2014年〜アーカイブ利用可能
          
          【実装ファイル】
          src/signals/short_interest.py（新規作成）
          src/data/finra.py（新規作成 - データ取得）
          
          【実装内容】
          ```python
          # src/data/finra.py
          class FINRAClient:
              """FINRA APIクライアント"""
              
              BASE_URL = "https://api.finra.org/data/group/otcMarket/name/EquityShortInterest"
              
              def get_short_interest(
                  self,
                  symbol: str | None = None,
                  start_date: datetime | None = None,
                  end_date: datetime | None = None,
              ) -> pl.DataFrame:
                  """
                  ショートインタレストデータを取得
                  
                  Returns:
                      DataFrame with columns:
                      - settlementDate
                      - symbol
                      - shortInterest (shares)
                      - avgDailyShareVolume
                      - daysToCover
                  """
                  payload = {
                      "fields": [
                          "settlementDate",
                          "symbolCode", 
                          "currentShortPositionQuantity",
                          "averageDailyVolumeQuantity",
                          "daysToCoverQuantity",
                      ],
                      "dateRangeFilters": [{
                          "fieldName": "settlementDate",
                          "startDate": start_date.isoformat(),
                          "endDate": end_date.isoformat(),
                      }],
                  }
                  
                  if symbol:
                      payload["domainFilters"] = [{
                          "fieldName": "symbolCode",
                          "values": [symbol],
                      }]
                  
                  response = requests.post(self.BASE_URL, json=payload)
                  return pl.DataFrame(response.json())
          
          # src/signals/short_interest.py
          class ShortInterestSignal(SignalBase):
              """
              ショートインタレストシグナル
              
              空売り残高比率から将来リターンを予測。
              高ショートインタレスト = 弱気シグナル。
              
              学術的根拠:
              - Rapach et al. (2016)
              - 年率14.6%のリスク調整済みリターン
              
              メトリクス:
              - Short Interest Ratio (SIR) = 空売り株数 / 発行済株式数
              - Days to Cover (DTC) = 空売り株数 / 平均出来高
              """
              
              signal_name = "short_interest"
              
              def __init__(
                  self,
                  metric: str = "days_to_cover",  # "sir" or "days_to_cover"
                  lookback: int = 30,             # 変化率計算期間
                  use_change: bool = True,        # 水準 vs 変化率
              ):
                  self.metric = metric
                  self.lookback = lookback
                  self.use_change = use_change
                  self.finra_client = FINRAClient()
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  シグナル = -1 * (SIR or DTC)  # 高SI = 売りシグナル
                  
                  use_change=Trueの場合:
                  シグナル = -1 * (現在SI - N日前SI)
                  """
                  ...
          ```
          
          【パラメータ範囲】
          - metric: ["sir", "days_to_cover"]
          - lookback: [14, 30, 60]
          - use_change: [True, False]
          
          【シグナル分類】
          - 種別: "independent"
          
          【注意事項】
          - FINRAデータは2週間遅延
          - OTC銘柄とNASDAQ/NYSEで別エンドポイントの可能性
          
        priority: medium
        assigned_to: ashigaru5
        
      - task_id: task_042_6
        description: |
          シグナルレジストリへの登録と統合テスト
          
          【実装内容】
          1. src/signals/registry.pyに5つの新シグナルを登録
          2. 統合テストの作成
          3. バックテストでの動作確認
          
          【登録例】
          ```python
          # src/signals/registry.py に追加
          from src.signals.lead_lag import LeadLagSignal
          from src.signals.fifty_two_week_high import FiftyTwoWeekHighMomentumSignal
          from src.signals.short_term_reversal import ShortTermReversalSignal
          from src.signals.insider_trading import InsiderTradingSignal
          from src.signals.short_interest import ShortInterestSignal
          
          SIGNAL_REGISTRY.update({
              "lead_lag": LeadLagSignal,
              "fifty_two_week_high_momentum": FiftyTwoWeekHighMomentumSignal,
              "short_term_reversal": ShortTermReversalSignal,
              "insider_trading": InsiderTradingSignal,
              "short_interest": ShortInterestSignal,
          })
          ```
          
          【テスト項目】
          - 各シグナルの単体テスト
          - レジストリからの動的ロード
          - バックテストでのシグナル生成
          - キャッシュとの互換性（independent/relative分類）
          
        priority: high
        assigned_to: ashigaru6
        depends_on: [task_042_1, task_042_2, task_042_3, task_042_4, task_042_5]

    expected_outcome: |
      【新規シグナル5種】
      | シグナル | 学術的根拠 | データソース | 分類 |
      |---------|-----------|-------------|------|
      | lead_lag | Oxford研究 | yfinance | relative |
      | fifty_two_week_high_momentum | George & Hwang 2004 | yfinance | independent |
      | short_term_reversal | Jegadeesh 1990 | yfinance | relative |
      | insider_trading | Seyhun研究 | SEC EDGAR | independent |
      | short_interest | Rapach et al. 2016 | FINRA | independent |
      
      【コスト】
      全て無料データソースで実装。追加コストなし。
      
      【次フェーズへの準備】
      有料データシグナル（PEAD, Accruals等）はdocs/TODO_signals.mdに記録済み。

  - id: cmd_042_addendum
    timestamp: "2026-01-30T11:00:00"
    command: "cmd_042への追加要件: Lead-Lagシグナルのパフォーマンス最適化"
    project: multi-asset-portfolio
    priority: critical
    status: pending
    context: |
      殿より、Lead-Lag関係シグナルの計算コスト最適化を命じられた。
      
      【計算コスト分析】
      Lead-Lagは以下の理由で計算コストが非常に高い:
      - 資産ペア: O(n²) → 300銘柄で90,000ペア
      - 時間シフト: O(lag_max) → lag=5で5パターン
      - 相関計算: O(window) → window=60で60回
      
      ナイーブ実装では1回の計算に30-40秒要する。
      
      【最適化要件】
      以下の最適化を必須とする:
      1. Numba JIT並列化（10倍高速化）
      2. シグナルキャッシュ統合（増分更新対応）
      3. 計算量削減アルゴリズム
      
    tasks:
      - task_id: task_042_1_opt
        description: |
          task_042_1（LeadLagSignal）に以下の最適化を追加せよ
          
          【1. Numba JIT並列化】
          
          src/backtest/numba_compute.py に追加:
          ```python
          @njit(parallel=True, cache=True)
          def compute_lagged_correlations_numba(
              returns: np.ndarray,  # shape: (n_assets, n_days)
              lag_range: tuple,     # (min_lag, max_lag)
              window: int,
          ) -> np.ndarray:
              """
              全資産ペアの時間ラグ付き相関を並列計算
              
              Returns:
                  correlations: shape (n_assets, n_assets, n_lags)
              """
              n_assets, n_days = returns.shape
              n_lags = lag_range[1] - lag_range[0] + 1
              correlations = np.zeros((n_assets, n_assets, n_lags))
              
              # 外側ループを並列化
              for i in prange(n_assets):
                  for j in range(i + 1, n_assets):
                      for lag_idx, lag in enumerate(range(lag_range[0], lag_range[1] + 1)):
                          # 時間シフトを適用
                          if lag >= 0:
                              x = returns[i, lag:]
                              y = returns[j, :-lag] if lag > 0 else returns[j, :]
                          else:
                              x = returns[i, :lag]
                              y = returns[j, -lag:]
                          
                          # Spearman相関（ランク→Pearson）
                          corr = spearmanr_numba(x[-window:], y[-window:])
                          correlations[i, j, lag_idx] = corr
                          correlations[j, i, lag_idx] = corr  # 対称性
              
              return correlations
          ```
          
          【2. 計算量削減アルゴリズム】
          
          以下の最適化で計算量を1/10以下に削減:
          
          A. **段階的フィルタリング**:
          ```python
          def compute_with_early_stopping(self, returns, min_correlation=0.3):
              """
              Step 1: lag=0で相関を計算
              Step 2: 相関 > min_correlation のペアのみlag検索
              
              効果: 有意なペアが10%なら計算量90%削減
              """
              # Phase 1: lag=0の相関行列（高速）
              corr_lag0 = correlation_matrix(returns, window)
              
              # Phase 2: 有意ペアのみ詳細検索
              significant_pairs = np.where(np.abs(corr_lag0) > min_correlation)
              
              # Phase 3: 有意ペアのlag探索のみ実行
              for i, j in zip(*significant_pairs):
                  self._search_optimal_lag(returns, i, j, lag_range)
          ```
          
          B. **最適lagの効率的探索**:
          ```python
          def _search_optimal_lag(self, returns, i, j, lag_range):
              """
              二分探索的にピークを検出
              
              1. 粗いグリッド（lag=0,5,10,15,20）で概算
              2. ピーク周辺を詳細検索
              
              効果: lag検索が O(lag_max) → O(log(lag_max))
              """
              # 粗いグリッド
              coarse_lags = np.linspace(lag_range[0], lag_range[1], 5, dtype=int)
              coarse_corrs = [self._compute_corr(returns, i, j, lag) for lag in coarse_lags]
              
              # ピーク検出
              peak_idx = np.argmax(np.abs(coarse_corrs))
              
              # 詳細検索（ピーク周辺±5）
              fine_start = max(lag_range[0], coarse_lags[peak_idx] - 5)
              fine_end = min(lag_range[1], coarse_lags[peak_idx] + 5)
              
              best_lag, best_corr = self._fine_search(returns, i, j, fine_start, fine_end)
              return best_lag, best_corr
          ```
          
          【3. キャッシュ統合】
          
          src/backtest/signal_precompute.py に統合:
          ```python
          # シグナル分類に追加
          RELATIVE_SIGNALS = {
              "sector_relative_*", "cross_asset_*",
              "lead_lag",  # 追加: 銘柄追加時は再計算必要
          }
          
          class SignalPrecomputer:
              def precompute_lead_lag(
                  self,
                  prices: pl.DataFrame,
                  lag_range: tuple = (1, 5),
                  window: int = 60,
                  min_correlation: float = 0.3,
              ) -> pl.DataFrame:
                  """
                  Lead-Lag相関を事前計算・キャッシュ
                  
                  キャッシュ戦略:
                  1. 相関行列をParquetに保存
                  2. Leader-Followerペアリストを保存
                  3. 新期間追加時は増分更新
                  """
                  cache_path = self._cache_dir / "lead_lag_correlations.parquet"
                  pairs_path = self._cache_dir / "lead_lag_pairs.parquet"
                  
                  # キャッシュ検証
                  if self._is_cache_valid(cache_path, prices):
                      return pl.read_parquet(pairs_path)
                  
                  # 計算（最適化版）
                  returns = self._compute_returns(prices)
                  correlations = compute_lagged_correlations_numba(
                      returns.to_numpy(), lag_range, window
                  )
                  
                  # Leader-Followerペア抽出
                  pairs = self._extract_lead_lag_pairs(
                      correlations, min_correlation
                  )
                  
                  # キャッシュ保存
                  pairs.write_parquet(pairs_path, compression="snappy")
                  
                  return pairs
          ```
          
          【4. シグナル生成の効率化】
          
          ```python
          class LeadLagSignal(SignalBase):
              """最適化版Lead-Lagシグナル"""
              
              def __init__(
                  self,
                  lag_range: tuple = (1, 5),  # 短縮: (1,5)推奨
                  window: int = 60,
                  min_correlation: float = 0.3,
                  top_n_leaders: int = 3,     # 削減: 3推奨
                  use_precomputed: bool = True,  # キャッシュ使用
              ):
                  ...
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  """
                  事前計算済みのLead-Lagペアからシグナル生成
                  
                  計算量: O(n_assets × top_n_leaders) = O(n)
                  （事前計算済みの場合）
                  """
                  if self.use_precomputed:
                      # キャッシュからペア読み込み
                      pairs = self._load_lead_lag_pairs()
                      
                      # 各Followerに対してLeaderのシグナルを伝播
                      for follower, leaders in pairs.groupby("follower"):
                          leader_signals = self._aggregate_leader_signals(
                              prices, leaders, self.top_n_leaders
                          )
                          signals[follower] = leader_signals
                      
                      return signals
                  else:
                      # フルコンピュート（非推奨）
                      return self._full_compute(prices)
          ```
          
        priority: critical
        assigned_to: ashigaru1
        depends_on: []
        
      - task_id: task_042_bench
        description: |
          Lead-Lagシグナルのベンチマークテストを作成せよ
          
          【テスト項目】
          1. 計算時間計測
             - ナイーブ実装 vs Numba最適化
             - 100銘柄、300銘柄、500銘柄
          
          2. メモリ使用量計測
             - ピークメモリ
             - キャッシュサイズ
          
          3. 精度検証
             - ナイーブ vs 最適化の結果一致
             - 段階的フィルタリングの精度影響
          
          【ベンチマーク目標】
          | 銘柄数 | ナイーブ | 最適化 | 目標 |
          |--------|---------|--------|------|
          | 100 | ~5秒 | <0.5秒 | 10x |
          | 300 | ~40秒 | <4秒 | 10x |
          | 500 | ~120秒 | <12秒 | 10x |
          
          【テストファイル】
          tests/benchmarks/test_lead_lag_performance.py
          
        priority: high
        assigned_to: ashigaru2
        depends_on: [task_042_1_opt]

    expected_outcome: |
      【最適化効果】
      | 最適化 | 効果 | 実装難易度 |
      |--------|------|-----------|
      | Numba並列化 | 10倍高速化 | 低 |
      | 段階的フィルタリング | 5-10倍削減 | 中 |
      | 二分探索lag検索 | 2-3倍削減 | 中 |
      | キャッシュ統合 | 2回目以降0秒 | 低 |
      | **合計** | **50-100倍** | - |
      
      【推奨パラメータ】
      - lag_range: (1, 5) ← 学術的にも1-5日が有効
      - min_correlation: 0.3 ← 有意なペアに絞る
      - top_n_leaders: 3 ← 上位3銘柄で十分
      
      これにより、300銘柄での計算時間:
      40秒 → 0.5秒以下（初回）、0秒（キャッシュヒット時）

  - id: cmd_043
    timestamp: "2026-01-30T12:00:00"
    command: "スキル設計書作成（3スキル）"
    project: multi-agent-shogun
    priority: high
    status: pending
    context: |
      殿より、スキル化候補の設計書作成を命じられた。
      
      【スキル化の目的】
      足軽が繰り返し使用できる再利用可能なパターンを抽出し、
      今後の開発効率を向上させる。
      
      【対象スキル（優先度順）】
      1. numba-scipy-replacer（最優先）
      2. external-data-signal（高優先）
      3. sec_edgar_data_client（中優先）
      
    tasks:
      - task_id: task_043_1
        description: |
          numba-scipy-replacerスキル設計書を作成せよ
          
          【スキル概要】
          scipy関数をNumba JITで高速化する汎用パターン
          
          【設計書の内容】
          skills/numba-scipy-replacer.md を作成し、以下を記載:
          
          ```markdown
          # numba-scipy-replacer スキル
          
          ## 概要
          scipy.stats等の関数をNumba JITで高速化する手順。
          5-20倍の高速化を実現しつつ、精度を100%維持。
          
          ## 適用条件
          - scipy関数がボトルネックとなっている
          - 関数が純粋な数値計算（I/O、乱数なし）
          - ループ内で大量に呼び出される
          
          ## 実装手順
          
          ### Step 1: 対象関数の特定
          プロファイリングでボトルネックを特定。
          
          ### Step 2: Numba互換の実装
          ```python
          from numba import njit, prange
          
          @njit(cache=True)
          def target_function_numba(x, y):
              # scipy互換の実装
              # numpy関数のみ使用
              pass
          ```
          
          ### Step 3: 精度検証
          ```python
          def test_accuracy():
              # scipy版と結果比較
              scipy_result = scipy.stats.target_function(x, y)
              numba_result = target_function_numba(x, y)
              assert np.allclose(scipy_result, numba_result, rtol=1e-10)
          ```
          
          ### Step 4: ベンチマーク
          ```python
          def benchmark():
              # 高速化率を測定
              # 目標: 5倍以上
          ```
          
          ## 実装例
          
          ### spearmanr_numba
          [src/backtest/numba_compute.py:573-624 の内容を記載]
          
          ### rankdata_numba
          [src/backtest/numba_compute.py:537-569 の内容を記載]
          
          ## 注意事項
          - NaN/Inf の処理を明示的に行う
          - フォールバック実装を用意（Numba未インストール時）
          - `cache=True` でコンパイル時間を削減
          
          ## 適用可能な他の関数
          - scipy.stats.kendalltau
          - scipy.stats.pearsonr
          - scipy.signal.correlate
          - scipy.linalg.qr
          ```
          
        priority: critical
        assigned_to: ashigaru1
        
      - task_id: task_043_2
        description: |
          external-data-signalスキル設計書を作成せよ
          
          【スキル概要】
          外部API→キャッシュ→シグナル生成の汎用パターン
          
          【設計書の内容】
          skills/external-data-signal.md を作成し、以下を記載:
          
          ```markdown
          # external-data-signal スキル
          
          ## 概要
          外部APIからデータを取得し、キャッシュを経由して
          トレーディングシグナルに変換する汎用パターン。
          
          ## アーキテクチャ
          ```
          外部API → APIクライアント → キャッシュ層 → シグナル生成
                         ↓
                    レート制限
                    リトライ
                    エラーハンドリング
          ```
          
          ## 実装手順
          
          ### Step 1: APIクライアント作成
          ```python
          # src/data/{api_name}.py
          class ExternalAPIClient:
              BASE_URL = "https://api.example.com"
              RATE_LIMIT = 10  # requests per second
              
              def __init__(self, cache_dir: Path | None = None):
                  self.session = requests.Session()
                  self.cache_dir = cache_dir or Path("cache/external")
                  self._last_request_time = 0
              
              def _rate_limit(self):
                  elapsed = time.time() - self._last_request_time
                  if elapsed < 1.0 / self.RATE_LIMIT:
                      time.sleep(1.0 / self.RATE_LIMIT - elapsed)
                  self._last_request_time = time.time()
              
              def get_data(self, symbol: str, **params) -> pl.DataFrame:
                  # キャッシュチェック
                  cache_path = self._get_cache_path(symbol, params)
                  if cache_path.exists():
                      return pl.read_parquet(cache_path)
                  
                  # API呼び出し
                  self._rate_limit()
                  response = self.session.get(...)
                  
                  # キャッシュ保存
                  df = pl.DataFrame(response.json())
                  df.write_parquet(cache_path)
                  
                  return df
          ```
          
          ### Step 2: シグナルクラス作成
          ```python
          # src/signals/{signal_name}.py
          class ExternalDataSignal(SignalBase):
              signal_name = "external_data"
              
              def __init__(self, lookback: int = 30, ...):
                  self.client = ExternalAPIClient()
                  self.lookback = lookback
              
              def compute(self, prices: pl.DataFrame) -> pl.DataFrame:
                  # 外部データ取得
                  external_data = self._fetch_external_data(prices)
                  
                  # シグナル計算
                  signals = self._compute_signal(prices, external_data)
                  
                  # 正規化
                  signals = self._normalize(signals)
                  
                  return signals
              
              def _normalize(self, raw_signal: pl.Series) -> pl.Series:
                  # Z-score → tanh で [-1, 1] に圧縮
                  zscore = (raw_signal - raw_signal.mean()) / raw_signal.std()
                  return np.tanh(zscore / 2)
          ```
          
          ## 実装例
          
          ### FINRAClient + ShortInterestSignal
          [src/data/finra.py と src/signals/short_interest.py の内容を記載]
          
          ## 注意事項
          - レート制限を必ず実装
          - キャッシュの有効期限を設定
          - API障害時のフォールバック
          - 外部データ欠損時のハンドリング
          
          ## 適用可能な他のAPI
          - オプションデータAPI（IVolatility等）
          - 機関投資家ポジションAPI
          - 経済指標API（FRED等）
          - ニュースセンチメントAPI
          ```
          
        priority: high
        assigned_to: ashigaru2
        
      - task_id: task_043_3
        description: |
          sec_edgar_data_clientスキル設計書を作成せよ
          
          【スキル概要】
          SEC EDGAR APIからデータを取得する専門パターン
          
          【設計書の内容】
          skills/sec-edgar-data-client.md を作成し、以下を記載:
          
          ```markdown
          # sec-edgar-data-client スキル
          
          ## 概要
          SEC EDGAR APIを使用して企業の公開情報を取得する専門パターン。
          Form 4（インサイダー取引）、10-K、10-Q等に対応。
          
          ## SEC EDGAR API仕様
          - ベースURL: https://data.sec.gov
          - 認証: 不要（公開API）
          - レート制限: 10 requests/second（User-Agent必須）
          - データ形式: JSON/XML
          
          ## 実装手順
          
          ### Step 1: CIK-Tickerマッピング
          ```python
          class CIKMapper:
              MAPPING_URL = "https://www.sec.gov/files/company_tickers.json"
              
              def __init__(self):
                  self._mapping = {}
                  self._load_mapping()
              
              def get_cik(self, ticker: str) -> str:
                  return self._mapping.get(ticker.upper())
          ```
          
          ### Step 2: APIクライアント
          ```python
          class SECEdgarClient:
              BASE_URL = "https://data.sec.gov"
              
              def __init__(self, user_agent: str = "YourApp/1.0 (contact@example.com)"):
                  self.session = requests.Session()
                  self.session.headers["User-Agent"] = user_agent
                  self.cik_mapper = CIKMapper()
              
              def get_filings(self, ticker: str, form_type: str = None) -> list:
                  cik = self.cik_mapper.get_cik(ticker)
                  url = f"{self.BASE_URL}/submissions/CIK{cik.zfill(10)}.json"
                  response = self.session.get(url)
                  filings = response.json()["filings"]["recent"]
                  
                  if form_type:
                      filings = [f for f in filings if f["form"] == form_type]
                  
                  return filings
          ```
          
          ### Step 3: フォーム別パーサー
          ```python
          class Form4Parser:
              def parse(self, filing_url: str) -> list[InsiderTransaction]:
                  # XMLを解析してInsiderTransactionリストを返す
                  pass
          
          class Form10KParser:
              def parse(self, filing_url: str) -> dict:
                  # 財務諸表データを抽出
                  pass
          ```
          
          ## 実装例
          
          ### SECEdgarClient + InsiderTradingSignal
          [src/data/sec_edgar.py と src/signals/insider_trading.py の内容を記載]
          
          ## 対応フォーム一覧
          | フォーム | 内容 | 用途 |
          |---------|------|------|
          | Form 4 | インサイダー取引 | InsiderTradingSignal |
          | Form 3 | 初期保有報告 | 初期ポジション |
          | Form 5 | 年次報告 | 年次集計 |
          | 10-K | 年次報告書 | ファンダメンタル |
          | 10-Q | 四半期報告書 | ファンダメンタル |
          | 8-K | 臨時報告書 | イベント検知 |
          | Schedule 13D | 大量保有報告 | アクティビスト検知 |
          
          ## 注意事項
          - User-Agentヘッダー必須（SEC要件）
          - レート制限10req/sec厳守
          - XMLパース時のエラーハンドリング
          - CIKマッピングのキャッシュ
          ```
          
        priority: medium
        assigned_to: ashigaru3

    expected_outcome: |
      【成果物】
      skills/
      ├── numba-scipy-replacer.md  # 最優先
      ├── external-data-signal.md   # 高優先
      └── sec-edgar-data-client.md  # 中優先
      
      【効果】
      - 今後の開発で再利用可能なパターン集
      - 足軽の学習時間短縮（1-3日→数時間）
      - コード品質の標準化

  - id: cmd_044
    timestamp: "2026-01-30T02:30:00"
    command: "15年バックテスト全頻度実行（日次・週次・月次）"
    project: multi-asset-portfolio
    priority: high
    status: pending
    context: |
      殿が離席されるため、バックグラウンドで全頻度のバックテストを実行せよ。
      
      【実行スクリプト】
      scripts/run_all_backtests.py
      
      【実行順序】（計算コスト昇順）
      1. monthly（約180リバランス）- 最軽量
      2. weekly（約780リバランス）- 中程度
      3. daily（約3,900リバランス）- 最重量
      
      【再開機能】
      - 10リバランスごとにチェックポイント自動保存
      - 中断後、再実行で最新チェックポイントから自動再開
      - チェックポイント: results/checkpoints/{monthly,weekly,daily}/
      
      【設定】
      - 期間: 2010-01-01 〜 2025-01-01（15年）
      - 銘柄: 20 ETF
      - 初期資本: 100万ドル
      
      【既に実行開始済み】
      現在バックグラウンドで実行中（PID: 80525）
      ログ: results/backtest_all.log
      
    tasks:
      - task_id: task_044_1
        description: |
          バックテスト実行の監視と結果記録
          
          【監視方法】
          tail -f results/backtest_all.log
          
          【完了確認】
          results/backtest_{monthly,weekly,daily}_15y.json が生成されること
          
          【中断時の再開】
          python scripts/run_all_backtests.py
          （自動で最新チェックポイントから再開）
          
          【報告内容】
          - 各頻度のSharpe Ratio
          - 各頻度の年率リターン
          - 各頻度の最大ドローダウン
          - 実行時間
          
        priority: high
        assigned_to: ashigaru1

    expected_outcome: |
      【成果物】
      results/
      ├── backtest_monthly_15y.json
      ├── backtest_weekly_15y.json
      └── backtest_daily_15y.json
      
      【期待メトリクス】
      - Sharpe Ratio: > 0.8
      - 年率リターン: > 8%
      - 最大ドローダウン: < 25%

  - id: cmd_045
    timestamp: "2026-01-30T17:30:00"
    command: "S3キャッシュレイヤー統合（Phase 1-5）"
    project: multi-asset-portfolio
    priority: high
    status: pending
    context: |
      殿より承認を得た。S3キャッシュレイヤーの統合を実施せよ。
      
      【完了済み】
      - StorageBackend実装: src/utils/storage_backend.py
      - マイグレーションスクリプト: scripts/migrate_cache_to_s3.py
      - 既存キャッシュS3アップロード: 1884ファイル完了
      - S3バケット: stock-local-dev-014498665038
      
      【AWS認証情報】
      環境変数で設定すること:
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      ※コードに認証情報を含めるな
      
      【StorageBackend使用方法】
      ```python
      from src.utils.storage_backend import init_s3_backend, StorageConfig
      
      backend = init_s3_backend(
          bucket='stock-local-dev-014498665038',
          prefix='.cache',
          local_cache_path='/tmp/.backtest_cache',
      )
      
      # 透過的に操作
      backend.write_parquet(df, 'signals/momentum.parquet')
      df = backend.read_parquet('signals/momentum.parquet')
      ```
      
    tasks:
      # ========== Phase 1: 既存キャッシュクラス統合（3日） ==========
      - task_id: task_045_1
        description: |
          【SignalPrecomputer S3統合】
          
          src/backtest/signal_precompute.py を StorageBackend 対応に修正。
          
          【変更方針】
          1. Path操作を StorageBackend メソッドに置換
          2. 既存API（get_signal_at_date等）は維持
          3. 初期化時に storage_backend を受け取るオプション追加
          
          【修正箇所】
          - __init__: cache_dir → storage_backend オプション追加
          - _save_parquet: Path.write → backend.write_parquet
          - _load_parquet: Path.read → backend.read_parquet
          - _save_metadata: json.dump → backend.write_json
          - _load_metadata: json.load → backend.read_json
          
          【後方互換性】
          - cache_dir指定時は従来通りローカル動作
          - storage_backend指定時はS3動作
          
          【テスト】
          - 既存テストがパスすること
          - S3モードでの読み書きテスト追加
          
        priority: high
        assigned_to: ashigaru1

      - task_id: task_045_2
        description: |
          【CovarianceCache S3統合】
          
          src/backtest/covariance_cache.py を StorageBackend 対応に修正。
          
          【変更方針】
          1. Pickle保存を StorageBackend.write_pickle に置換
          2. 既存API（save_state, load_state等）は維持
          
          【修正箇所】
          - CovarianceCache.__init__: storage_backend オプション追加
          - save_state: pickle.dump → backend.write_pickle
          - load_state: pickle.load → backend.read_pickle
          
          【テスト】
          - 既存テストがパスすること
          - S3モードでの状態保存・復元テスト追加
          
        priority: high
        assigned_to: ashigaru2

      - task_id: task_045_3
        description: |
          【DataCache S3統合】
          
          src/data/cache.py を StorageBackend 対応に修正。
          
          【変更方針】
          1. Parquet保存を StorageBackend.write_parquet に置換
          2. 既存API（get, put等）は維持
          3. DuckDBバックエンドは変更なし（ローカル専用）
          
          【修正箇所】
          - DataCache.__init__: storage_backend オプション追加
          - _save_parquet: Path操作 → backend.write_parquet
          - _load_parquet: Path操作 → backend.read_parquet
          - _save_metadata: json → backend.write_json
          
          【テスト】
          - 既存テストがパスすること
          - S3モードでのキャッシュ読み書きテスト追加
          
        priority: high
        assigned_to: ashigaru3

      # ========== Phase 2: 設定ファイル対応（1日） ==========
      - task_id: task_045_4
        description: |
          【UnifiedCacheManager S3対応】
          
          src/utils/cache_manager.py にS3バックエンド設定を追加。
          
          【変更内容】
          1. StorageConfig を受け取るオプション追加
          2. 各キャッシュラッパーにstorage_backend伝播
          3. get_storage_backend() との連携
          
          【修正箇所】
          - UnifiedCacheManager.__init__: storage_config オプション
          - _create_signal_cache: storage_backend 渡し
          - _create_data_cache: storage_backend 渡し
          
        priority: medium
        assigned_to: ashigaru1
        depends_on: [task_045_1, task_045_2, task_045_3]

      - task_id: task_045_5
        description: |
          【settings.yaml ストレージ設定追加】
          
          config/settings.yaml に storage セクションを追加。
          
          【追加内容】
          ```yaml
          storage:
            backend: "local"  # "local" or "s3"
            s3_bucket: "stock-local-dev-014498665038"
            s3_prefix: ".cache"
            local_cache_enabled: true
            local_cache_path: "/tmp/.backtest_cache"
            local_cache_ttl_hours: 24
          ```
          
          【注意】
          - AWS認証情報は環境変数から取得（YAMLに含めない）
          - デフォルトは "local"（既存動作維持）
          
        priority: medium
        assigned_to: ashigaru2
        depends_on: [task_045_1, task_045_2, task_045_3]

      - task_id: task_045_6
        description: |
          【settings.py StorageConfig読み込み】
          
          src/config/settings.py に StorageConfig 読み込みを追加。
          
          【変更内容】
          1. storage セクションの読み込み
          2. StorageConfig オブジェクト生成
          3. Settings クラスに storage_config 属性追加
          
          【実装例】
          ```python
          from src.utils.storage_backend import StorageConfig
          
          class Settings:
              def __init__(self, ...):
                  ...
                  self.storage_config = self._load_storage_config()
              
              def _load_storage_config(self) -> StorageConfig:
                  storage = self._raw.get("storage", {})
                  return StorageConfig(
                      backend=storage.get("backend", "local"),
                      s3_bucket=storage.get("s3_bucket", ""),
                      ...
                  )
          ```
          
        priority: medium
        assigned_to: ashigaru3
        depends_on: [task_045_1, task_045_2, task_045_3]

      # ========== Phase 3: バックテストエンジン統合（2日） ==========
      - task_id: task_045_7
        description: |
          【FastBacktestEngine S3統合】
          
          src/backtest/fast_engine.py に StorageBackend 初期化を追加。
          
          【変更内容】
          1. FastBacktestConfig に storage_config オプション追加
          2. エンジン初期化時に StorageBackend 初期化
          3. SignalPrecomputer, CovarianceCache に伝播
          
        priority: high
        assigned_to: ashigaru1
        depends_on: [task_045_4, task_045_5, task_045_6]

      - task_id: task_045_8
        description: |
          【UnifiedExecutor S3統合】
          
          src/orchestrator/unified_executor.py に S3キャッシュ設定を追加。
          
          【変更内容】
          1. run_backtest に storage_config オプション追加
          2. Settings から storage_config を自動取得
          3. 内部で使用するキャッシュクラスに伝播
          
        priority: high
        assigned_to: ashigaru2
        depends_on: [task_045_4, task_045_5, task_045_6]

      - task_id: task_045_9
        description: |
          【run_all_backtests.py S3対応】
          
          scripts/run_all_backtests.py に S3モードオプション追加。
          
          【変更内容】
          1. --s3 フラグ追加
          2. S3モード時は init_s3_backend() 呼び出し
          3. 環境変数から認証情報取得
          
          【使用例】
          ```bash
          # ローカルモード（デフォルト）
          python scripts/run_all_backtests.py
          
          # S3モード
          AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=xxx \
          python scripts/run_all_backtests.py --s3
          ```
          
        priority: medium
        assigned_to: ashigaru3
        depends_on: [task_045_4, task_045_5, task_045_6]

      # ========== Phase 4: 高速化オプション統合（3日） ==========
      - task_id: task_045_10
        description: |
          【Numba並列化デフォルト有効化 + ResourceConfig統合】

          src/backtest/fast_engine.py のデフォルト設定を変更。
          **ResourceConfig（src/config/resource_config.py）を活用すること。**

          【変更内容】
          ```python
          from src.config.resource_config import get_current_resource_config

          @dataclass
          class FastBacktestConfig:
              use_numba: bool = True
              numba_parallel: bool = True  # False → True に変更

              @classmethod
              def from_resource_config(cls) -> "FastBacktestConfig":
                  """ResourceConfigからインスタンスを生成"""
                  rc = get_current_resource_config()
                  return cls(
                      use_numba=rc.use_numba,
                      numba_parallel=rc.numba_parallel,
                      use_gpu=rc.use_gpu,
                      # ... その他のフィールド
                  )
          ```

          【テスト】
          - 既存テストがパスすること
          - ベンチマーク: 高速化倍率の測定

          【期待効果】
          - 4-8倍高速化

        priority: high
        assigned_to: ashigaru1
        depends_on: [task_045_7, task_045_8, task_045_9]

      - task_id: task_045_11
        description: |
          【GPU計算オプション整備 + ResourceConfig統合】

          GPU計算の利用を簡便化。
          **ResourceConfig（src/config/resource_config.py）のGPU検出機能を活用すること。**
          ※ ResourceConfigには既にGPU自動検出が実装済み（GPUInfo dataclass）

          【変更内容】
          1. ResourceConfigのGPU検出機能を利用
          2. GPU利用時の設定ガイド追加
          3. CuPyインストール確認スクリプト追加

          【実装】
          ResourceConfigには既に以下が実装済み:
          ```python
          # src/config/resource_config.py より
          @dataclass
          class GPUInfo:
              available: bool = False
              device_count: int = 0
              devices: list[dict] = field(default_factory=list)
              total_memory_gb: float = 0.0
          ```

          FastBacktestConfigでの使用:
          ```python
          from src.config.resource_config import get_current_resource_config

          rc = get_current_resource_config()
          use_gpu = rc.use_gpu  # 自動検出済み
          gpu_memory_fraction = rc.gpu_memory_fraction
          ```

          【期待効果】
          - GPU環境で10-50倍高速化

        priority: medium
        assigned_to: ashigaru2
        depends_on: [task_045_7, task_045_8, task_045_9]

      - task_id: task_045_12
        description: |
          【Ray分散処理統合 + ResourceConfig統合】

          RayBacktestEngine の利用を簡便化。
          **ResourceConfig（src/config/resource_config.py）のray_workers設定を活用すること。**

          【変更内容】
          1. BacktestEngineFactory に "ray" モード追加
          2. Ray自動初期化オプション
          3. ResourceConfigからワーカー数を取得

          【使用例】
          ```python
          from src.config.resource_config import get_current_resource_config

          rc = get_current_resource_config()
          engine = BacktestEngineFactory.create(
              mode="ray",
              n_workers=rc.ray_workers,  # 自動検出されたCPUコア数
          )
          ```

          【期待効果】
          - CPUコア数に応じた線形スケール

        priority: medium
        assigned_to: ashigaru3
        depends_on: [task_045_7, task_045_8, task_045_9]

      - task_id: task_045_12a
        description: |
          【既存ハードコード制限のResourceConfig置換】

          既存コードのハードコードされたリソース制限を ResourceConfig 参照に置換。
          src/config/resource_config.py が実装済み（将軍が作成）。

          【置換対象】
          1. src/signals/signal_generation.py (L60付近)
             - 変更前: `max_workers=8`
             - 変更後: `max_workers=get_current_resource_config().max_workers`

          2. src/utils/unified_cache_manager.py (L42付近)
             - 変更前: `max_memory_mb=512`
             - 変更後: `max_memory_mb=get_current_resource_config().cache_max_memory_mb`

          3. src/backtest/fast_engine.py
             - batch_size → ResourceConfig.batch_size
             - parallel_fetchers → ResourceConfig.parallel_fetchers

          4. src/utils/cache_manager.py
             - CachePolicy のデフォルト値を ResourceConfig から取得

          【ResourceConfig使用方法】
          ```python
          from src.config.resource_config import get_current_resource_config

          config = get_current_resource_config()
          # 以下が自動検出される:
          # - config.max_workers (CPUコア数、専用サーバーなら全コア)
          # - config.cache_max_memory_mb (利用可能メモリの70%)
          # - config.cache_max_entries (メモリに応じて計算)
          # - config.batch_size (ワーカー数 × 10)
          # - config.parallel_fetchers (min(max_workers, 8))
          ```

          【テスト】
          - 既存テストがパスすること
          - リソース検出が正しく動作すること（psutil依存）

          【期待効果】
          - 専用サーバーでのリソース使用効率向上
          - ハードコード制限の撤廃

        priority: high
        assigned_to: ashigaru4
        depends_on: [task_045_7, task_045_8, task_045_9]

      # ========== Phase 5: ドキュメント・テスト（1日） ==========
      - task_id: task_045_13
        description: |
          【S3キャッシュ利用ガイド作成】
          
          docs/s3_cache_guide.md を作成。
          
          【内容】
          1. セットアップ手順（AWS認証情報設定）
          2. 設定ファイルの記述方法
          3. CLI使用例
          4. トラブルシューティング
          5. コスト見積もり
          
        priority: medium
        assigned_to: ashigaru1
        depends_on: [task_045_10, task_045_11, task_045_12, task_045_12a]

      - task_id: task_045_14
        description: |
          【S3統合テスト追加】

          tests/integration/test_s3_cache.py を作成。

          【テスト項目】
          1. StorageBackend 読み書き
          2. SignalPrecomputer S3モード
          3. CovarianceCache S3モード
          4. DataCache S3モード
          5. ローカルキャッシュTTL
          6. ResourceConfig 動的設定（新規追加）

          【注意】
          - CI環境ではスキップ（AWS認証情報なし）
          - @pytest.mark.skipif で条件分岐

        priority: medium
        assigned_to: ashigaru2
        depends_on: [task_045_10, task_045_11, task_045_12, task_045_12a]

      - task_id: task_045_15
        description: |
          【ベンチマーク実行・報告】

          各高速化オプションの効果を測定・報告。

          【測定項目】
          1. ローカルキャッシュ vs S3キャッシュ
          2. Numba並列化の効果
          3. GPU計算の効果（GPU環境のみ）
          4. Ray分散の効果
          5. ResourceConfig動的設定の効果（新規追加）

          【報告形式】
          | 設定 | 月次(秒) | 週次(秒) | 日次(秒) | 高速化倍率 |

          【期待結果】
          - 日次バックテスト: 30-60分 → 数秒〜1分

        priority: medium
        assigned_to: ashigaru3
        depends_on: [task_045_10, task_045_11, task_045_12, task_045_12a]

    expected_outcome: |
      【成果物】
      1. S3キャッシュ対応の全キャッシュクラス
      2. 設定ファイルによるバックエンド切替
      3. 高速化オプション（Numba/GPU/Ray）の統合
      4. ドキュメント・テスト完備
      
      【期待効果】
      - 日次15年バックテスト: 30-60分 → 数秒〜1分
      - クラウド環境（EC2/Lambda）でのキャッシュ共有
      - S3による耐久性向上（99.999999999%）
