name: Performance Benchmark

on:
  # Weekly schedule (Sunday at 00:00 UTC)
  schedule:
    - cron: '0 0 * * 0'

  # Manual trigger
  workflow_dispatch:
    inputs:
      quick:
        description: 'Run quick benchmark'
        required: false
        default: 'false'
        type: boolean

  # On push to main (optional, for tracking)
  push:
    branches:
      - main
    paths:
      - 'src/backtest/**'
      - 'src/signals/**'
      - 'scripts/benchmark.py'

jobs:
  benchmark:
    name: Run Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for git info

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil  # For memory measurement

      - name: Run benchmark
        id: benchmark
        run: |
          if [ "${{ github.event.inputs.quick }}" = "true" ]; then
            python scripts/benchmark.py --quick --save --compare
          else
            python scripts/benchmark.py --save --compare
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: results/benchmark_history.json
          retention-days: 90

      - name: Check for regression
        if: github.event_name == 'push'
        run: |
          # Check if any benchmark is significantly slower (>20%)
          python -c "
          import json
          from pathlib import Path

          history_path = Path('results/benchmark_history.json')
          if not history_path.exists():
              print('No history to check')
              exit(0)

          with open(history_path) as f:
              history = json.load(f)

          if len(history) < 2:
              print('Not enough history to compare')
              exit(0)

          current = history[-1]
          previous = history[-2]

          regressions = []
          for curr_result in current['results']:
              for prev_result in previous['results']:
                  if (curr_result['name'] == prev_result['name'] and
                      curr_result['scenario'] == prev_result['scenario']):
                      prev_time = prev_result['execution_time_sec']
                      curr_time = curr_result['execution_time_sec']
                      if prev_time > 0:
                          change = (curr_time - prev_time) / prev_time * 100
                          if change > 20:
                              regressions.append({
                                  'name': curr_result['name'],
                                  'scenario': curr_result['scenario'],
                                  'change': change,
                              })

          if regressions:
              print('⚠️ Performance Regressions Detected:')
              for r in regressions:
                  print(f\"  - {r['name']} ({r['scenario']}): +{r['change']:.1f}%\")
              exit(1)
          else:
              print('✓ No significant performance regressions')
          "

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const historyPath = 'results/benchmark_history.json';

            if (!fs.existsSync(historyPath)) {
              console.log('No benchmark history found');
              return;
            }

            const history = JSON.parse(fs.readFileSync(historyPath, 'utf8'));
            const latest = history[history.length - 1];

            let body = '## Benchmark Results\n\n';
            body += `| Benchmark | Scenario | Time (s) | Memory (MB) |\n`;
            body += `|-----------|----------|----------|-------------|\n`;

            for (const result of latest.results) {
              const status = result.success ? '✓' : '✗';
              body += `| ${status} ${result.name} | ${result.scenario} | ${result.execution_time_sec.toFixed(3)} | ${result.memory_peak_mb.toFixed(1)} |\n`;
            }

            body += `\n_Total time: ${latest.total_time_sec.toFixed(2)}s_`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

  benchmark-compare:
    name: Compare Benchmark History
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'schedule'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download latest artifact
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: results/

      - name: Generate trend report
        run: |
          python -c "
          import json
          from pathlib import Path
          from datetime import datetime

          history_path = Path('results/benchmark_history.json')
          if not history_path.exists():
              print('No history found')
              exit(0)

          with open(history_path) as f:
              history = json.load(f)

          print('# Benchmark Trend Report')
          print(f'Generated: {datetime.now().isoformat()}')
          print(f'Total runs: {len(history)}')
          print()

          if len(history) >= 5:
              print('## Recent Trend (Last 5 Runs)')
              for run in history[-5:]:
                  print(f\"- {run['timestamp'][:10]}: {run['total_time_sec']:.2f}s\")
          "
